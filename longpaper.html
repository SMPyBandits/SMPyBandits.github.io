

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <script type="text/javascript">

            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-38514290-2']);
            _gaq.push(['_trackPageview']);

            (function() {
                var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
                ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
            })();
            </script>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Summary &mdash; SMPyBandits 0.9.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="SMPyBandits 0.9.1 documentation" href="index.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> SMPyBandits
          

          
            
            <img src="_static/logo.png" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                0.9
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="README.html"><em>SMPyBandits</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="docs/modules.html">SMPyBandits modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="How_to_run_the_code.html">How to run the code ?</a></li>
<li class="toctree-l1"><a class="reference internal" href="PublicationsWithSMPyBandits.html">List of research publications using Lilian Besson&#8217;s SMPyBandits project</a></li>
<li class="toctree-l1"><a class="reference internal" href="Aggregation.html"><strong>Policy aggregation algorithms</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="MultiPlayers.html"><strong>Multi-players simulation environment</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="DoublingTrick.html"><strong>Doubling Trick for Multi-Armed Bandits</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="API.html">Short documentation of the API</a></li>
<li class="toctree-l1"><a class="reference internal" href="TODO.html">ðŸ’¥ TODO</a></li>
<li class="toctree-l1"><a class="reference internal" href="plots/README.html">Some illustrations for this project</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/README.html">Jupyter Notebooks ðŸ““ by Naereen &#64; GitHub</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/list.html">List of notebooks for SMPyBandits documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Profiling.html">A note on execution times, speed and profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="uml_diagrams/README.html">UML diagrams</a></li>
<li class="toctree-l1"><a class="reference internal" href="logs/README.html"><code class="docutils literal"><span class="pre">logs/</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="CONTRIBUTING.html">On Github Issues and Pull Requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="CODE_OF_CONDUCT.html">Contributor Covenant Code of Conduct</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">SMPyBandits</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Summary</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/longpaper.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<hr class="docutils" />
<p>title: &#8220;<em>SMPyBandits</em>&#8220;
subtitle: &#8220;a Research Framework for Single and Multi-Players Multi-Arms Bandits Algorithms in Python&#8221;
title-meta: &#8220;<em>SMPyBandits</em>: an Open-Source Research Framework for Single and Multi-Players Multi-Arms Bandits Algorithms in Python&#8221;
author-meta: Lilian Besson
author:</p>
<ul class="simple">
<li>name: Lilian Besson
orcid: 0000-0003-2767-2563
thanks:
Thanks to Emilie Kaufmann and Christophe Moy for their review and support.
affiliation:
PhD Student at CentraleSupÃ©lec, campus of Rennes, SCEE team &amp; Inria Lille Nord Europe, SequeL team.
email: Lilian.Besson[AT]CentraleSupelec[.]fr
tags:</li>
<li>sequential learning</li>
<li>multi-arm bandits</li>
<li>multi-player multi-arm bandits</li>
<li>aggregation of sequential learning algorithms</li>
<li>learning theory
date: 22 February 2018
lang: en-US
babel-lang: english
bibliography: paper.bib
biblio-style: alpha
link-citations: true
colorlinks: true
numbersections: yes
section-titles: yes
usehancyhdr: yes
fontsize: 11pt
geometry: scale=0.72
fontfamily: palatino
abstract:
I present the open-source numerical environment <em>SMPyBandits</em>, written in Python and designed to be an easy to use framework for experimenting with single- and multi-player algorithms and in different variations of the multi-armed bandits problem.
thanks:
Thanks to Emilie Kaufmann and Christophe Moy.</li>
</ul>
<hr class="docutils" />
<div class="section" id="Summary">
<h1>Summary<a class="headerlink" href="#Summary" title="Permalink to this headline">Â¶</a></h1>
<p>This article presents <a class="reference external" href="https://perso.crans.org/besson/">my</a> numerical environment <em>SMPyBandits</em>, written in <a class="reference external" href="https://www.python.org/">Python (2 or 3)</a> [&#64;python], for numerical simulations on <em>single</em>-player and <em>multi</em>-players <a class="reference external" href="https://en.wikipedia.org/wiki/Multi-armed_bandit">Multi-Armed Bandits (MAB)</a> algorithms [&#64;Bubeck12].</p>
<p><em>SMPyBandits</em> is the most complete open-source implementation of state-of-the-art algorithms tackling various kinds of sequential learning problems referred to as Multi-Armed Bandits.
It aims at being extensive, simple to use and maintain, with a clean and perfectly documented codebase. But most of all it allows fast prototyping of simulations and experiments, with an easy configuration system and command-line options to customize experiments while starting them (see below for an example).</p>
<p><em>SMPyBandits</em> does not aim at being blazing fast or perfectly memory efficient, and comes with a pure Python implementation with no dependency except standard open-source Python packages.
Even if some critical parts are also available as a <code class="docutils literal"><span class="pre">C</span></code> Python extension, and even by using Numba [&#64;numba] whenever it is possible, if simulation speed really matters, one should rather refer to less exhaustive but faster implementations, like for example [&#64;TorLibbandit] in <code class="docutils literal"><span class="pre">C++</span></code> or [&#64;VishMABjl] in Julia.</p>
<hr class="docutils" />
<div class="section" id="Presentation">
<h2>Presentation<a class="headerlink" href="#Presentation" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="Single-Player-MAB">
<h3>Single-Player MAB<a class="headerlink" href="#Single-Player-MAB" title="Permalink to this headline">Â¶</a></h3>
<p>Multi-Armed Bandit (MAB) problems are well-studied sequential decision making problems in which an agent repeatedly chooses an action (the &#8220;arm&#8221; of a one-armed bandit) in order to maximize some total reward [&#64;Robbins52,LaiRobbins85]. Initial motivation for their study came from the modeling of clinical trials, as early as 1933 with the seminal work of Thompson  [&#64;Thompson33]. In this example, arms correspond to different treatments with unknown, random effect. Since then, MAB models have been proved useful for many more applications, that range from cognitive radio [&#64;Jouini09] to online content optimization (news article recommendation [&#64;Li10], online advertising [&#64;LiChapelle11] or A/B Testing [&#64;Kaufmann14;Jamieson17]), or portfolio optimization [&#64;Sani12].</p>
<p>This Python package is the most complete open-source implementation of single-player (classical) bandit algorithms (<a class="reference external" href="https://smpybandits.github.io/docs/Policies.html">over 65!</a>).
We use a well-designed hierarchical structure and <a class="reference external" href="https://smpybandits.github.io/uml_diagrams/README.html">class inheritance scheme</a> to minimize redundancy in the codebase, and for instance the code specific to the UCB algorithm [&#64;LaiRobbins85;&#64;Auer02] is as short as this (and fully documented), by inheriting from the <a class="reference external" href="https://smpybandits.github.io/docs/Policies.IndexPolicy.html"><code class="docutils literal"><span class="pre">IndexPolicy</span></code></a> class:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">sqrt</span><span class="p">,</span> <span class="n">log</span>
<span class="kn">from</span> <span class="nn">.IndexPolicy</span> <span class="kn">import</span> <span class="n">IndexPolicy</span>

<span class="k">class</span> <span class="nc">UCB</span><span class="p">(</span><span class="n">IndexPolicy</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; The UCB policy for bounded bandits.</span>
<span class="sd">  Reference: [Lai &amp; Robbins, 1985]. &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">computeIndex</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Compute the current index, at time t and</span>
<span class="sd">    after :math:`N_k(t)` pulls of arm k:</span>

<span class="sd">    .. math::</span>

<span class="sd">        I_k(t) = \frac{X_k(t)}{N_k(t)}</span>
<span class="sd">        + \sqrt{\frac{2 \log(t)}{N_k(t)}}.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pulls</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># forced exploration</span>
      <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;+inf&#39;</span><span class="p">)</span>   <span class="c1"># in the first steps</span>
    <span class="k">else</span><span class="p">:</span>                    <span class="c1"># or compute UCB index</span>
      <span class="n">estimated_mean</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">pulls</span><span class="p">[</span><span class="n">arm</span><span class="p">])</span>
      <span class="n">exploration_bias</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">pulls</span><span class="p">[</span><span class="n">arm</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">estimated_mean</span> <span class="o">+</span> <span class="n">exploration_bias</span>
</pre></div>
</div>
</div>
<div class="section" id="Multi-Players-MAB">
<h3>Multi-Players MAB<a class="headerlink" href="#Multi-Players-MAB" title="Permalink to this headline">Â¶</a></h3>
<p>For Cognitive Radio applications, a well-studied extension is to consider $M\geq2$ players, interacting on the <em>same</em> $K$ arms. Whenever two or more players select the same arm at the same time, they all suffer from a collision.
Different collision models has been proposed, and the simplest one consist in giving a $0$ reward to each colliding players.
Without any centralized supervision or coordination between players, they must learn to access the $M$ best resources (<em>i.e.</em>, arms with highest means) without collisions.</p>
<p>This package implements <a class="reference external" href="https://smpybandits.github.io/docs/Environment.CollisionModels.html">all the collision models</a> found in the literature, as well as all the algorithms from the last 10 years or so (including <a class="reference external" href="https://smpybandits.github.io/docs/PoliciesMultiPlayers.rhoRand.html"><code class="docutils literal"><span class="pre">rhoRand</span></code></a> from 2009, <a class="reference external" href="https://smpybandits.github.io/docs/Policies.MEGA.html"><code class="docutils literal"><span class="pre">MEGA</span></code></a> from 2015, <a class="reference external" href="https://smpybandits.github.io/docs/Policies.MusicalChair.html"><code class="docutils literal"><span class="pre">MusicalChair</span></code></a> from 2016, and our state-of-the-art algorithms <a class="reference external" href="https://smpybandits.github.io/docs/PoliciesMultiPlayers.RandTopM.html"><code class="docutils literal"><span class="pre">RandTopM</span></code></a> and <a class="reference external" href="https://smpybandits.github.io/docs/PoliciesMultiPlayers.MCTopM.html"><code class="docutils literal"><span class="pre">MCTopM</span></code></a>) from [&#64;BessonALT2018].</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="Purpose">
<h2>Purpose<a class="headerlink" href="#Purpose" title="Permalink to this headline">Â¶</a></h2>
<p>The main goal of this package is to implement <a class="reference external" href="https://smpybandits.github.io/API.html">with the same API</a> most of the existing single- and multi-player multi-armed bandit algorithms.
Each algorithm comes with a clean documentation page, containing a reference to the research article(s) that introduced it, and with remarks on its numerical efficiency.</p>
<p>It is neither the first nor the only open-source implementation of multi-armed bandits algorithms, although one can notice the absence of any well-maintained reference implementation.
I built <em>SMPyBandits</em> from a framework called <em>pymaBandits</em> [&#64;pymaBandits], which implemented a few algorithms and three kinds of arms, in both Python and MATLAB.
The goal was twofolds, first to implement as many algorithms as possible to have a complete implementation of the current state of research in MAB, and second to implement multi-players simulations with different models.</p>
<p>Since November $2016$, I follow actively the latest publications related to Multi-Armed Bandits (MAB) research, and usually I implement quickly any new algorithms. For instance, <a class="reference external" href="https://smpybandits.github.io/docs/Policies.Exp3PlusPlus.html">Exp3++</a>, <a class="reference external" href="https://smpybandits.github.io/docs/Policies.CORRAL.html">CORRAL</a> and <a class="reference external" href="https://smpybandits.github.io/docs/Policies.SparseUCB.html">SparseUCB</a> were each introduced by articles (<a class="reference external" href="https://arxiv.org/pdf/1702.06103">for Exp3++</a>, <a class="reference external" href="https://arxiv.org/abs/1612.06246v2">for CORRAL</a>, <a class="reference external" href="https://arxiv.org/abs/1706.01383">for SparseUCB</a>) presented at COLT in July 2017, <a class="reference external" href="https://smpybandits.github.io/docs/Policies.LearnExp.html">LearnExp</a> comes from a <a class="reference external" href="https://arxiv.org/abs/1702.04825">NIPS 2017 paper</a>, and <a class="reference external" href="https://smpybandits.github.io/docs/Policies.klUCBPlusPlus.html">kl-UCB++</a> from an <a class="reference external" href="https://hal.inria.fr/hal-01475078">ALT 2017 paper</a>.</p>
</div>
<hr class="docutils" />
<div class="section" id="Features">
<h2>Features<a class="headerlink" href="#Features" title="Permalink to this headline">Â¶</a></h2>
<p>With this numerical framework, simulations can run on a single CPU or a multi-core machine using joblib [&#64;joblib],
and summary plots are automatically saved as high-quality PNG, PDF and EPS (ready for being used in research article), using matplotlib [&#64;matplotlib] and seaborn [&#64;seaborn].
Making new simulations is very easy, one only needs to write a configuration script and no knowledge of the internal code architecture.</p>
<div class="section" id="Examples-of-configuration-for-some-simulations">
<h3>Examples of configuration for some simulations<a class="headerlink" href="#Examples-of-configuration-for-some-simulations" title="Permalink to this headline">Â¶</a></h3>
<p>A small script <a class="reference external" href="https://smpybandits.github.io/docs/configuration.html"><code class="docutils literal"><span class="pre">configuration.py</span></code></a> is used to import the <a class="reference external" href="https://smpybandits.github.io/docs/Arms.html">arm classes</a>, the <a class="reference external" href="https://smpybandits.github.io/docs/Policies.html">policy classes</a> and define the problems and the experiments.
For instance, we can compare the standard anytime <a class="reference external" href="https://smpybandits.github.io/docs/Policies.klUCB.html"><code class="docutils literal"><span class="pre">klUCB</span></code></a> algorithm against the non-anytime variant <a class="reference external" href="https://smpybandits.github.io/docs/Policies.klUCBPlusPlus.html"><code class="docutils literal"><span class="pre">klUCBPlusPlus</span></code></a> algorithm, as well as <a class="reference external" href="https://smpybandits.github.io/docs/Policies.UCBalpha.html"><code class="docutils literal"><span class="pre">UCB</span></code></a> (with $\alpha=1$) and <a class="reference external" href="https://smpybandits.github.io/docs/Policies.Thompson.html"><code class="docutils literal"><span class="pre">Thompson</span></code></a> (with <a class="reference external" href="https://smpybandits.github.io/docs/Policies.Posterior.Beta.html">Beta posterior</a>).
See below in Figure \ref{fig:plot1} for the result showing the average regret for these $4$ algorithms.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">Arms</span> <span class="kn">import</span> <span class="o">*</span><span class="p">;</span>  <span class="kn">from</span> <span class="nn">Policies</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">configuration</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s2">&quot;horizon&quot;</span><span class="p">:</span> <span class="mi">10000</span><span class="p">,</span>     <span class="c1"># Finite horizon of the simulation</span>
  <span class="s2">&quot;repetitions&quot;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>  <span class="c1"># Number of repetitions</span>
  <span class="s2">&quot;n_jobs&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>         <span class="c1"># Max number of cores for parallelization</span>
  <span class="c1"># Environment configuration, you can set up more than one.</span>
  <span class="s2">&quot;environment&quot;</span><span class="p">:</span> <span class="p">[</span> <span class="p">{</span>
      <span class="s2">&quot;arm_type&quot;</span><span class="p">:</span> <span class="n">Bernoulli</span><span class="p">,</span>
      <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>
  <span class="p">}],</span>
  <span class="c1"># Policies that should be simulated, and their parameters.</span>
  <span class="s2">&quot;policies&quot;</span><span class="p">:</span> <span class="p">[</span>
      <span class="p">{</span><span class="s2">&quot;archtype&quot;</span><span class="p">:</span> <span class="n">klUCB</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{}</span> <span class="p">},</span>
      <span class="p">{</span><span class="s2">&quot;archtype&quot;</span><span class="p">:</span> <span class="n">klUCBPlusPlus</span><span class="p">,</span>
       <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{</span> <span class="s2">&quot;horizon&quot;</span><span class="p">:</span> <span class="mi">10000</span> <span class="p">}</span> <span class="p">},</span>
      <span class="p">{</span><span class="s2">&quot;archtype&quot;</span><span class="p">:</span> <span class="n">UCBalpha</span><span class="p">,</span>
       <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{</span> <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mi">1</span> <span class="p">}</span> <span class="p">},</span>
      <span class="p">{</span><span class="s2">&quot;archtype&quot;</span><span class="p">:</span> <span class="n">Thompson</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{}</span> <span class="p">},</span>
  <span class="p">]}</span>
</pre></div>
</div>
<p>For a second example, this snippet is a minimal example [^confmultiplayers] of configuration for multiplayer simulations, comparing different multi-player algorithms used with the <a class="reference external" href="https://smpybandits.github.io/docs/Policies.klUCB.html"><code class="docutils literal"><span class="pre">klUCB</span></code></a> index policy.
See below in Figure \ref{fig:plot2} for an illustration.</p>
<p>[^confmultiplayers]:  See the file <a class="reference external" href="https://smpybandits.github.io/docs/configuration_multiplayers.html"><code class="docutils literal"><span class="pre">configuration_multiplayers.py</span></code></a> in the code for more details.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">Arms</span> <span class="kn">import</span> <span class="o">*</span><span class="p">;</span>  <span class="kn">from</span> <span class="nn">Policies</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">PoliciesMultiPlayers</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">nbPlayers</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">configuration</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s2">&quot;horizon&quot;</span><span class="p">:</span> <span class="mi">10000</span><span class="p">,</span>    <span class="c1"># Finite horizon of the simulation</span>
  <span class="s2">&quot;repetitions&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>  <span class="c1"># Number of repetitions</span>
  <span class="s2">&quot;n_jobs&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>        <span class="c1"># Max number of cores for parallelization</span>
  <span class="c1"># Environment configuration, you can set up more than one.</span>
  <span class="s2">&quot;environment&quot;</span><span class="p">:</span> <span class="p">[</span> <span class="p">{</span>
      <span class="s2">&quot;arm_type&quot;</span><span class="p">:</span> <span class="n">Bernoulli</span><span class="p">,</span>
      <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>
  <span class="p">}</span> <span class="p">],</span>
  <span class="c1"># Policies that should be simulated, and their parameters.</span>
  <span class="s2">&quot;successive_players&quot;</span><span class="p">:</span> <span class="p">[</span>
      <span class="n">CentralizedMultiplePlay</span><span class="p">(</span><span class="n">nbPlayers</span><span class="p">,</span> <span class="n">nbArms</span><span class="p">,</span> <span class="n">klUCB</span><span class="p">)</span><span class="o">.</span><span class="n">children</span><span class="p">,</span>
      <span class="n">RandTopM</span><span class="p">(</span><span class="n">nbPlayers</span><span class="p">,</span> <span class="n">nbArms</span><span class="p">,</span> <span class="n">klUCB</span><span class="p">)</span><span class="o">.</span><span class="n">children</span><span class="p">,</span>
      <span class="n">MCTopM</span><span class="p">(</span><span class="n">nbPlayers</span><span class="p">,</span> <span class="n">nbArms</span><span class="p">,</span> <span class="n">klUCB</span><span class="p">)</span><span class="o">.</span><span class="n">children</span><span class="p">,</span>
      <span class="n">Selfish</span><span class="p">(</span><span class="n">nbPlayers</span><span class="p">,</span> <span class="n">nbArms</span><span class="p">,</span> <span class="n">klUCB</span><span class="p">)</span><span class="o">.</span><span class="n">children</span><span class="p">,</span>
      <span class="n">rhoRand</span><span class="p">(</span><span class="n">nbPlayers</span><span class="p">,</span> <span class="n">nbArms</span><span class="p">,</span> <span class="n">klUCB</span><span class="p">)</span><span class="o">.</span><span class="n">children</span><span class="p">,</span>
  <span class="p">]</span> <span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="Documentation">
<h3>Documentation<a class="headerlink" href="#Documentation" title="Permalink to this headline">Â¶</a></h3>
<p>A complete sphinx [&#64;sphinx] documentation for each algorithms and every piece of code, included the constants in the different configuration files, is available here: <a class="reference external" href="https://smpybandits.github.io/"><code class="docutils literal"><span class="pre">https://smpybandits.github.io</span></code></a>.</p>
</div>
<div class="section" id="Other-noticeable-features">
<h3>Other noticeable features<a class="headerlink" href="#Other-noticeable-features" title="Permalink to this headline">Â¶</a></h3>
<div class="section" id="Single-player-Policies">
<h4><a class="reference external" href="https://smpybandits.github.io/docs/Policies.html">Single-player Policies</a><a class="headerlink" href="#Single-player-Policies" title="Permalink to this headline">Â¶</a></h4>
<ul class="simple">
<li>More than 65 algorithms, including all known variants of the <a class="reference external" href="https://smpybandits.github.io/docs/Policies.UCB.html"><code class="docutils literal"><span class="pre">UCB</span></code></a>, <a class="reference external" href="https://smpybandits.github.io/docs/Policies.klUCB.html">kl-UCB</a>, <a class="reference external" href="https://smpybandits.github.io/docs/Policies.MOSS.html"><code class="docutils literal"><span class="pre">MOSS</span></code></a> and <a class="reference external" href="https://smpybandits.github.io/docs/Policies.Thompson.html">Thompson Sampling</a> algorithms, as well as other less known algorithms (<a class="reference external" href="https://smpybandits.github.io/docs/Policies.OCUCB.html"><code class="docutils literal"><span class="pre">OCUCB</span></code></a>, <a class="reference external" href="https://smpybandits.github.io/docs/Policies.OCUCB.html"><code class="docutils literal"><span class="pre">BESA</span></code></a>, <a class="reference external" href="https://smpybandits.github.io/docs/Policies.OSSB.html"><code class="docutils literal"><span class="pre">OSSB</span></code></a> etc).</li>
<li>Implementation of very recent Multi-Armed Bandits algorithms, e.g., <a class="reference external" href="https://smpybandits.github.io/docs/Policies.klUCBPlusPlus.html"><code class="docutils literal"><span class="pre">kl-UCB++</span></code></a>, <a class="reference external" href="https://smpybandits.github.io/docs/Policies.UCBdagger.html"><code class="docutils literal"><span class="pre">UCB-dagger</span></code></a>,  or <a class="reference external" href="https://smpybandits.github.io/docs/Policies.MOSSAnytime.html"><code class="docutils literal"><span class="pre">MOSS-anytime</span></code></a> (from <a class="reference external" href="http://proceedings.mlr.press/v48/degenne16.pdf">this COLT 2016 article</a>).</li>
<li>Experimental policies: <a class="reference external" href="https://smpybandits.github.io/docs/Policies.BlackBoxOpt.html"><code class="docutils literal"><span class="pre">BlackBoxOpt</span></code></a> or <a class="reference external" href="https://smpybandits.github.io/docs/Policies.UnsupervisedLearning.html"><code class="docutils literal"><span class="pre">UnsupervisedLearning</span></code></a> (using Gaussian processes to learn the arms distributions).</li>
</ul>
</div>
<div class="section" id="Arms-and-problems">
<h4>Arms and problems<a class="headerlink" href="#Arms-and-problems" title="Permalink to this headline">Â¶</a></h4>
<ul class="simple">
<li>The framework mainly targets stochastic bandits, with arms following <a class="reference external" href="https://smpybandits.github.io/docs/Arms/Bernoulli.html"><code class="docutils literal"><span class="pre">Bernoulli</span></code></a>, bounded (truncated) or unbounded <a class="reference external" href="https://smpybandits.github.io/docs/Arms/Gaussian.html"><code class="docutils literal"><span class="pre">Gaussian</span></code></a>, <a class="reference external" href="https://smpybandits.github.io/docs/Arms/Exponential.html"><code class="docutils literal"><span class="pre">Exponential</span></code></a>, <a class="reference external" href="https://smpybandits.github.io/docs/Arms/Gamma.html"><code class="docutils literal"><span class="pre">Gamma</span></code></a> or <a class="reference external" href="https://smpybandits.github.io/docs/Arms/Poisson.html"><code class="docutils literal"><span class="pre">Poisson</span></code></a> distributions.</li>
<li>The default configuration is to use a fixed problem for N repetitions (e.g. 1000 repetitions, use <a class="reference external" href="https://smpybandits.github.io/docs/Environment/MAB.html"><code class="docutils literal"><span class="pre">MAB.MAB</span></code></a>), but there is also a perfect support for &#8220;Bayesian&#8221; problems where the mean vector $\mu_1,\dots,\mu_K$ change <em>at every repetition</em> (see <a class="reference external" href="https://smpybandits.github.io/docs/Environment/MAB.html"><code class="docutils literal"><span class="pre">MAB.DynamicMAB</span></code></a>).</li>
<li>There is also a good support for Markovian problems, see <a class="reference external" href="https://smpybandits.github.io/docs/Environment/MAB.html"><code class="docutils literal"><span class="pre">MAB.MarkovianMAB</span></code></a>, even though I preferred to not implement policies specifically designed for Markovian problems.</li>
</ul>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="Other-remarks">
<h2>Other remarks<a class="headerlink" href="#Other-remarks" title="Permalink to this headline">Â¶</a></h2>
<ul class="simple">
<li>The framework is implemented in an imperative and object oriented style. Algorithm and arms are represented as classes, and the API of the <code class="docutils literal"><span class="pre">Arms</span></code>, <code class="docutils literal"><span class="pre">Policy</span></code> and <code class="docutils literal"><span class="pre">MultiPlayersPolicy</span></code> classes is <a class="reference external" href="https://smpybandits.github.io/API.html">clearly documented</a>.</li>
<li>The code is <a class="reference external" href="https://smpybandits.github.io/logs/main_pylint_log.txt">clean</a>, and a special care is given to keep it compatible for both <a class="reference external" href="https://smpybandits.github.io/logs/main_pylint_log.txt">Python 2</a> and <a class="reference external" href="https://smpybandits.github.io/logs/main_pylint3_log.txt">Python 3</a>.</li>
<li>The joblib library [&#64;joblib] is used for the <a class="reference external" href="https://smpybandits.github.io/docs/Environment.Evaluator.html"><code class="docutils literal"><span class="pre">Evaluator</span></code></a> classes, so the simulations are easily ran in parallel on multi-core machines and servers [^nogpu].</li>
</ul>
<p>[^nogpu]:  Note that <em>SMPyBandits</em> does no need a GPU and is not optimized to run on a cluster. In particular, it does not take advantage of popular libraries like <a class="reference external" href="https://github.com/pydata/numexpr">numexpr</a>, <a class="reference external" href="http://www.deeplearning.net/software/theano/">theano</a> or <a class="reference external" href="https://www.tensorflow.org/">tensorflow</a>.</p>
<div class="section" id="How-to-run-the-experiments-?">
<h3>How to run the experiments ?<a class="headerlink" href="#How-to-run-the-experiments-?" title="Permalink to this headline">Â¶</a></h3>
<p>For example, this short bash snippet [^docforconf] shows how to clone the code, install the requirements for Python 3 (in a virtualenv [&#64;virtualenv]), and starts some simulation for $N=1000$ repetitions of the default non-Bayesian Bernoulli-distributed problem, for $K=9$ arms, an horizon of $T=10000$ and on $4$ CPUs [^speedofsimu].
Using environment variables (<code class="docutils literal"><span class="pre">N=1000</span></code>) when launching the simulation is not required but it is convenient.</p>
<p>[^docforconf]:  See <a class="reference external" href="https://smpybandits.github.io/How_to_run_the_code.html">this page of the documentation</a> for more details.
[^speedofsimu]:  It takes about $20$ to $40$ minutes for each simulation, on a standard $4$-cores $64$ bits GNU/Linux laptop.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># 1. get the code in /tmp/, or wherever you want</span>
<span class="nb">cd</span> /tmp/
git clone https://GitHub.com/SMPyBandits/SMPyBandits.git
<span class="nb">cd</span> SMPyBandits.git
<span class="c1"># 2. just be sure you have the latest virtualenv from Python 3</span>
sudo pip3 install --upgrade virtualenv
<span class="c1"># 3. create and active the virtualenv</span>
virtualenv3 venv <span class="o">||</span> virtualenv venv
. venv/bin/activate
<span class="c1"># 4. install the requirements in the virtualenv</span>
pip3 install -r requirements.txt
<span class="c1"># 5. run a single-player simulation!</span>
<span class="nv">N</span><span class="o">=</span><span class="m">1000</span> <span class="nv">T</span><span class="o">=</span><span class="m">10000</span> <span class="nv">K</span><span class="o">=</span><span class="m">9</span> <span class="nv">N_JOBS</span><span class="o">=</span><span class="m">4</span> make single
<span class="c1"># 6. run a multi-player simulation for 3 players!</span>
<span class="nv">N</span><span class="o">=</span><span class="m">1000</span> <span class="nv">T</span><span class="o">=</span><span class="m">10000</span> <span class="nv">M</span><span class="o">=</span><span class="m">3</span> <span class="nv">K</span><span class="o">=</span><span class="m">9</span> <span class="nv">N_JOBS</span><span class="o">=</span><span class="m">4</span> make moremulti
</pre></div>
</div>
</div>
<div class="section" id="Examples-of-illustrations">
<h3>Examples of illustrations<a class="headerlink" href="#Examples-of-illustrations" title="Permalink to this headline">Â¶</a></h3>
<p>The two simulations above produce these plots showing the average cumulated regret [^regret] for each algorithm, which is the reference measure of efficiency for algorithms in the multi-armed bandits framework.</p>
<p>[^regret]:  The regret is the difference between the cumulated rewards of the best fixed-armed strategy (which is the oracle strategy for stationary bandits) and the cumulated rewards of the considered algorithms.</p>
<p><img alt="Single-player simulation showing the regret of $4$ algorithms, and the asymptotic lower-bound from [&#64;LaiRobbins85]. They all perform very well, and at finite time they are empirically  the asymptotic lower-bound. Each algorithm is known to be order-optimal (, its regret is proved to match the lower-bound up-to a constant), and each but UCB is known to be optimal ( with the constant matching the lower-bound).\label{fig:plot1}" src="_images/1.png" />{ width=95% }</p>
<p><img alt="Multi-player simulation showing the regret of $6$ algorithms, and the asymptotic lower-bound from [&#64;BessonALT2018]. The best algorithm is the centralized version, but for decentralized algorithms, our proposals outperform the previous state-of-the-art  policy.\label{fig:plot2}" src="_images/2.png" />{ width=95% }</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="Research-using-SMPyBandits">
<h2>Research using <em>SMPyBandits</em><a class="headerlink" href="#Research-using-SMPyBandits" title="Permalink to this headline">Â¶</a></h2>
<p><em>SMPyBandits</em> was used for the following research articles since $2017$ [^summaryphd]:</p>
<p>[^summaryphd]: <a class="reference external" href="http://perso.crans.org/besson/">I (Lilian Besson)</a> have <a class="reference external" href="http://perso.crans.org/besson/phd/">started my PhD</a> in October $2016$, and this is a part of my <strong>on going</strong> research since December $2016$. I launched the <a class="reference external" href="https://smpybandits.github.io/">documentation</a> on March $2017$, I wrote my first research articles using this framework in $2017$ and I was finally able to open-source my project in February $2018$.</p>
<ul class="simple">
<li>For this first article, [&#64;Bonnefoi17], <em>SMPyBandits</em> was not used to generate the main figures, but to explore on a smaller scale many other approaches (using <a class="reference external" href="https://smpybandits.github.io/docs/Environment.EvaluatorSparseMultiPlayers.html"><code class="docutils literal"><span class="pre">EvaluatorSparseMultiPlayers</span></code></a>).</li>
<li>For [&#64;BessonALT2018], we used <em>SMPyBandits</em> for all the simulations for multi-player bandit algorithms [^article1]. We designed the two <a class="reference external" href="https://smpybandits.github.io/docs/PoliciesMultiPlayers.RandTopM.html"><code class="docutils literal"><span class="pre">RandTopM</span></code></a> and <a class="reference external" href="https://smpybandits.github.io/docs/PoliciesMultiPlayers.MCTopM.html"><code class="docutils literal"><span class="pre">MCTopM</span></code></a> algorithms and proved than they enjoy logarithmic regret in the usual setting, and outperform significantly the previous state-of-the-art solutions (<em>i.e.</em>, <a class="reference external" href="https://smpybandits.github.io/docs/PoliciesMultiPlayers.rhoRand.html"><code class="docutils literal"><span class="pre">rhoRand</span></code></a>, <a class="reference external" href="https://smpybandits.github.io/docs/Policies.MEGA.html"><code class="docutils literal"><span class="pre">MEGA</span></code></a> and <a class="reference external" href="https://smpybandits.github.io/docs/Policies.MusicalChair.html"><code class="docutils literal"><span class="pre">MusicalChair</span></code></a>).</li>
</ul>
<p>[^article1]:  More details and illustrations are given on the documentation page, <a class="reference external" href="https://smpybandits.github.io/MultiPlayers.html"><code class="docutils literal"><span class="pre">MultiPlayers</span></code></a>.</p>
<ul class="simple">
<li>In [&#64;BessonWCNC2018], we used <em>SMPyBandits</em> to illustrate and compare different aggregation algorithms [^article2]. We designed a variant of the Exp3 algorithm for online aggregation of experts [&#64;Bubeck12], called <a class="reference external" href="https://smpybandits.github.io/docs/Policies.Aggregator.html"><code class="docutils literal"><span class="pre">Aggregator</span></code></a>. Aggregating experts is a well-studied idea in sequential learning and in machine learning in general. We showed that it can be used in practice to select on the run the best bandit algorithm for a certain problem from a fixed pool of experts. This idea and algorithm can have interesting impact for Opportunistic Spectrum Access applications [&#64;Jouini09] that use multi-armed bandits algorithms for sequential learning and network efficiency optimization.</li>
</ul>
<p>[^article2]:  More details and illustrations are given on the documentation page, <a class="reference external" href="https://smpybandits.github.io/Aggregation.html"><code class="docutils literal"><span class="pre">Aggregation</span></code></a>.</p>
<ul class="simple">
<li>In [&#64;Besson2018c], we used <em>SMPyBandits</em> to illustrate and compare different &#8220;doubling trick&#8221; schemes [^article3]. In sequential learning, an algorithm is <em>anytime</em> if it does not need to know the horizon $T$ of the experiments. A well-known trick for transforming any non-anytime algorithm to an anytime variant is the &#8220;Doubling Trick&#8221;: start with an horizon $T_0\in\mathbb{N}$, and when $t &gt; T_i$, use $T_{i+1} = 2 T_i$. We studied two generic sequences of growing horizons (geometric and exponential), and we proved two theorems that generalized previous results. A geometric sequence suffices to minimax regret bounds (in $R_T = \mathcal{O}(\sqrt(T))$), with a constant multiplicative loss $\ell \leq 4$, but cannot be used to conserve a logarithmic regret bound (in $R_T = \mathcal{O}(\log(T))$). And an exponential sequence can be used to conserve logarithmic bounds, with a constant multiplicative loss also $\ell \leq 4$ in the usual setting. It is still an open question to know if a well-tuned exponential sequence can conserve minimax bounds or weak minimax bounds (in $R_T = \mathcal{O}(\sqrt{T \log(T)})$).</li>
</ul>
<p>[^article3]:  More details and illustrations are given on the documentation page, <a class="reference external" href="https://smpybandits.github.io/DoublingTrick.html"><code class="docutils literal"><span class="pre">DoublingTrick</span></code></a>.</p>
</div>
<div class="section" id="Dependencies">
<h2>Dependencies<a class="headerlink" href="#Dependencies" title="Permalink to this headline">Â¶</a></h2>
<p>The framework is written in Python [&#64;python], using matplotlib [&#64;matplotlib] for 2D plotting, numpy [&#64;numpy] for data storing, random number generations and and operations on arrays, scipy [&#64;scipy] for statistical and special functions, and seaborn [&#64;seaborn] for pretty plotting and colorblind-aware colormaps.
Optional dependencies include joblib [&#64;joblib] for parallel simulations, numba [&#64;numba] for automatic speed-up on some small functions, as well as sphinx [&#64;sphinx] for generating the documentations.
I also acknowledge the use of virtualenv [&#64;virtualenv] for launching simulations in isolated environments, and jupyter [&#64;jupyter] used with ipython [&#64;ipython] to experiment with the code.</p>
<hr class="docutils" />
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016-2018, Lilian Besson (Naereen).
      Last updated on 01 Mar 2018, 18h.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.9.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>