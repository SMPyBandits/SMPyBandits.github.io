

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <script type="text/javascript">

      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-38514290-2']);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Policy aggregation algorithms &mdash; SMPyBandits 0.9.6 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Multi-players simulation environment" href="MultiPlayers.html" />
    <link rel="prev" title="List of research publications using Lilian Besson‚Äôs SMPyBandits project" href="PublicationsWithSMPyBandits.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> SMPyBandits
          

          
            
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.9
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="README.html"><em>SMPyBandits</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="docs/modules.html">SMPyBandits modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="How_to_run_the_code.html">How to run the code ?</a></li>
<li class="toctree-l1"><a class="reference internal" href="PublicationsWithSMPyBandits.html">List of research publications using Lilian Besson‚Äôs SMPyBandits project</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#"><strong>Policy aggregation algorithms</strong></a><ul>
<li class="toctree-l2"><a class="reference internal" href="#idea">Idea</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mathematical-explanations">Mathematical explanations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ensemble-voting-for-mab-algorithms">Ensemble voting for MAB algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="#my-algorithm-aggregator">My algorithm: Aggregator</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#configuration">Configuration:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-run-the-experiments">How to run the experiments ?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#some-illustrations">Some illustrations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#on-a-simple-bernoulli-problem-semi-log-y-scale">On a ‚Äúsimple‚Äù Bernoulli problem (semi-log-y scale)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#on-a-harder-bernoulli-problem">On a ‚Äúharder‚Äù Bernoulli problem</a></li>
<li class="toctree-l3"><a class="reference internal" href="#on-an-easy-gaussian-problem">On an ‚Äúeasy‚Äù Gaussian problem</a></li>
<li class="toctree-l3"><a class="reference internal" href="#on-a-harder-problem-mixing-bernoulli-gaussian-exponential-arms">On a harder problem, mixing Bernoulli, Gaussian, Exponential arms</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scroll-license-github-license">üìú License ? </a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="MultiPlayers.html"><strong>Multi-players simulation environment</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="DoublingTrick.html"><strong>Doubling Trick for Multi-Armed Bandits</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="SparseBandits.html"><strong>Structure and Sparsity of Stochastic Multi-Armed Bandits</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="NonStationaryBandits.html"><strong>Non-Stationary Stochastic Multi-Armed Bandits</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="API.html">Short documentation of the API</a></li>
<li class="toctree-l1"><a class="reference internal" href="About_parallel_computations.html">About parallel computations</a></li>
<li class="toctree-l1"><a class="reference internal" href="TODO.html">üí• TODO</a></li>
<li class="toctree-l1"><a class="reference internal" href="plots/README.html">Some illustrations for this project</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/README.html">Jupyter Notebooks üìì by Naereen &#64; GitHub</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/list.html">List of notebooks for SMPyBandits</a></li>
<li class="toctree-l1"><a class="reference internal" href="Profiling.html">A note on execution times, speed and profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="uml_diagrams/README.html">UML diagrams</a></li>
<li class="toctree-l1"><a class="reference internal" href="logs/README.html"><code class="docutils literal notranslate"><span class="pre">logs</span></code> files</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">SMPyBandits</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li><strong>Policy aggregation algorithms</strong></li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/Aggregation.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="policy-aggregation-algorithms">
<h1><strong>Policy aggregation algorithms</strong><a class="headerlink" href="#policy-aggregation-algorithms" title="Permalink to this headline">¬∂</a></h1>
<ul class="simple">
<li><p>Remark: I wrote a small research article on that topic, it will be a better introduction as a small self-contained document to explain this idea and the algorithms. Reference: <a class="reference external" href="https://hal.inria.fr/hal-01705292">[Aggregation of Multi-Armed Bandits Learning Algorithms for Opportunistic Spectrum Access, Lilian Besson and Emilie Kaufmann and Christophe Moy, 2017]</a>, presented at the <a class="reference external" href="http://wcnc2018.ieee-wcnc.org/">IEEE WCNC 2018</a> conference.</p></li>
</ul>
<blockquote>
<div><p>PDF : <a class="reference external" href="https://hal.inria.fr/hal-01705292/document">BKM_IEEEWCNC_2018.pdf</a> | HAL notice : <a class="reference external" href="https://hal.inria.fr/hal-01705292/">BKM_IEEEWCNC_2018</a> | BibTeX : <a class="reference external" href="https://hal.inria.fr/hal-01705292/bibtex">BKM_IEEEWCNC_2018.bib</a> | <a class="reference external" href="Aggregation.html">Source code and documentation</a>
<a class="reference external" href="https://hal.inria.fr/hal-01705292"><img alt="Published" src="https://img.shields.io/badge/Published%3F-accepted-green.svg" /></a>  <a class="reference external" href="https://bitbucket.org/lbesson/aggregation-of-multi-armed-bandits-learning-algorithms-for/commits/"><img alt="Maintenance" src="https://img.shields.io/badge/Maintained%3F-finished-green.svg" /></a>  <a class="reference external" href="https://bitbucket.org/lbesson/ama"><img alt="Ask Me Anything !" src="https://img.shields.io/badge/Ask%20me-anything-1abc9c.svg" /></a></p>
</div></blockquote>
<div class="section" id="idea">
<h2>Idea<a class="headerlink" href="#idea" title="Permalink to this headline">¬∂</a></h2>
<p>The basic idea of a policy aggregation algorithm is to run in parallel some online learning algorithms, denoted <code class="docutils literal notranslate"><span class="pre">$A_1,\ldots,A_N$</span></code> (<code class="docutils literal notranslate"><span class="pre">$A_i$</span></code>), and make them all vote at each step, and use some probabilistic scheme to select a decision from their votes.</p>
<p>Hopefully, if all the algorithms <code class="docutils literal notranslate"><span class="pre">$A_i$</span></code> are not too bad and at least one of them is efficient for the problem at hand, the aggregation algorithm will learn to mainly trust the efficient one(s) and discard the votes from the others.
An efficient aggregation algorithm should have performances similar to the best child algorithm <code class="docutils literal notranslate"><span class="pre">$A_i$</span></code>, in any problem.</p>
<p>The <a class="reference external" href="http://sbubeck.com/SurveyBCB12.pdf">Exp4 algorithm</a> by [Auer et al, 2002] is the first aggregation algorithm for online bandit algorithms, and recently other algorithms include <a class="reference external" href="https://smpybandits.github.io/docs/Policies.LearnExp.html"><code class="docutils literal notranslate"><span class="pre">LearnExp</span></code></a> ([<a class="reference external" href="https://arxiv.org/abs/1702.04825">Singla et al, 2017</a>]) and <a class="reference external" href="https://smpybandits.github.io/docs/Policies.CORRAL.html"><code class="docutils literal notranslate"><span class="pre">CORRAL</span></code></a> ([<a class="reference external" href="https://arxiv.org/abs/1612.06246v2">Agarwal et al, 2017</a>]).</p>
<hr class="docutils" />
<div class="section" id="mathematical-explanations">
<h3>Mathematical explanations<a class="headerlink" href="#mathematical-explanations" title="Permalink to this headline">¬∂</a></h3>
<p>Initially, every child algorithms <code class="docutils literal notranslate"><span class="pre">$A_i$</span></code> has the same ‚Äútrust‚Äù probability <code class="docutils literal notranslate"><span class="pre">$p_i$</span></code>, and at every step, the aggregated bandit first listen to the decision from all its children <code class="docutils literal notranslate"><span class="pre">$A_i$</span></code> (<code class="docutils literal notranslate"><span class="pre">$a_{i,t}$</span></code> in <code class="docutils literal notranslate"><span class="pre">$\{1,\ldots,K\}$</span></code>), and then decide which arm to select by a probabilistic vote: the probability of selecting arm <code class="docutils literal notranslate"><span class="pre">$k$</span></code> is the sum of the trust probability of the children who voted for arm <code class="docutils literal notranslate"><span class="pre">$k$</span></code>.
It could also be done the other way: the aggregated bandit could first decide which children to listen to, then trust him.</p>
<p>But we want to update the trust probability of all the children algorithms, not only one, when it was wised to trust them.
Mathematically, when the aggregated arm choose to pull the arm <code class="docutils literal notranslate"><span class="pre">$k$</span></code> at step <code class="docutils literal notranslate"><span class="pre">$t$</span></code>, if it yielded a positive reward <code class="docutils literal notranslate"><span class="pre">$r_{k,t}$</span></code>, then the probability of all children algorithms <code class="docutils literal notranslate"><span class="pre">$A_i$</span></code> who decided (independently) to chose <code class="docutils literal notranslate"><span class="pre">$k$</span></code> (i.e., <code class="docutils literal notranslate"><span class="pre">$a_{i,t}</span> <span class="pre">=</span> <span class="pre">k$</span></code>) are increased multiplicatively: <code class="docutils literal notranslate"><span class="pre">$p_i</span> <span class="pre">\leftarrow</span> <span class="pre">p_i</span> <span class="pre">*</span> <span class="pre">\exp(+</span> <span class="pre">\beta</span> <span class="pre">*</span> <span class="pre">r_{k,t})$</span></code> where <code class="docutils literal notranslate"><span class="pre">$\beta$</span></code> is a positive <em>learning rate</em>, e.g., <code class="docutils literal notranslate"><span class="pre">$\beta</span> <span class="pre">=</span> <span class="pre">0.1$</span></code>.</p>
<p>It is also possible to decrease multiplicatively the trust of all the children algorithms who did not decided to chose the arm <code class="docutils literal notranslate"><span class="pre">$k$</span></code> at every step <code class="docutils literal notranslate"><span class="pre">$t$</span></code>: if <code class="docutils literal notranslate"><span class="pre">$a_{i,t}</span> <span class="pre">\neq</span> <span class="pre">k$</span></code> then <code class="docutils literal notranslate"><span class="pre">$p_i</span> <span class="pre">\leftarrow</span> <span class="pre">p_i</span> <span class="pre">*</span> <span class="pre">\exp(-</span> <span class="pre">\beta</span> <span class="pre">*</span> <span class="pre">r_{k,t})$</span></code>. I did not observe any difference of behavior between these two options (implemented with the Boolean parameter <code class="docutils literal notranslate"><span class="pre">updateAllChildren</span></code>).</p>
</div>
<div class="section" id="ensemble-voting-for-mab-algorithms">
<h3>Ensemble voting for MAB algorithms<a class="headerlink" href="#ensemble-voting-for-mab-algorithms" title="Permalink to this headline">¬∂</a></h3>
<p>This algorithm can be seen as the Multi-Armed Bandits (i.e., sequential reinforcement learning) counterpart of an <em>ensemble voting</em> technique, as used for classifiers or regression algorithm in usual supervised machine learning (see, e.g., <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier"><code class="docutils literal notranslate"><span class="pre">sklearn.ensemble.VotingClassifier</span></code></a> in <a class="reference external" href="http://scikit-learn.org/">scikit-learn</a>).</p>
<p>Another approach could be to do some sort of <a class="reference external" href="http://scikit-learn.org/stable/modules/grid_search.html">grid search</a>.</p>
</div>
<div class="section" id="my-algorithm-aggregator">
<h3>My algorithm: <a class="reference external" href="https://smpybandits.github.io/docs/Policies.Aggregator.html">Aggregator</a><a class="headerlink" href="#my-algorithm-aggregator" title="Permalink to this headline">¬∂</a></h3>
<p>It is based on a modification of Exp4, and the details are given in its documentation, see <a class="reference external" href="https://smpybandits.github.io/docs/Policies.Aggregator.html"><code class="docutils literal notranslate"><span class="pre">Aggregator</span></code></a>.</p>
<p>All the mathematical details can be found in my paper, <a class="reference external" href="https://hal.inria.fr/hal-01705292">[Aggregation of Multi-Armed Bandits Learning Algorithms for Opportunistic Spectrum Access, Lilian Besson and Emilie Kaufmann and Christophe Moy, 2017]</a>, presented at the <a class="reference external" href="http://wcnc2018.ieee-wcnc.org/">IEEE WCNC 2018</a> conference.</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="configuration">
<h2>Configuration:<a class="headerlink" href="#configuration" title="Permalink to this headline">¬∂</a></h2>
<p>A simple python file, <a class="reference external" href="https://smpybandits.github.io/docs/configuration_comparing_aggregation_algorithms.html"><code class="docutils literal notranslate"><span class="pre">configuration_comparing_aggregation_algorithms.py</span></code></a>, is used to import the <a class="reference external" href="Arms/">arm classes</a>, the <a class="reference external" href="Policies/">policy classes</a> and define the problems and the experiments.</p>
<p>For example, this will compare the classical MAB algorithms <a class="reference external" href="https://smpybandits.github.io/docs/Policies.UCB.html"><code class="docutils literal notranslate"><span class="pre">UCB</span></code></a>, <a class="reference external" href="https://smpybandits.github.io/docs/Policies.Thompson.html"><code class="docutils literal notranslate"><span class="pre">Thompson</span></code></a>, <a class="reference external" href="https://smpybandits.github.io/docs/Policies.BayesUCB.html"><code class="docutils literal notranslate"><span class="pre">BayesUCB</span></code></a>, <a class="reference external" href="https://smpybandits.github.io/docs/Policies.klUCB.html"><code class="docutils literal notranslate"><span class="pre">klUCB</span></code></a> algorithms.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">configuration</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;horizon&quot;</span><span class="p">:</span> <span class="mi">10000</span><span class="p">,</span>    <span class="c1"># Finite horizon of the simulation</span>
    <span class="s2">&quot;repetitions&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>  <span class="c1"># number of repetitions</span>
    <span class="s2">&quot;n_jobs&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>        <span class="c1"># Maximum number of cores for parallelization: use ALL your CPU</span>
    <span class="s2">&quot;verbosity&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>      <span class="c1"># Verbosity for the joblib calls</span>
    <span class="c1"># Environment configuration, you can set up more than one.</span>
    <span class="s2">&quot;environment&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;arm_type&quot;</span><span class="p">:</span> <span class="n">Bernoulli</span><span class="p">,</span>  <span class="c1"># Only Bernoulli is available as far as now</span>
            <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">],</span>
    <span class="c1"># Policies that should be simulated, and their parameters.</span>
    <span class="s2">&quot;policies&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;archtype&quot;</span><span class="p">:</span> <span class="n">UCB</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{}</span> <span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;archtype&quot;</span><span class="p">:</span> <span class="n">Thompson</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{}</span> <span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;archtype&quot;</span><span class="p">:</span> <span class="n">klUCB</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{}</span> <span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;archtype&quot;</span><span class="p">:</span> <span class="n">BayesUCB</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{}</span> <span class="p">},</span>
    <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>To add an aggregated bandit algorithm (<a class="reference external" href="https://smpybandits.github.io/docs/Policies.Aggregator.html"><code class="docutils literal notranslate"><span class="pre">Aggregator</span></code> class</a>), you can use this piece of code, to aggregate all the algorithms defined before and dynamically add it to <code class="docutils literal notranslate"><span class="pre">configuration</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">current_policies</span> <span class="o">=</span> <span class="n">configuration</span><span class="p">[</span><span class="s2">&quot;policies&quot;</span><span class="p">]</span>
<span class="n">configuration</span><span class="p">[</span><span class="s2">&quot;policies&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_policies</span> <span class="o">+</span>
    <span class="p">[{</span>  <span class="c1"># Add one Aggregator policy, from all the policies defined above</span>
        <span class="s2">&quot;archtype&quot;</span><span class="p">:</span> <span class="n">Aggregator</span><span class="p">,</span>
        <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;learningRate&quot;</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span>  <span class="c1"># Tweak this if needed</span>
            <span class="s2">&quot;updateAllChildren&quot;</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
            <span class="s2">&quot;children&quot;</span><span class="p">:</span> <span class="n">current_policies</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">}]</span>
</pre></div>
</div>
<p>The learning rate can be tuned automatically, by using the heuristic proposed by [<a class="reference external" href="http://sbubeck.com/SurveyBCB12.pdf">Bubeck and Cesa-Bianchi</a>, Theorem 4.2], without knowledge of the horizon, a decreasing learning rate <code class="docutils literal notranslate"><span class="pre">$\eta_t</span> <span class="pre">=</span> <span class="pre">\sqrt(\frac{\log(N)}{t</span> <span class="pre">*</span> <span class="pre">K})$</span></code>.</p>
</div>
<hr class="docutils" />
<div class="section" id="how-to-run-the-experiments">
<h2><a class="reference internal" href="How_to_run_the_code.html"><span class="doc">How to run the experiments ?</span></a><a class="headerlink" href="#how-to-run-the-experiments" title="Permalink to this headline">¬∂</a></h2>
<p>You should use the provided <a class="reference external" href="Makefile"><code class="docutils literal notranslate"><span class="pre">Makefile</span></code></a> file to do this simply:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># if not already installed, otherwise update with &#39;git pull&#39;</span>
git clone https://github.com/SMPyBandits/SMPyBandits/
<span class="nb">cd</span> SMPyBandits
make install  <span class="c1"># install the requirements ONLY ONCE</span>
make comparing_aggregation_algorithms   <span class="c1"># run and log the main.py script</span>
</pre></div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="some-illustrations">
<h2>Some illustrations<a class="headerlink" href="#some-illustrations" title="Permalink to this headline">¬∂</a></h2>
<p>Here are some plots illustrating the performances of the different <a class="reference external" href="https://smpybandits.github.io/docs/Policies/">policies</a> implemented in this project, against various problems (with <a class="reference external" href="https://smpybandits.github.io/docs/Arms.Bernoulli.html"><code class="docutils literal notranslate"><span class="pre">Bernoulli</span></code></a> arms only):</p>
<div class="section" id="on-a-simple-bernoulli-problem-semi-log-y-scale">
<h3>On a ‚Äúsimple‚Äù Bernoulli problem (semi-log-y scale)<a class="headerlink" href="#on-a-simple-bernoulli-problem-semi-log-y-scale" title="Permalink to this headline">¬∂</a></h3>
<p><img alt="On a &quot;simple&quot; Bernoulli problem (semi-log-y scale)." src="_images/main_semilogy____env1-4_932221613383548446.png" /></p>
<p>Aggregator is the most efficient, and very similar to Exp4 here.</p>
</div>
<div class="section" id="on-a-harder-bernoulli-problem">
<h3>On a ‚Äúharder‚Äù Bernoulli problem<a class="headerlink" href="#on-a-harder-bernoulli-problem" title="Permalink to this headline">¬∂</a></h3>
<p><img alt="On a &quot;harder&quot; Bernoulli problem, they all have similar performances, except LearnExp." src="_images/main____env2-4_932221613383548446.png" /></p>
<p>They all have similar performances, except LearnExp, which performs badly.
We can check that the problem is indeed harder as the lower-bound (in black) is much larger.</p>
</div>
<div class="section" id="on-an-easy-gaussian-problem">
<h3>On an ‚Äúeasy‚Äù Gaussian problem<a class="headerlink" href="#on-an-easy-gaussian-problem" title="Permalink to this headline">¬∂</a></h3>
<p><img alt="On an &quot;easy&quot; Gaussian problem, only Aggregator shows reasonable performances, thanks to BayesUCB and Thompson sampling." src="_images/main____env3-4_932221613383548446.png" /></p>
<p>Only Aggregator shows reasonable performances, thanks to BayesUCB and Thompson sampling.
CORRAL and LearnExp clearly appears sub-efficient.</p>
</div>
<div class="section" id="on-a-harder-problem-mixing-bernoulli-gaussian-exponential-arms">
<h3>On a harder problem, mixing Bernoulli, Gaussian, Exponential arms<a class="headerlink" href="#on-a-harder-problem-mixing-bernoulli-gaussian-exponential-arms" title="Permalink to this headline">¬∂</a></h3>
<p><img alt="On a harder problem, mixing Bernoulli, Gaussian, Exponential arms, with 3 arms of each types with the ." src="_images/main_semilogy____env4-4_932221613383548446.png" /></p>
<p>This problem is much harder as it has 3 arms of each types with the <em>same mean</em>.</p>
<p><img alt="The semi-log-x scale clearly shows the logarithmic growth of the regret for the best algorithms and our proposal Aggregator, even in a hard &quot;mixed&quot; problem." src="_images/main_semilogx____env4-4_932221613383548446.png" /></p>
<p>The semi-log-x scale clearly shows the logarithmic growth of the regret for the best algorithms and our proposal Aggregator, even in a hard ‚Äúmixed‚Äù problem.</p>
<blockquote>
<div><p>These illustrations come from my article, <a class="reference external" href="https://hal.inria.fr/hal-01705292">[Aggregation of Multi-Armed Bandits Learning Algorithms for Opportunistic Spectrum Access, Lilian Besson and Emilie Kaufmann and Christophe Moy, 2017]</a>, presented at the <a class="reference external" href="http://wcnc2018.ieee-wcnc.org/">IEEE WCNC 2018</a> conference.</p>
</div></blockquote>
</div>
<hr class="docutils" />
<div class="section" id="scroll-license-github-license">
<h3>üìú License ? <a class="reference external" href="https://github.com/SMPyBandits/SMPyBandits/blob/master/LICENSE"><img alt="GitHub license" src="https://img.shields.io/github/license/SMPyBandits/SMPyBandits.svg" /></a><a class="headerlink" href="#scroll-license-github-license" title="Permalink to this headline">¬∂</a></h3>
<p><a class="reference external" href="https://lbesson.mit-license.org/">MIT Licensed</a> (file <a class="reference external" href="LICENSE">LICENSE</a>).</p>
<p>¬© 2016-2018 <a class="reference external" href="https://GitHub.com/Naereen">Lilian Besson</a>.</p>
<p><a class="reference external" href="https://github.com/SMPyBandits/SMPyBandits/"><img alt="Open Source? Yes!" src="https://badgen.net/badge/Open%20Source%20%3F/Yes%21/blue?icon=github" /></a>
<a class="reference external" href="https://GitHub.com/SMPyBandits/SMPyBandits/graphs/commit-activity"><img alt="Maintenance" src="https://img.shields.io/badge/Maintained%3F-yes-green.svg" /></a>
<a class="reference external" href="https://GitHub.com/Naereen/ama"><img alt="Ask Me Anything !" src="https://img.shields.io/badge/Ask%20me-anything-1abc9c.svg" /></a>
<a class="reference external" href="https://GitHub.com/SMPyBandits/SMPyBandits/"><img alt="Analytics" src="https://ga-beacon.appspot.com/UA-38514290-17/github.com/SMPyBandits/SMPyBandits/README.md?pixel" /></a>
<img alt="https://pypi.org/project/SMPyBandits" src="https://pypi.org/project/SMPyBandits" /><img alt="PyPI version" src="https://img.shields.io/pypi/v/smpybandits.svg" />
<img alt="https://pypi.org/project/SMPyBandits" src="https://pypi.org/project/SMPyBandits" /><img alt="PyPI implementation" src="https://img.shields.io/pypi/implementation/smpybandits.svg" />
<a class="reference external" href="https://pypi.org/project/SMPyBandits"><img alt="https://pypi.org/project/SMPyBandits" src="https://pypi.org/project/SMPyBandits" /><img alt="PyPI pyversions" src="https://img.shields.io/pypi/pyversions/smpybandits.svg?logo=python" />
</a>
<a class="reference external" href="https://pypi.org/project/SMPyBandits"><img alt="https://pypi.org/project/SMPyBandits" src="https://pypi.org/project/SMPyBandits" /><img alt="PyPI download" src="https://img.shields.io/pypi/dm/smpybandits.svg" /></a>
<a class="reference external" href="https://pypi.org/project/SMPyBandits"><img alt="https://pypi.org/project/SMPyBandits" src="https://pypi.org/project/SMPyBandits" /><img alt="PyPI status" src="https://img.shields.io/pypi/status/smpybandits.svg" /></a>
<a class="reference external" href="https://SMPyBandits.ReadTheDocs.io/en/latest/?badge=latest"><img alt="Documentation Status" src="https://readthedocs.org/projects/smpybandits/badge/?version=latest" /></a>
<a class="reference external" href="https://travis-ci.org/SMPyBandits/SMPyBandits"><img alt="Build Status" src="https://travis-ci.org/SMPyBandits/SMPyBandits.svg?branch=master" /></a>
<a class="reference external" href="https://GitHub.com/SMPyBandits/SMPyBandits/stargazers"><img alt="Stars of https://github.com/SMPyBandits/SMPyBandits/" src="https://badgen.net/github/stars/SMPyBandits/SMPyBandits" /></a>
<a class="reference external" href="https://github.com/SMPyBandits/SMPyBandits/releases"><img alt="Releases of https://github.com/SMPyBandits/SMPyBandits/" src="https://badgen.net/github/release/SMPyBandits/SMPyBandits" /></a></p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="MultiPlayers.html" class="btn btn-neutral float-right" title="Multi-players simulation environment" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="PublicationsWithSMPyBandits.html" class="btn btn-neutral float-left" title="List of research publications using Lilian Besson‚Äôs SMPyBandits project" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016-2018, Lilian Besson (Naereen)
      <span class="lastupdated">
        Last updated on 25 Feb 2020, 14h.
      </span>

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>