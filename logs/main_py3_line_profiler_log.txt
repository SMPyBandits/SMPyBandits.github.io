Timer unit: 1e-06 s

Total time: 12.9948 s
File: ./Environment/Evaluator.py
Function: start_one_env at line 66

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    66                                               @profile  # DEBUG with kernprof (cf. https://github.com/rkern/line_profiler#kernprof
    67                                               def start_one_env(self, envId, env):
    68         1           49     49.0      0.0              print("\nEvaluating environment:", repr(env))
    69         1            2      2.0      0.0              self.policies = []
    70         1         1205   1205.0      0.0              self.__initPolicies__(env)
    71                                                       # if self.useJoblibForPolicies:
    72                                                       #     n_jobs = len(self.policies)
    73                                                       #     joblib.Parallel(n_jobs=n_jobs, verbose=self.cfg['verbosity'])(
    74                                                       #         joblib.delayed(delayed_start)(self, env, policy, polId, envId)
    75                                                       #         for polId, policy in enumerate(self.policies)
    76                                                       #     )
    77                                                       # else:
    78                                                       #     for polId, policy in enumerate(self.policies):
    79                                                       #         delayed_start(self, env, policy, polId, envId)
    80                                                       # # FIXME try to also parallelize this loop on policies ?
    81         8           15      1.9      0.0              for polId, policy in enumerate(self.policies):
    82         7          180     25.7      0.0                  print("\n- Evaluating policy #{}/{}: {} ...".format(polId + 1, len(self.policies), policy))
    83         7            9      1.3      0.0                  if self.useJoblib:
    84                                                               results = joblib.Parallel(n_jobs=self.cfg['n_jobs'], verbose=self.cfg['verbosity'])(
    85                                                                   joblib.delayed(delayed_play)(env, policy, self.cfg['horizon'])
    86                                                                   for _ in range(self.cfg['repetitions'])
    87                                                               )
    88                                                           else:
    89         7            7      1.0      0.0                      results = []
    90        14           25      1.8      0.0                      for _ in range(self.cfg['repetitions']):
    91         7     12992543 1856077.6    100.0                          r = delayed_play(env, policy, self.cfg['horizon'])
    92         7           14      2.0      0.0                          results.append(r)
    93        14           14      1.0      0.0                  for r in results:
    94         7          679     97.0      0.0                      self.rewards[polId, envId, :] += np.cumsum(r.rewards)
    95         7           27      3.9      0.0                      self.pulls[envId][polId, :] += r.pulls

Total time: 12.8526 s
File: ./Environment/Evaluator.py
Function: delayed_play at line 174

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   174                                           @profile  # DEBUG with kernprof (cf. https://github.com/rkern/line_profiler#kernprof
   175                                           def delayed_play(env, policy, horizon):
   176                                               # We have to deepcopy because this function is Parallel-ized
   177         7         2542    363.1      0.0      env = deepcopy(env)
   178         7         4067    581.0      0.0      policy = deepcopy(policy)
   179         7           63      9.0      0.0      horizon = deepcopy(horizon)
   180                                           
   181         7          257     36.7      0.0      policy.startGame()
   182         7          227     32.4      0.0      result = Result(env.nbArms, horizon)
   183     70007        38750      0.6      0.3      for t in range(horizon):
   184     70000     11059967    158.0     86.1          choice = policy.choice()
   185     70000       162318      2.3      1.3          reward = env.arms[choice].draw(t)
   186     70000      1375782     19.7     10.7          policy.getReward(choice, reward)
   187     70000       208635      3.0      1.6          result.store(t, choice, reward)
   188         7            3      0.4      0.0      return result

Total time: 0.2007 s
File: ./Policies/AdBandits.py
Function: getReward at line 43

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    43                                               @profile  # DEBUG with kernprof (cf. https://github.com/rkern/line_profiler#kernprof)
    44                                               def getReward(self, arm, reward):
    45     40000        80722      2.0     40.2          self.posterior[arm].update(reward)
    46     40000        50366      1.3     25.1          self.rewards[arm] += reward
    47     40000        46085      1.2     23.0          self.pulls[arm] += 1
    48     40000        23527      0.6     11.7          self.t += 1

Total time: 4.52687 s
File: ./Policies/AdBandits.py
Function: choice at line 53

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    53                                               @profile  # DEBUG with kernprof (cf. https://github.com/rkern/line_profiler#kernprof)
    54                                               def choice(self):
    55                                                   # Thompson Exploration
    56     40000        66053      1.7      1.5          if rn.random() > 1.0 * self.t / (self.horizon * self.alpha):
    57                                                       # XXX if possible, this part should also use numpy arrays to be faster?
    58     62250       129797      2.1      2.9              upperbounds = [self.computeIndex(i) for i in range(self.nbArms)]
    59      6225         8095      1.3      0.2              maxIndex = max(upperbounds)
    60     62250        46982      0.8      1.0              bestArms = [arm for (arm, index) in enumerate(upperbounds) if index == maxIndex]
    61      6225        13628      2.2      0.3              arm = rn.choice(bestArms)
    62                                                   # UCB-Bayes
    63                                                   else:
    64     33775       200483      5.9      4.4              expectations = (1.0 + self.rewards) / (2.0 + self.pulls)
    65    337750      2910724      8.6     64.3              upperbounds = [self.posterior[arm].quantile(1. - 1. / self.t) for arm in range(self.nbArms)]
    66     33775       671501     19.9     14.8              regret = np.max(upperbounds) - expectations
    67     33775       224370      6.6      5.0              remin = np.min(regret)
    68     33775       132376      3.9      2.9              admissible = np.where(regret == remin)[0]
    69     33775       101821      3.0      2.2              arm = rn.choice(admissible)
    70     40000        21036      0.5      0.5          return arm

Total time: 0.000299 s
File: ./Policies/Aggr.py
Function: startGame at line 69

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    69                                               @profile  # DEBUG with kernprof (cf. https://github.com/rkern/line_profiler#kernprof)
    70                                               def startGame(self):
    71         2            2      1.0      0.7          self.t = 0
    72                                                   # self.rewards = np.zeros(self.nbArms)
    73                                                   # self.pulls = np.zeros(self.nbArms)
    74                                                   # Start all child children
    75         2            2      1.0      0.7          if self.USE_JOBLIB:
    76                                                       # FIXME the parallelization here was not improving anything
    77                                                       joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbosity)(
    78                                                           joblib.delayed(delayed_startGame)(self, i)
    79                                                           for i in range(self.nbChildren)
    80                                                       )
    81                                                   else:
    82        14           15      1.1      5.0              for i in range(self.nbChildren):
    83        12          243     20.2     81.3                  self.children[i].startGame()
    84         2           37     18.5     12.4          self.choices = (-1) * np.ones(self.nbChildren, dtype=int)

Total time: 0.844994 s
File: ./Policies/Aggr.py
Function: getReward at line 86

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    86                                               @profile  # DEBUG with kernprof (cf. https://github.com/rkern/line_profiler#kernprof)
    87                                               def getReward(self, arm, reward):
    88     10000        15530      1.6      1.8          self.t += 1
    89                                                   # self.rewards[arm] += reward
    90                                                   # self.pulls[arm] += 1
    91                                                   # FIXME I am trying to reduce the learning rate (geometrically) when t increase...
    92     10000         8341      0.8      1.0          if self.decreaseRate is not None:
    93     10000        46237      4.6      5.5              learningRate = self.learningRate * np.exp(- self.t / self.decreaseRate)
    94                                                   else:
    95                                                       learningRate = self.learningRate
    96     10000         7966      0.8      0.9          if self.USE_JOBLIB:
    97                                                       # FIXME the parallelization here was not improving anything
    98                                                       joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbosity)(
    99                                                           joblib.delayed(delayed_getReward)(self, arm, reward, i)
   100                                                           for i in range(self.nbChildren)
   101                                                       )
   102                                                   else:
   103                                                       # Give reward to all child children
   104     70000        61497      0.9      7.3              for i in range(self.nbChildren):
   105     60000       351723      5.9     41.6                  self.children[i].getReward(arm, reward)
   106                                                   # FIXED do this with numpy arrays instead ! FIXME try it !
   107     10000        28902      2.9      3.4          scalingConstant = np.exp(reward * learningRate)
   108     10000        91004      9.1     10.8          self.trusts[self.choices == arm] *= scalingConstant
   109                                                   # for i in range(self.nbChildren):
   110                                                   #     if self.choices[i] == arm:  # this child's choice was chosen
   111                                                   #         # 3. increase self.trusts for the children who were true
   112                                                   #         self.trusts[i] *= np.exp(reward * learningRate)
   113                                                   # FIXED do this with numpy arrays instead ! FIXME try it !
   114                                                   # DONE test both, by changing the option self.update_all_children
   115     10000        12008      1.2      1.4          if self.update_all_children:
   116     10000        62852      6.3      7.4              self.trusts[self.choices != arm] /= scalingConstant
   117                                                       # for i in range(self.nbChildren):
   118                                                       #     if self.choices[i] != arm:  # this child's choice was not chosen
   119                                                       #         # 3. XXX decrease self.trusts for the children who were wrong
   120                                                       #         self.trusts[i] *= np.exp(- reward * learningRate)
   121                                                   # 4. renormalize self.trusts to make it a proba dist
   122                                                   # In practice, it also decreases the self.trusts for the children who were wrong
   123                                                   # print("  The most trusted child policy is the {}th with confidence {}.".format(1 + np.argmax(self.trusts), np.max(self.trusts)))  # DEBUG
   124     10000       158934     15.9     18.8          self.trusts = self.trusts / np.sum(self.trusts)

Total time: 6.14311 s
File: ./Policies/Aggr.py
Function: choice at line 127

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   127                                               @profile  # DEBUG with kernprof (cf. https://github.com/rkern/line_profiler#kernprof)
   128                                               def choice(self):
   129                                                   # 1. make vote every child children
   130     10000         7653      0.8      0.1          if self.USE_JOBLIB:
   131                                                       # FIXME the parallelization here was not improving anything
   132                                                       joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbosity)(
   133                                                           joblib.delayed(delayed_choice)(self, i)
   134                                                           for i in range(self.nbChildren)
   135                                                       )
   136                                                   else:
   137     70000        69429      1.0      1.1              for i in range(self.nbChildren):
   138     60000      5651674     94.2     92.0                  self.choices[i] = self.children[i].choice()
   139                                                       # ? we could be faster here, first sample according to self.trusts, then make it decide
   140                                                       # XXX in fact, no we need to vector self.choices to update the self.trusts probabilities!
   141                                                   # print("self.choices =", self.choices)  # DEBUG
   142                                                   # 2. select the vote to trust, randomly
   143     10000       414354     41.4      6.7          return rn.choice(self.choices, p=self.trusts)

