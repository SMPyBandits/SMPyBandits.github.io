Reading argument from command line, importing the configuration module from arg = configuration_nonstationary (module = configuration_nonstationary in directory )...

Using Upsilon_T = 4 break-points (time when at least one arm changes), and C_T = 4 change-points (number of changes of all arms).
Loaded experiments configuration from 'configuration_nonstationnary.py' :
configuration['policies'] = [{'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}, 'change_label': 'klUCB'}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}, 'change_label': 'Thompson Sampling'}, {'archtype': <class 'Policies.OracleSequentiallyRestartPolicy.OracleSequentiallyRestartPolicy'>, 'params': {'changePoints': [0, 1000, 2000, 3000, 4000], 'listOfMeans': [[0.3, 0.5, 0.9], [0.3, 0.2, 0.9], [0.3, 0.2, 0.1], [0.7, 0.2, 0.1], [0.7, 0.5, 0.1]], 'policy': <class 'Policies.klUCB.klUCB'>, 'reset_for_all_change': True, 'reset_for_suboptimal_change': False}}, {'archtype': <class 'Policies.DiscountedUCB.DiscountedklUCB'>, 'params': {'gamma': 0.95}}, {'archtype': <class 'Policies.SlidingWindowUCB.SWklUCB'>, 'params': {'tau': 206}, 'change_label': 'SW-klUCB'}, {'archtype': <class 'Policies.DiscountedThompson.DiscountedThompson'>, 'params': {'posterior': <class 'Policies.Posterior.DiscountedBeta.DiscountedBeta'>, 'gamma': 0.95}, 'change_label': 'DTS'}, {'archtype': <class 'Policies.Monitored_UCB.Monitored_IndexPolicy'>, 'params': {'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True, 'horizon': 5000, 'w': 150}, 'change_label': 'M-klUCB'}, {'archtype': <class 'Policies.CUSUM_UCB.CUSUM_IndexPolicy'>, 'params': {'horizon': 5000, 'policy': <class 'Policies.klUCB.klUCB'>, 'max_nb_random_events': 4, 'lazy_detect_change_only_x_steps': 20}, 'change_label': 'CUSUM-klUCB'}, {'archtype': <class 'Policies.GLR_UCB.BernoulliGLR_IndexPolicy_WithDeterministicExploration'>, 'params': {'policy': <class 'Policies.klUCB_forGLR.klUCB_forGLR'>, 'per_arm_restart': True, 'delta': 0.00408248290463863, 'alpha0': 0.007148647364956354, 'lazy_detect_change_only_x_steps': 10, 'lazy_try_value_s_only_x_steps': 10}, 'change_label': 'GLR-klUCB(Local)'}, {'archtype': <class 'Policies.GLR_UCB.BernoulliGLR_IndexPolicy_WithDeterministicExploration'>, 'params': {'policy': <class 'Policies.klUCB_forGLR.klUCB_forGLR'>, 'per_arm_restart': False, 'delta': 0.007071067811865475, 'alpha0': 0.00412727348049926, 'lazy_detect_change_only_x_steps': 10, 'lazy_try_value_s_only_x_steps': 10}, 'change_label': 'GLR-klUCB(Global)'}]
configuration['environment'] = [{'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.3, 0.5, 0.9], [0.3, 0.2, 0.9], [0.3, 0.2, 0.1], [0.7, 0.2, 0.1], [0.7, 0.5, 0.1]], 'changePoints': [0, 1000, 2000, 3000, 4000]}}]
====> TURNING DEBUG MODE ON <=====
plots/ is already a directory here...
Number of policies in this comparison: 10
Time horizon: 5000
Number of repetitions: 40
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: 4
Using this dictionary to create a new environment:
 {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.3, 0.5, 0.9], [0.3, 0.2, 0.9], [0.3, 0.2, 0.1], [0.7, 0.2, 0.1], [0.7, 0.5, 0.1]], 'changePoints': [0, 1000, 2000, 3000, 4000]}}
  Special MAB problem, with arm (possibly) changing at every time step, read from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.3, 0.5, 0.9], [0.3, 0.2, 0.9], [0.3, 0.2, 0.1], [0.7, 0.2, 0.1], [0.7, 0.5, 0.1]], 'changePoints': [0, 1000, 2000, 3000, 4000]}} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = {'listOfMeans': [[0.3, 0.5, 0.9], [0.3, 0.2, 0.9], [0.3, 0.2, 0.1], [0.7, 0.2, 0.1], [0.7, 0.5, 0.1]], 'changePoints': [0, 1000, 2000, 3000, 4000]}
 - with 'listOfMeans' = [[0.3 0.5 0.9]
 [0.3 0.2 0.9]
 [0.3 0.2 0.1]
 [0.7 0.2 0.1]
 [0.7 0.5 0.1]]
 - with 'changePoints' = [0, 1000, 2000, 3000, 4000]


 ==> Creating the dynamic arms ...
   - with 'nbArms' = 3
   - with 'arms' = [B(0.3), B(0.5), B(0.9)]
 - Initial draw of 'means' = [0.3 0.5 0.9]
Number of environments to try: 1


Evaluating environment: PieceWiseStationaryMAB(nbArms: 3, arms: [B(0.3), B(0.5), B(0.9)])
- Adding policy #1 = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}, 'change_label': 'klUCB'} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}, 'change_label': 'klUCB'} ...
- Adding policy #2 = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}, 'change_label': 'Thompson Sampling'} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}, 'change_label': 'Thompson Sampling'} ...
- Adding policy #3 = {'archtype': <class 'Policies.OracleSequentiallyRestartPolicy.OracleSequentiallyRestartPolicy'>, 'params': {'changePoints': [0, 1000, 2000, 3000, 4000], 'listOfMeans': [[0.3, 0.5, 0.9], [0.3, 0.2, 0.9], [0.3, 0.2, 0.1], [0.7, 0.2, 0.1], [0.7, 0.5, 0.1]], 'policy': <class 'Policies.klUCB.klUCB'>, 'reset_for_all_change': True, 'reset_for_suboptimal_change': False}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'archtype': <class 'Policies.OracleSequentiallyRestartPolicy.OracleSequentiallyRestartPolicy'>, 'params': {'changePoints': [0, 1000, 2000, 3000, 4000], 'listOfMeans': [[0.3, 0.5, 0.9], [0.3, 0.2, 0.9], [0.3, 0.2, 0.1], [0.7, 0.2, 0.1], [0.7, 0.5, 0.1]], 'policy': <class 'Policies.klUCB.klUCB'>, 'reset_for_all_change': True, 'reset_for_suboptimal_change': False}} ...
Info: creating a new policy Oracle-klUCB, with change points = [[1000, 2000, 3000, 4000], [1000, 2000, 3000, 4000], [1000, 2000, 3000, 4000]]...
- Adding policy #4 = {'archtype': <class 'Policies.DiscountedUCB.DiscountedklUCB'>, 'params': {'gamma': 0.95}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'archtype': <class 'Policies.DiscountedUCB.DiscountedklUCB'>, 'params': {'gamma': 0.95}} ...
- Adding policy #5 = {'archtype': <class 'Policies.SlidingWindowUCB.SWklUCB'>, 'params': {'tau': 206}, 'change_label': 'SW-klUCB'} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][4]' = {'archtype': <class 'Policies.SlidingWindowUCB.SWklUCB'>, 'params': {'tau': 206}, 'change_label': 'SW-klUCB'} ...
- Adding policy #6 = {'archtype': <class 'Policies.DiscountedThompson.DiscountedThompson'>, 'params': {'posterior': <class 'Policies.Posterior.DiscountedBeta.DiscountedBeta'>, 'gamma': 0.95}, 'change_label': 'DTS'} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][5]' = {'archtype': <class 'Policies.DiscountedThompson.DiscountedThompson'>, 'params': {'posterior': <class 'Policies.Posterior.DiscountedBeta.DiscountedBeta'>, 'gamma': 0.95}, 'change_label': 'DTS'} ...
- Adding policy #7 = {'archtype': <class 'Policies.Monitored_UCB.Monitored_IndexPolicy'>, 'params': {'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True, 'horizon': 5000, 'w': 150}, 'change_label': 'M-klUCB'} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][6]' = {'archtype': <class 'Policies.Monitored_UCB.Monitored_IndexPolicy'>, 'params': {'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True, 'horizon': 5000, 'w': 150}, 'change_label': 'M-klUCB'} ...
- Adding policy #8 = {'archtype': <class 'Policies.CUSUM_UCB.CUSUM_IndexPolicy'>, 'params': {'horizon': 5000, 'policy': <class 'Policies.klUCB.klUCB'>, 'max_nb_random_events': 4, 'lazy_detect_change_only_x_steps': 20}, 'change_label': 'CUSUM-klUCB'} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][7]' = {'archtype': <class 'Policies.CUSUM_UCB.CUSUM_IndexPolicy'>, 'params': {'horizon': 5000, 'policy': <class 'Policies.klUCB.klUCB'>, 'max_nb_random_events': 4, 'lazy_detect_change_only_x_steps': 20}, 'change_label': 'CUSUM-klUCB'} ...
- Adding policy #9 = {'archtype': <class 'Policies.GLR_UCB.BernoulliGLR_IndexPolicy_WithDeterministicExploration'>, 'params': {'policy': <class 'Policies.klUCB_forGLR.klUCB_forGLR'>, 'per_arm_restart': True, 'delta': 0.00408248290463863, 'alpha0': 0.007148647364956354, 'lazy_detect_change_only_x_steps': 10, 'lazy_try_value_s_only_x_steps': 10}, 'change_label': 'GLR-klUCB(Local)'} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][8]' = {'archtype': <class 'Policies.GLR_UCB.BernoulliGLR_IndexPolicy_WithDeterministicExploration'>, 'params': {'policy': <class 'Policies.klUCB_forGLR.klUCB_forGLR'>, 'per_arm_restart': True, 'delta': 0.00408248290463863, 'alpha0': 0.007148647364956354, 'lazy_detect_change_only_x_steps': 10, 'lazy_try_value_s_only_x_steps': 10}, 'change_label': 'GLR-klUCB(Local)'} ...
- Adding policy #10 = {'archtype': <class 'Policies.GLR_UCB.BernoulliGLR_IndexPolicy_WithDeterministicExploration'>, 'params': {'policy': <class 'Policies.klUCB_forGLR.klUCB_forGLR'>, 'per_arm_restart': False, 'delta': 0.007071067811865475, 'alpha0': 0.00412727348049926, 'lazy_detect_change_only_x_steps': 10, 'lazy_try_value_s_only_x_steps': 10}, 'change_label': 'GLR-klUCB(Global)'} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][9]' = {'archtype': <class 'Policies.GLR_UCB.BernoulliGLR_IndexPolicy_WithDeterministicExploration'>, 'params': {'policy': <class 'Policies.klUCB_forGLR.klUCB_forGLR'>, 'per_arm_restart': False, 'delta': 0.007071067811865475, 'alpha0': 0.00412727348049926, 'lazy_detect_change_only_x_steps': 10, 'lazy_try_value_s_only_x_steps': 10}, 'change_label': 'GLR-klUCB(Global)'} ...



- Evaluating policy #1/10: kl-UCB ...

New means vector = [0.3 0.5 0.9], best arm(s) = [2], at time t = 0 ...

New means vector = [0.3 0.2 0.9], best arm(s) = [2], at time t = 1000 ...

New means vector = [0.3 0.2 0.1], best arm(s) = [0], at time t = 2000 ...

New means vector = [0.7 0.2 0.1], best arm(s) = [0], at time t = 3000 ...

New means vector = [0.7 0.5 0.1], best arm(s) = [0], at time t = 4000 ...

Estimated order by the policy kl-UCB after 5000 steps: [1 2 0] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 38.89% (relative success)...



- Evaluating policy #2/10: Thompson ...

New means vector = [0.3 0.5 0.9], best arm(s) = [2], at time t = 0 ...

New means vector = [0.3 0.2 0.9], best arm(s) = [2], at time t = 1000 ...

New means vector = [0.3 0.2 0.1], best arm(s) = [0], at time t = 2000 ...

New means vector = [0.7 0.2 0.1], best arm(s) = [0], at time t = 3000 ...

New means vector = [0.7 0.5 0.1], best arm(s) = [0], at time t = 4000 ...

Estimated order by the policy Thompson after 5000 steps: [1 2 0] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 38.89% (relative success)...



- Evaluating policy #3/10: Oracle-klUCB ...

New means vector = [0.3 0.5 0.9], best arm(s) = [2], at time t = 0 ...

New means vector = [0.3 0.2 0.9], best arm(s) = [2], at time t = 1000 ...

New means vector = [0.3 0.2 0.1], best arm(s) = [0], at time t = 2000 ...

New means vector = [0.7 0.2 0.1], best arm(s) = [0], at time t = 3000 ...

New means vector = [0.7 0.5 0.1], best arm(s) = [0], at time t = 4000 ...

Estimated order by the policy Oracle-klUCB after 5000 steps: [1 2 0] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 38.89% (relative success)...



- Evaluating policy #4/10: D-klUCB($\gamma=0.95$) ...

New means vector = [0.3 0.5 0.9], best arm(s) = [2], at time t = 0 ...

New means vector = [0.3 0.2 0.9], best arm(s) = [2], at time t = 1000 ...

New means vector = [0.3 0.2 0.1], best arm(s) = [0], at time t = 2000 ...

New means vector = [0.7 0.2 0.1], best arm(s) = [0], at time t = 3000 ...

New means vector = [0.7 0.5 0.1], best arm(s) = [0], at time t = 4000 ...

Estimated order by the policy D-klUCB($\gamma=0.95$) after 5000 steps: [0 1 2] ...
  ==> Optimal arm identification: 14.29% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #5/10: SW-klUCB($\tau=206$) ...

New means vector = [0.3 0.5 0.9], best arm(s) = [2], at time t = 0 ...

New means vector = [0.3 0.2 0.9], best arm(s) = [2], at time t = 1000 ...

New means vector = [0.3 0.2 0.1], best arm(s) = [0], at time t = 2000 ...

New means vector = [0.7 0.2 0.1], best arm(s) = [0], at time t = 3000 ...

New means vector = [0.7 0.5 0.1], best arm(s) = [0], at time t = 4000 ...

Estimated order by the policy SW-klUCB($\tau=206$) after 5000 steps: [1 2 0] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 38.89% (relative success)...



- Evaluating policy #6/10: DiscountedThompson($\gamma=0.95$) ...

New means vector = [0.3 0.5 0.9], best arm(s) = [2], at time t = 0 ...

New means vector = [0.3 0.2 0.9], best arm(s) = [2], at time t = 1000 ...

New means vector = [0.3 0.2 0.1], best arm(s) = [0], at time t = 2000 ...

New means vector = [0.7 0.2 0.1], best arm(s) = [0], at time t = 3000 ...

New means vector = [0.7 0.5 0.1], best arm(s) = [0], at time t = 4000 ...

Estimated order by the policy DiscountedThompson($\gamma=0.95$) after 5000 steps: [2 1 0] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 22.22% (relative success)...



- Evaluating policy #7/10: M-klUCB($w=150$) ...

New means vector = [0.3 0.5 0.9], best arm(s) = [2], at time t = 0 ...

New means vector = [0.3 0.2 0.9], best arm(s) = [2], at time t = 1000 ...

New means vector = [0.3 0.2 0.1], best arm(s) = [0], at time t = 2000 ...

New means vector = [0.7 0.2 0.1], best arm(s) = [0], at time t = 3000 ...

New means vector = [0.7 0.5 0.1], best arm(s) = [0], at time t = 4000 ...

Estimated order by the policy M-klUCB($w=150$) after 5000 steps: [2 1 0] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 22.22% (relative success)...



- Evaluating policy #8/10: CUSUM-klUCB(lazy detect 20) ...

New means vector = [0.3 0.5 0.9], best arm(s) = [2], at time t = 0 ...

New means vector = [0.3 0.2 0.9], best arm(s) = [2], at time t = 1000 ...

New means vector = [0.3 0.2 0.1], best arm(s) = [0], at time t = 2000 ...

New means vector = [0.7 0.2 0.1], best arm(s) = [0], at time t = 3000 ...

New means vector = [0.7 0.5 0.1], best arm(s) = [0], at time t = 4000 ...

Estimated order by the policy CUSUM-klUCB(lazy detect 20) after 5000 steps: [2 1 0] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 22.22% (relative success)...



- Evaluating policy #9/10: GLR-klUCB_forGLR(Local) ...

New means vector = [0.3 0.5 0.9], best arm(s) = [2], at time t = 0 ...

New means vector = [0.3 0.2 0.9], best arm(s) = [2], at time t = 1000 ...

New means vector = [0.3 0.2 0.1], best arm(s) = [0], at time t = 2000 ...

New means vector = [0.7 0.2 0.1], best arm(s) = [0], at time t = 3000 ...

New means vector = [0.7 0.5 0.1], best arm(s) = [0], at time t = 4000 ...

Estimated order by the policy GLR-klUCB_forGLR(Local) after 5000 steps: [1 2 0] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 38.89% (relative success)...



- Evaluating policy #10/10: GLR-klUCB_forGLR(Global) ...

New means vector = [0.3 0.5 0.9], best arm(s) = [2], at time t = 0 ...

New means vector = [0.3 0.2 0.9], best arm(s) = [2], at time t = 1000 ...

New means vector = [0.3 0.2 0.1], best arm(s) = [0], at time t = 2000 ...

New means vector = [0.7 0.2 0.1], best arm(s) = [0], at time t = 3000 ...

New means vector = [0.7 0.5 0.1], best arm(s) = [0], at time t = 4000 ...

Estimated order by the policy GLR-klUCB_forGLR(Global) after 5000 steps: [1 2 0] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 38.89% (relative success)...

Giving the vector of final regrets ...

  For policy #0 called 'klUCB' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 159
Mean of   last regrets R_T = 263
Median of last regrets R_T = 242
Max of    last regrets R_T = 483
Standard deviation     R_T = 78.3
263 \pm 78

  For policy #1 called 'Thompson Sampling' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 239
Mean of   last regrets R_T = 444
Median of last regrets R_T = 443
Max of    last regrets R_T = 749
Standard deviation     R_T = 106
444 \pm 106

  For policy #2 called 'Oracle-klUCB' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = -16
Mean of   last regrets R_T = 33
Median of last regrets R_T = 33
Max of    last regrets R_T = 84
Standard deviation     R_T = 26.3
33 \pm 26

  For policy #3 called 'D-klUCB($\gamma=0.95$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = -31
Mean of   last regrets R_T = 1.31e+03
Median of last regrets R_T = 1.4e+03
Max of    last regrets R_T = 2.39e+03
Standard deviation     R_T = 441
1315 \pm 441

  For policy #4 called 'SW-klUCB' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 102
Mean of   last regrets R_T = 182
Median of last regrets R_T = 175
Max of    last regrets R_T = 264
Standard deviation     R_T = 31.4
182 \pm 31

  For policy #5 called 'DTS' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 145
Mean of   last regrets R_T = 221
Median of last regrets R_T = 222
Max of    last regrets R_T = 324
Standard deviation     R_T = 46.3
221 \pm 46

  For policy #6 called 'M-klUCB' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 186
Mean of   last regrets R_T = 284
Median of last regrets R_T = 287
Max of    last regrets R_T = 346
Standard deviation     R_T = 34.3
284 \pm 34

  For policy #7 called 'CUSUM-klUCB' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 71
Mean of   last regrets R_T = 149
Median of last regrets R_T = 151
Max of    last regrets R_T = 214
Standard deviation     R_T = 32.8
149 \pm 33

  For policy #8 called 'GLR-klUCB(Local)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 17
Mean of   last regrets R_T = 66.3
Median of last regrets R_T = 65.5
Max of    last regrets R_T = 107
Standard deviation     R_T = 21.9
66 \pm 22

  For policy #9 called 'GLR-klUCB(Global)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 46
Mean of   last regrets R_T = 94.2
Median of last regrets R_T = 91.5
Max of    last regrets R_T = 180
Standard deviation     R_T = 28.5
94 \pm 28

Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'Oracle-klUCB'	was ranked	1 / 10 for this simulation (last regret = 34.34).
- Policy 'GLR-klUCB(Local)'	was ranked	2 / 10 for this simulation (last regret = 70.505).
- Policy 'GLR-klUCB(Global)'	was ranked	3 / 10 for this simulation (last regret = 99.49).
- Policy 'CUSUM-klUCB'	was ranked	4 / 10 for this simulation (last regret = 150.8).
- Policy 'SW-klUCB'	was ranked	5 / 10 for this simulation (last regret = 175.42).
- Policy 'DTS'	was ranked	6 / 10 for this simulation (last regret = 210.65).
- Policy 'klUCB'	was ranked	7 / 10 for this simulation (last regret = 262.92).
- Policy 'M-klUCB'	was ranked	8 / 10 for this simulation (last regret = 286.19).
- Policy 'Thompson Sampling'	was ranked	9 / 10 for this simulation (last regret = 445.87).
- Policy 'D-klUCB($\gamma=0.95$)'	was ranked	10 / 10 for this simulation (last regret = 1297.2).

Giving the mean and std running times ...

For policy #1 called 'Thompson Sampling' ...
    247 ms ± 10.8 ms per loop (mean ± std. dev. of 40 run)

For policy #5 called 'DTS' ...
    284 ms ± 12.4 ms per loop (mean ± std. dev. of 40 run)

For policy #4 called 'SW-klUCB' ...
    518 ms ± 57.1 ms per loop (mean ± std. dev. of 40 run)

For policy #0 called 'klUCB' ...
    583 ms ± 16.9 ms per loop (mean ± std. dev. of 40 run)

For policy #3 called 'D-klUCB($\gamma=0.95$)' ...
    591 ms ± 21.7 ms per loop (mean ± std. dev. of 40 run)

For policy #2 called 'Oracle-klUCB' ...
    633 ms ± 26.5 ms per loop (mean ± std. dev. of 40 run)

For policy #6 called 'M-klUCB' ...
    823 ms ± 18.1 ms per loop (mean ± std. dev. of 40 run)

For policy #7 called 'CUSUM-klUCB' ...
    1.08 s ± 118 ms per loop (mean ± std. dev. of 40 run)

For policy #9 called 'GLR-klUCB(Global)' ...
    2.02 s ± 44.7 ms per loop (mean ± std. dev. of 40 run)

For policy #8 called 'GLR-klUCB(Local)' ...
    2.07 s ± 45 ms per loop (mean ± std. dev. of 40 run)

Giving the mean and std memory consumption ...

For policy #1 called 'Thompson Sampling' ...
    799.3 B ± 85.7 B (mean ± std. dev. of 40 runs)

For policy #0 called 'klUCB' ...
    933.5 B ± 223.2 B (mean ± std. dev. of 40 runs)

For policy #5 called 'DTS' ...
    941 B ± 0 B (mean ± std. dev. of 40 runs)

For policy #3 called 'D-klUCB($\gamma=0.95$)' ...
    1.1 KiB ± 0 B (mean ± std. dev. of 40 runs)

For policy #4 called 'SW-klUCB' ...
    3.8 KiB ± 1 KiB (mean ± std. dev. of 40 runs)

For policy #6 called 'M-klUCB' ...
    5.5 KiB ± 841.4 B (mean ± std. dev. of 40 runs)

For policy #7 called 'CUSUM-klUCB' ...
    7 KiB ± 3.9 KiB (mean ± std. dev. of 40 runs)

For policy #2 called 'Oracle-klUCB' ...
    9.3 KiB ± 5.3 KiB (mean ± std. dev. of 40 runs)

For policy #9 called 'GLR-klUCB(Global)' ...
    15.5 KiB ± 7.6 KiB (mean ± std. dev. of 40 runs)

For policy #8 called 'GLR-klUCB(Local)' ...
    17.3 KiB ± 9.3 KiB (mean ± std. dev. of 40 runs)

Giving the mean and std number of CP detections ...

For policy #0 called 'klUCB' ...
    0 ± 0 (mean ± std. dev. of 40 runs)

For policy #1 called 'Thompson Sampling' ...
    0 ± 0 (mean ± std. dev. of 40 runs)

For policy #3 called 'D-klUCB($\gamma=0.95$)' ...
    0 ± 0 (mean ± std. dev. of 40 runs)

For policy #4 called 'SW-klUCB' ...
    0 ± 0 (mean ± std. dev. of 40 runs)

For policy #5 called 'DTS' ...
    0 ± 0 (mean ± std. dev. of 40 runs)

For policy #6 called 'M-klUCB' ...
    1.12 ± 0.331 (mean ± std. dev. of 40 runs)

For policy #9 called 'GLR-klUCB(Global)' ...
    2 ± 0 (mean ± std. dev. of 40 runs)

For policy #8 called 'GLR-klUCB(Local)' ...
    2.05 ± 0.218 (mean ± std. dev. of 40 runs)

For policy #2 called 'Oracle-klUCB' ...
    4 ± 0 (mean ± std. dev. of 40 runs)

For policy #7 called 'CUSUM-klUCB' ...
    9.28 ± 2.1 (mean ± std. dev. of 40 runs)
