


Checking environment number 0
For a piecewise stationary problem with M = 5 sequences...

Checking m-th (m = 0) sequence, µ_m = [0.1, 0.2], µ_m+1 = [0.1, 0.3] and tau_m = 0 and tau_m+1 = 400
   - For arm i = 0, gap = 0 and length = 400 with lowerbound on length = 0...
   - For arm i = 1, gap = 0.1 and length = 400 with lowerbound on length = 6081...
WARNING For arm i = 1, gap = 0.1 and length = 400 < lowerbound on length = 6081 !!

Checking m-th (m = 1) sequence, µ_m = [0.1, 0.3], µ_m+1 = [0.5, 0.3] and tau_m = 400 and tau_m+1 = 800
   - For arm i = 0, gap = 0.4 and length = 400 with lowerbound on length = 381...
   - For arm i = 1, gap = 0 and length = 400 with lowerbound on length = 0...

Checking m-th (m = 2) sequence, µ_m = [0.5, 0.3], µ_m+1 = [0.4, 0.3] and tau_m = 800 and tau_m+1 = 1200
   - For arm i = 0, gap = 0.1 and length = 400 with lowerbound on length = 6081...
WARNING For arm i = 0, gap = 0.1 and length = 400 < lowerbound on length = 6081 !!
   - For arm i = 1, gap = 0 and length = 400 with lowerbound on length = 0...

Checking m-th (m = 3) sequence, µ_m = [0.4, 0.3], µ_m+1 = [0.3, 0.9] and tau_m = 1200 and tau_m+1 = 1600
   - For arm i = 0, gap = 0.1 and length = 400 with lowerbound on length = 6081...
WARNING For arm i = 0, gap = 0.1 and length = 400 < lowerbound on length = 6081 !!
   - For arm i = 1, gap = 0.6 and length = 400 with lowerbound on length = 169...



Checking environment number 1
For a piecewise stationary problem with M = 5 sequences...

Checking m-th (m = 0) sequence, µ_m = [0.2, 0.5, 0.9], µ_m+1 = [0.2, 0.2, 0.9] and tau_m = 0 and tau_m+1 = 400
   - For arm i = 0, gap = 0 and length = 400 with lowerbound on length = 0...
   - For arm i = 1, gap = 0.3 and length = 400 with lowerbound on length = 676...
WARNING For arm i = 1, gap = 0.3 and length = 400 < lowerbound on length = 676 !!
   - For arm i = 2, gap = 0 and length = 400 with lowerbound on length = 0...

Checking m-th (m = 1) sequence, µ_m = [0.2, 0.2, 0.9], µ_m+1 = [0.2, 0.2, 0.1] and tau_m = 400 and tau_m+1 = 800
   - For arm i = 0, gap = 0 and length = 400 with lowerbound on length = 0...
   - For arm i = 1, gap = 0 and length = 400 with lowerbound on length = 0...
   - For arm i = 2, gap = 0.8 and length = 400 with lowerbound on length = 96...

Checking m-th (m = 2) sequence, µ_m = [0.2, 0.2, 0.1], µ_m+1 = [0.7, 0.2, 0.1] and tau_m = 800 and tau_m+1 = 1200
   - For arm i = 0, gap = 0.5 and length = 400 with lowerbound on length = 244...
   - For arm i = 1, gap = 0 and length = 400 with lowerbound on length = 0...
   - For arm i = 2, gap = 0 and length = 400 with lowerbound on length = 0...

Checking m-th (m = 3) sequence, µ_m = [0.7, 0.2, 0.1], µ_m+1 = [0.7, 0.5, 0.1] and tau_m = 1200 and tau_m+1 = 1600
   - For arm i = 0, gap = 0 and length = 400 with lowerbound on length = 0...
   - For arm i = 1, gap = 0.3 and length = 400 with lowerbound on length = 676...
WARNING For arm i = 1, gap = 0.3 and length = 400 < lowerbound on length = 676 !!
   - For arm i = 2, gap = 0 and length = 400 with lowerbound on length = 0...



Checking environment number 2
For a piecewise stationary problem with M = 5 sequences...

Checking m-th (m = 0) sequence, µ_m = [0.4, 0.5, 0.9], µ_m+1 = [0.5, 0.4, 0.7] and tau_m = 0 and tau_m+1 = 400
   - For arm i = 0, gap = 0.1 and length = 400 with lowerbound on length = 6081...
WARNING For arm i = 0, gap = 0.1 and length = 400 < lowerbound on length = 6081 !!
   - For arm i = 1, gap = 0.1 and length = 400 with lowerbound on length = 6081...
WARNING For arm i = 1, gap = 0.1 and length = 400 < lowerbound on length = 6081 !!
   - For arm i = 2, gap = 0.2 and length = 400 with lowerbound on length = 1521...
WARNING For arm i = 2, gap = 0.2 and length = 400 < lowerbound on length = 1521 !!

Checking m-th (m = 1) sequence, µ_m = [0.5, 0.4, 0.7], µ_m+1 = [0.6, 0.3, 0.5] and tau_m = 400 and tau_m+1 = 800
   - For arm i = 0, gap = 0.1 and length = 400 with lowerbound on length = 6081...
WARNING For arm i = 0, gap = 0.1 and length = 400 < lowerbound on length = 6081 !!
   - For arm i = 1, gap = 0.1 and length = 400 with lowerbound on length = 6081...
WARNING For arm i = 1, gap = 0.1 and length = 400 < lowerbound on length = 6081 !!
   - For arm i = 2, gap = 0.2 and length = 400 with lowerbound on length = 1521...
WARNING For arm i = 2, gap = 0.2 and length = 400 < lowerbound on length = 1521 !!

Checking m-th (m = 2) sequence, µ_m = [0.6, 0.3, 0.5], µ_m+1 = [0.7, 0.2, 0.3] and tau_m = 800 and tau_m+1 = 1200
   - For arm i = 0, gap = 0.1 and length = 400 with lowerbound on length = 6081...
WARNING For arm i = 0, gap = 0.1 and length = 400 < lowerbound on length = 6081 !!
   - For arm i = 1, gap = 0.1 and length = 400 with lowerbound on length = 6081...
WARNING For arm i = 1, gap = 0.1 and length = 400 < lowerbound on length = 6081 !!
   - For arm i = 2, gap = 0.2 and length = 400 with lowerbound on length = 1521...
WARNING For arm i = 2, gap = 0.2 and length = 400 < lowerbound on length = 1521 !!

Checking m-th (m = 3) sequence, µ_m = [0.7, 0.2, 0.3], µ_m+1 = [0.8, 0.1, 0.1] and tau_m = 1200 and tau_m+1 = 1600
   - For arm i = 0, gap = 0.1 and length = 400 with lowerbound on length = 6081...
WARNING For arm i = 0, gap = 0.1 and length = 400 < lowerbound on length = 6081 !!
   - For arm i = 1, gap = 0.1 and length = 400 with lowerbound on length = 6081...
WARNING For arm i = 1, gap = 0.1 and length = 400 < lowerbound on length = 6081 !!
   - For arm i = 2, gap = 0.2 and length = 400 with lowerbound on length = 1521...
WARNING For arm i = 2, gap = 0.2 and length = 400 < lowerbound on length = 1521 !!



Checking environment number 3
For a piecewise stationary problem with M = 5 sequences...

Checking m-th (m = 0) sequence, µ_m = [0.1, 0.5, 0.9], µ_m+1 = [0.3, 0.4, 0.1] and tau_m = 0 and tau_m+1 = 1000
   - For arm i = 0, gap = 0.2 and length = 1000 with lowerbound on length = 1521...
WARNING For arm i = 0, gap = 0.2 and length = 1000 < lowerbound on length = 1521 !!
   - For arm i = 1, gap = 0.1 and length = 1000 with lowerbound on length = 6081...
WARNING For arm i = 1, gap = 0.1 and length = 1000 < lowerbound on length = 6081 !!
   - For arm i = 2, gap = 0.8 and length = 1000 with lowerbound on length = 96...

Checking m-th (m = 1) sequence, µ_m = [0.3, 0.4, 0.1], µ_m+1 = [0.5, 0.3, 0.2] and tau_m = 1000 and tau_m+1 = 1250
   - For arm i = 0, gap = 0.2 and length = 250 with lowerbound on length = 1521...
WARNING For arm i = 0, gap = 0.2 and length = 250 < lowerbound on length = 1521 !!
   - For arm i = 1, gap = 0.1 and length = 250 with lowerbound on length = 6081...
WARNING For arm i = 1, gap = 0.1 and length = 250 < lowerbound on length = 6081 !!
   - For arm i = 2, gap = 0.1 and length = 250 with lowerbound on length = 6081...
WARNING For arm i = 2, gap = 0.1 and length = 250 < lowerbound on length = 6081 !!

Checking m-th (m = 2) sequence, µ_m = [0.5, 0.3, 0.2], µ_m+1 = [0.7, 0.4, 0.3] and tau_m = 1250 and tau_m+1 = 1500
   - For arm i = 0, gap = 0.2 and length = 250 with lowerbound on length = 1521...
WARNING For arm i = 0, gap = 0.2 and length = 250 < lowerbound on length = 1521 !!
   - For arm i = 1, gap = 0.1 and length = 250 with lowerbound on length = 6081...
WARNING For arm i = 1, gap = 0.1 and length = 250 < lowerbound on length = 6081 !!
   - For arm i = 2, gap = 0.1 and length = 250 with lowerbound on length = 6081...
WARNING For arm i = 2, gap = 0.1 and length = 250 < lowerbound on length = 6081 !!

Checking m-th (m = 3) sequence, µ_m = [0.7, 0.4, 0.3], µ_m+1 = [0.1, 0.5, 0.2] and tau_m = 1500 and tau_m+1 = 1750
   - For arm i = 0, gap = 0.6 and length = 250 with lowerbound on length = 169...
   - For arm i = 1, gap = 0.1 and length = 250 with lowerbound on length = 6081...
WARNING For arm i = 1, gap = 0.1 and length = 250 < lowerbound on length = 6081 !!
   - For arm i = 2, gap = 0.1 and length = 250 with lowerbound on length = 6081...
WARNING For arm i = 2, gap = 0.1 and length = 250 < lowerbound on length = 6081 !!
Loaded experiments configuration from 'configuration_nonstationnary.py' :
configuration['policies'] = [{'archtype': <class 'Policies.Exp3PlusPlus.Exp3PlusPlus'>, 'params': {}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}}, {'archtype': <class 'Policies.DiscountedThompson.DiscountedThompson'>, 'params': {'posterior': <class 'Policies.Posterior.DiscountedBeta.DiscountedBeta'>, 'gamma': 0.99}}, {'archtype': <class 'Policies.DiscountedThompson.DiscountedThompson'>, 'params': {'posterior': <class 'Policies.Posterior.DiscountedBeta.DiscountedBeta'>, 'gamma': 0.9}}, {'archtype': <class 'Policies.DiscountedThompson.DiscountedThompson'>, 'params': {'posterior': <class 'Policies.Posterior.DiscountedBeta.DiscountedBeta'>, 'gamma': 0.7}}, {'archtype': <class 'Policies.CD_UCB.Exp3R'>, 'params': {'horizon': 2000}}, {'archtype': <class 'Policies.CD_UCB.Exp3RPlusPlus'>, 'params': {'horizon': 2000}}, {'archtype': <class 'Policies.AdSwitch.AdSwitch'>, 'params': {'horizon': 2000, 'C1': 1, 'C2': 1}}, {'archtype': <class 'Policies.LM_DSEE.LM_DSEE'>, 'params': {'nu': 0.25, 'DeltaMin': 0.1, 'a': 1, 'b': 0.25}}, {'archtype': <class 'Policies.CD_UCB.CUSUM_IndexPolicy'>, 'params': {'horizon': 2000, 'max_nb_random_events': 5, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}}, {'archtype': <class 'Policies.CD_UCB.CUSUM_IndexPolicy'>, 'params': {'horizon': 2000, 'max_nb_random_events': 5, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}}, {'archtype': <class 'Policies.CD_UCB.PHT_IndexPolicy'>, 'params': {'horizon': 2000, 'max_nb_random_events': 5, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}}, {'archtype': <class 'Policies.CD_UCB.PHT_IndexPolicy'>, 'params': {'horizon': 2000, 'max_nb_random_events': 5, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}}, {'archtype': <class 'Policies.CD_UCB.BernoulliGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}}, {'archtype': <class 'Policies.CD_UCB.BernoulliGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}}, {'archtype': <class 'Policies.CD_UCB.GaussianGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}}, {'archtype': <class 'Policies.CD_UCB.GaussianGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}}, {'archtype': <class 'Policies.CD_UCB.SubGaussianGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}}, {'archtype': <class 'Policies.CD_UCB.SubGaussianGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}}, {'archtype': <class 'Policies.Monitored_UCB.Monitored_IndexPolicy'>, 'params': {'horizon': 2000, 'w': 80, 'b': 26.07187326473663, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}}, {'archtype': <class 'Policies.Monitored_UCB.Monitored_IndexPolicy'>, 'params': {'horizon': 2000, 'w': 80, 'b': 26.07187326473663, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}}, {'archtype': <class 'Policies.SWHash_UCB.SWHash_IndexPolicy'>, 'params': {'alpha': 1, 'lmbda': 1, 'policy': <class 'Policies.UCB.UCB'>}}, {'archtype': <class 'Policies.SlidingWindowUCB.SWUCBPlus'>, 'params': {'horizon': 2000, 'alpha': 1}}, {'archtype': <class 'Policies.DiscountedUCB.DiscountedUCBPlus'>, 'params': {'max_nb_random_events': 5, 'alpha': 1, 'horizon': 2000}}, {'archtype': <class 'Policies.OracleSequentiallyRestartPolicy.OracleSequentiallyRestartPolicy'>, 'params': {'changePoints': [0, 400, 800, 1200, 1600], 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}}]
configuration['environment'] = [{'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.1, 0.2], [0.1, 0.3], [0.5, 0.3], [0.4, 0.3], [0.3, 0.9]], 'changePoints': [0, 400, 800, 1200, 1600]}}, {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.2, 0.5, 0.9], [0.2, 0.2, 0.9], [0.2, 0.2, 0.1], [0.7, 0.2, 0.1], [0.7, 0.5, 0.1]], 'changePoints': [0, 400, 800, 1200, 1600]}}, {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.4, 0.5, 0.9], [0.5, 0.4, 0.7], [0.6, 0.3, 0.5], [0.7, 0.2, 0.3], [0.8, 0.1, 0.1]], 'changePoints': [0, 400, 800, 1200, 1600]}}, {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.1, 0.5, 0.9], [0.3, 0.4, 0.1], [0.5, 0.3, 0.2], [0.7, 0.4, 0.3], [0.1, 0.5, 0.2]], 'changePoints': [0, 1000, 1250, 1500, 1750]}}]
====> TURNING DEBUG MODE ON <=====
plots/ is already a directory here...
Number of policies in this comparison: 27
Time horizon: 2000
Number of repetitions: 4
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: -1
Using this dictionary to create a new environment:
 {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.1, 0.2], [0.1, 0.3], [0.5, 0.3], [0.4, 0.3], [0.3, 0.9]], 'changePoints': [0, 400, 800, 1200, 1600]}}
  Special MAB problem, with arm (possibly) changing at every time step, read from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.1, 0.2], [0.1, 0.3], [0.5, 0.3], [0.4, 0.3], [0.3, 0.9]], 'changePoints': [0, 400, 800, 1200, 1600]}} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = {'listOfMeans': [[0.1, 0.2], [0.1, 0.3], [0.5, 0.3], [0.4, 0.3], [0.3, 0.9]], 'changePoints': [0, 400, 800, 1200, 1600]}
 - with 'listOfMeans' = [[0.1 0.2]
 [0.1 0.3]
 [0.5 0.3]
 [0.4 0.3]
 [0.3 0.9]]
 - with 'changePoints' = [0, 400, 800, 1200, 1600]


 ==> Creating the dynamic arms ...
   - with 'nbArms' = 2
   - with 'arms' = [B(0.1), B(0.2)]
 - Initial draw of 'means' = [0.1 0.2]
Using this dictionary to create a new environment:
 {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.2, 0.5, 0.9], [0.2, 0.2, 0.9], [0.2, 0.2, 0.1], [0.7, 0.2, 0.1], [0.7, 0.5, 0.1]], 'changePoints': [0, 400, 800, 1200, 1600]}}
  Special MAB problem, with arm (possibly) changing at every time step, read from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.2, 0.5, 0.9], [0.2, 0.2, 0.9], [0.2, 0.2, 0.1], [0.7, 0.2, 0.1], [0.7, 0.5, 0.1]], 'changePoints': [0, 400, 800, 1200, 1600]}} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = {'listOfMeans': [[0.2, 0.5, 0.9], [0.2, 0.2, 0.9], [0.2, 0.2, 0.1], [0.7, 0.2, 0.1], [0.7, 0.5, 0.1]], 'changePoints': [0, 400, 800, 1200, 1600]}
 - with 'listOfMeans' = [[0.2 0.5 0.9]
 [0.2 0.2 0.9]
 [0.2 0.2 0.1]
 [0.7 0.2 0.1]
 [0.7 0.5 0.1]]
 - with 'changePoints' = [0, 400, 800, 1200, 1600]


 ==> Creating the dynamic arms ...
   - with 'nbArms' = 3
   - with 'arms' = [B(0.2), B(0.5), B(0.9)]
 - Initial draw of 'means' = [0.2 0.5 0.9]
Using this dictionary to create a new environment:
 {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.4, 0.5, 0.9], [0.5, 0.4, 0.7], [0.6, 0.3, 0.5], [0.7, 0.2, 0.3], [0.8, 0.1, 0.1]], 'changePoints': [0, 400, 800, 1200, 1600]}}
  Special MAB problem, with arm (possibly) changing at every time step, read from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.4, 0.5, 0.9], [0.5, 0.4, 0.7], [0.6, 0.3, 0.5], [0.7, 0.2, 0.3], [0.8, 0.1, 0.1]], 'changePoints': [0, 400, 800, 1200, 1600]}} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = {'listOfMeans': [[0.4, 0.5, 0.9], [0.5, 0.4, 0.7], [0.6, 0.3, 0.5], [0.7, 0.2, 0.3], [0.8, 0.1, 0.1]], 'changePoints': [0, 400, 800, 1200, 1600]}
 - with 'listOfMeans' = [[0.4 0.5 0.9]
 [0.5 0.4 0.7]
 [0.6 0.3 0.5]
 [0.7 0.2 0.3]
 [0.8 0.1 0.1]]
 - with 'changePoints' = [0, 400, 800, 1200, 1600]


 ==> Creating the dynamic arms ...
   - with 'nbArms' = 3
   - with 'arms' = [B(0.4), B(0.5), B(0.9)]
 - Initial draw of 'means' = [0.4 0.5 0.9]
Using this dictionary to create a new environment:
 {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.1, 0.5, 0.9], [0.3, 0.4, 0.1], [0.5, 0.3, 0.2], [0.7, 0.4, 0.3], [0.1, 0.5, 0.2]], 'changePoints': [0, 1000, 1250, 1500, 1750]}}
  Special MAB problem, with arm (possibly) changing at every time step, read from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.1, 0.5, 0.9], [0.3, 0.4, 0.1], [0.5, 0.3, 0.2], [0.7, 0.4, 0.3], [0.1, 0.5, 0.2]], 'changePoints': [0, 1000, 1250, 1500, 1750]}} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = {'listOfMeans': [[0.1, 0.5, 0.9], [0.3, 0.4, 0.1], [0.5, 0.3, 0.2], [0.7, 0.4, 0.3], [0.1, 0.5, 0.2]], 'changePoints': [0, 1000, 1250, 1500, 1750]}
 - with 'listOfMeans' = [[0.1 0.5 0.9]
 [0.3 0.4 0.1]
 [0.5 0.3 0.2]
 [0.7 0.4 0.3]
 [0.1 0.5 0.2]]
 - with 'changePoints' = [0, 1000, 1250, 1500, 1750]


 ==> Creating the dynamic arms ...
   - with 'nbArms' = 3
   - with 'arms' = [B(0.1), B(0.5), B(0.9)]
 - Initial draw of 'means' = [0.1 0.5 0.9]
Number of environments to try: 4
Warning: forcing to use putatright = False because there is 2 items in the legend.


Evaluating environment: PieceWiseStationaryMAB(nbArms: 2, arms: [B(0.1), B(0.2)])
- Adding policy #1 = {'archtype': <class 'Policies.Exp3PlusPlus.Exp3PlusPlus'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'archtype': <class 'Policies.Exp3PlusPlus.Exp3PlusPlus'>, 'params': {}} ...
- Adding policy #2 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
- Adding policy #3 = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
- Adding policy #4 = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}} ...
- Adding policy #5 = {'archtype': <class 'Policies.DiscountedThompson.DiscountedThompson'>, 'params': {'posterior': <class 'Policies.Posterior.DiscountedBeta.DiscountedBeta'>, 'gamma': 0.99}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][4]' = {'archtype': <class 'Policies.DiscountedThompson.DiscountedThompson'>, 'params': {'posterior': <class 'Policies.Posterior.DiscountedBeta.DiscountedBeta'>, 'gamma': 0.99}} ...
- Adding policy #6 = {'archtype': <class 'Policies.DiscountedThompson.DiscountedThompson'>, 'params': {'posterior': <class 'Policies.Posterior.DiscountedBeta.DiscountedBeta'>, 'gamma': 0.9}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][5]' = {'archtype': <class 'Policies.DiscountedThompson.DiscountedThompson'>, 'params': {'posterior': <class 'Policies.Posterior.DiscountedBeta.DiscountedBeta'>, 'gamma': 0.9}} ...
- Adding policy #7 = {'archtype': <class 'Policies.DiscountedThompson.DiscountedThompson'>, 'params': {'posterior': <class 'Policies.Posterior.DiscountedBeta.DiscountedBeta'>, 'gamma': 0.7}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][6]' = {'archtype': <class 'Policies.DiscountedThompson.DiscountedThompson'>, 'params': {'posterior': <class 'Policies.Posterior.DiscountedBeta.DiscountedBeta'>, 'gamma': 0.7}} ...
- Adding policy #8 = {'archtype': <class 'Policies.CD_UCB.Exp3R'>, 'params': {'horizon': 2000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][7]' = {'archtype': <class 'Policies.CD_UCB.Exp3R'>, 'params': {'horizon': 2000}} ...
Warning: the policy Exp3R($T=2000$, $c=3.18$, $\alpha=0.01$) tried to use default value of gamma = 0.07258473744212861 but could not set attribute self.policy.gamma to gamma (maybe it's using an Exp3 with a non-constant value of gamma).
- Adding policy #9 = {'archtype': <class 'Policies.CD_UCB.Exp3RPlusPlus'>, 'params': {'horizon': 2000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][8]' = {'archtype': <class 'Policies.CD_UCB.Exp3RPlusPlus'>, 'params': {'horizon': 2000}} ...
Warning: the policy Exp3R++($T=2000$, $c=0.586$, $\alpha=0.294$) tried to use default value of gamma = 0.07258473744212861 but could not set attribute self.policy.gamma to gamma (maybe it's using an Exp3 with a non-constant value of gamma).
- Adding policy #10 = {'archtype': <class 'Policies.AdSwitch.AdSwitch'>, 'params': {'horizon': 2000, 'C1': 1, 'C2': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][9]' = {'archtype': <class 'Policies.AdSwitch.AdSwitch'>, 'params': {'horizon': 2000, 'C1': 1, 'C2': 1}} ...
- Adding policy #11 = {'archtype': <class 'Policies.LM_DSEE.LM_DSEE'>, 'params': {'nu': 0.25, 'DeltaMin': 0.1, 'a': 1, 'b': 0.25}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][10]' = {'archtype': <class 'Policies.LM_DSEE.LM_DSEE'>, 'params': {'nu': 0.25, 'DeltaMin': 0.1, 'a': 1, 'b': 0.25}} ...
- Adding policy #12 = {'archtype': <class 'Policies.CD_UCB.CUSUM_IndexPolicy'>, 'params': {'horizon': 2000, 'max_nb_random_events': 5, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][11]' = {'archtype': <class 'Policies.CD_UCB.CUSUM_IndexPolicy'>, 'params': {'horizon': 2000, 'max_nb_random_events': 5, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}} ...
compute_h_alpha_from_input_parameters__CUSUM() with:
T = 2000, UpsilonT = 5, K = 2, epsilon = 0.5, lmbda = 1, M = 100
Gave C2 = 1.0986122886681098, C1- = 2.1972245773362196 and C1+ = 0.6359887667199967 so C1 = 0.6359887667199967, and h = 9.420708132956397 and alpha = 0.003217095852289997
- Adding policy #13 = {'archtype': <class 'Policies.CD_UCB.CUSUM_IndexPolicy'>, 'params': {'horizon': 2000, 'max_nb_random_events': 5, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][12]' = {'archtype': <class 'Policies.CD_UCB.CUSUM_IndexPolicy'>, 'params': {'horizon': 2000, 'max_nb_random_events': 5, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}} ...
compute_h_alpha_from_input_parameters__CUSUM() with:
T = 2000, UpsilonT = 5, K = 2, epsilon = 0.5, lmbda = 1, M = 100
Gave C2 = 1.0986122886681098, C1- = 2.1972245773362196 and C1+ = 0.6359887667199967 so C1 = 0.6359887667199967, and h = 9.420708132956397 and alpha = 0.003217095852289997
- Adding policy #14 = {'archtype': <class 'Policies.CD_UCB.PHT_IndexPolicy'>, 'params': {'horizon': 2000, 'max_nb_random_events': 5, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][13]' = {'archtype': <class 'Policies.CD_UCB.PHT_IndexPolicy'>, 'params': {'horizon': 2000, 'max_nb_random_events': 5, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}} ...
compute_h_alpha_from_input_parameters__CUSUM() with:
T = 2000, UpsilonT = 5, K = 2, epsilon = 0.5, lmbda = 1, M = 100
Gave C2 = 1.0986122886681098, C1- = 2.1972245773362196 and C1+ = 0.6359887667199967 so C1 = 0.6359887667199967, and h = 9.420708132956397 and alpha = 0.003217095852289997
- Adding policy #15 = {'archtype': <class 'Policies.CD_UCB.PHT_IndexPolicy'>, 'params': {'horizon': 2000, 'max_nb_random_events': 5, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][14]' = {'archtype': <class 'Policies.CD_UCB.PHT_IndexPolicy'>, 'params': {'horizon': 2000, 'max_nb_random_events': 5, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}} ...
compute_h_alpha_from_input_parameters__CUSUM() with:
T = 2000, UpsilonT = 5, K = 2, epsilon = 0.5, lmbda = 1, M = 100
Gave C2 = 1.0986122886681098, C1- = 2.1972245773362196 and C1+ = 0.6359887667199967 so C1 = 0.6359887667199967, and h = 9.420708132956397 and alpha = 0.003217095852289997
- Adding policy #16 = {'archtype': <class 'Policies.CD_UCB.BernoulliGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][15]' = {'archtype': <class 'Policies.CD_UCB.BernoulliGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}} ...
- Adding policy #17 = {'archtype': <class 'Policies.CD_UCB.BernoulliGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][16]' = {'archtype': <class 'Policies.CD_UCB.BernoulliGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}} ...
- Adding policy #18 = {'archtype': <class 'Policies.CD_UCB.GaussianGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][17]' = {'archtype': <class 'Policies.CD_UCB.GaussianGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}} ...
- Adding policy #19 = {'archtype': <class 'Policies.CD_UCB.GaussianGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][18]' = {'archtype': <class 'Policies.CD_UCB.GaussianGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}} ...
- Adding policy #20 = {'archtype': <class 'Policies.CD_UCB.SubGaussianGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][19]' = {'archtype': <class 'Policies.CD_UCB.SubGaussianGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}} ...
- Adding policy #21 = {'archtype': <class 'Policies.CD_UCB.SubGaussianGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][20]' = {'archtype': <class 'Policies.CD_UCB.SubGaussianGLR_IndexPolicy'>, 'params': {'horizon': 2000, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}} ...
- Adding policy #22 = {'archtype': <class 'Policies.Monitored_UCB.Monitored_IndexPolicy'>, 'params': {'horizon': 2000, 'w': 80, 'b': 26.07187326473663, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][21]' = {'archtype': <class 'Policies.Monitored_UCB.Monitored_IndexPolicy'>, 'params': {'horizon': 2000, 'w': 80, 'b': 26.07187326473663, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}} ...
Warning: the formula for gamma in the paper gave gamma = 0.0, that's absurd, we use instead 0.009900990099009901
- Adding policy #23 = {'archtype': <class 'Policies.Monitored_UCB.Monitored_IndexPolicy'>, 'params': {'horizon': 2000, 'w': 80, 'b': 26.07187326473663, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][22]' = {'archtype': <class 'Policies.Monitored_UCB.Monitored_IndexPolicy'>, 'params': {'horizon': 2000, 'w': 80, 'b': 26.07187326473663, 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': False}} ...
Warning: the formula for gamma in the paper gave gamma = 0.0, that's absurd, we use instead 0.009900990099009901
- Adding policy #24 = {'archtype': <class 'Policies.SWHash_UCB.SWHash_IndexPolicy'>, 'params': {'alpha': 1, 'lmbda': 1, 'policy': <class 'Policies.UCB.UCB'>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][23]' = {'archtype': <class 'Policies.SWHash_UCB.SWHash_IndexPolicy'>, 'params': {'alpha': 1, 'lmbda': 1, 'policy': <class 'Policies.UCB.UCB'>}} ...
- Adding policy #25 = {'archtype': <class 'Policies.SlidingWindowUCB.SWUCBPlus'>, 'params': {'horizon': 2000, 'alpha': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][24]' = {'archtype': <class 'Policies.SlidingWindowUCB.SWUCBPlus'>, 'params': {'horizon': 2000, 'alpha': 1}} ...
- Adding policy #26 = {'archtype': <class 'Policies.DiscountedUCB.DiscountedUCBPlus'>, 'params': {'max_nb_random_events': 5, 'alpha': 1, 'horizon': 2000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][25]' = {'archtype': <class 'Policies.DiscountedUCB.DiscountedUCBPlus'>, 'params': {'max_nb_random_events': 5, 'alpha': 1, 'horizon': 2000}} ...
- Adding policy #27 = {'archtype': <class 'Policies.OracleSequentiallyRestartPolicy.OracleSequentiallyRestartPolicy'>, 'params': {'changePoints': [0, 400, 800, 1200, 1600], 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][26]' = {'archtype': <class 'Policies.OracleSequentiallyRestartPolicy.OracleSequentiallyRestartPolicy'>, 'params': {'changePoints': [0, 400, 800, 1200, 1600], 'policy': <class 'Policies.klUCB.klUCB'>, 'per_arm_restart': True}} ...
Info: creating a new policy OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm), with change points = [400, 800, 1200, 1600]...

===> Pre-computing the rewards ... Of shape (2, 4, 2000) ...
    In order for all simulated algorithms to face the same random rewards (robust comparison of A1,..,An vs Aggr(A1,..,An)) ...




- Evaluating policy #1/27: Exp3++ ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy Exp3++ after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #2/27: UCB($\alpha=1$) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy UCB($\alpha=1$) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #3/27: kl-UCB ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy kl-UCB after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #4/27: Thompson ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy Thompson after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #5/27: DiscountedThompson($\gamma=0.99$) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy DiscountedThompson($\gamma=0.99$) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #6/27: DiscountedThompson($\gamma=0.9$) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy DiscountedThompson($\gamma=0.9$) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #7/27: DiscountedThompson($\gamma=0.7$) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy DiscountedThompson($\gamma=0.7$) after 2000 steps: [1 0] ...
  ==> Optimal arm identification: 33.33% (relative success)...
  ==> Mean distance from optimal ordering: 25.00% (relative success)...



- Evaluating policy #8/27: Exp3R($T=2000$, $c=3.18$, $\alpha=0.01$) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy Exp3R($T=2000$, $c=3.18$, $\alpha=0.01$) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #9/27: Exp3R++($T=2000$, $c=0.586$, $\alpha=0.294$) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy Exp3R++($T=2000$, $c=3.92$, $\alpha=0.00658$) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #10/27: AdSwitch($T=2000$, $C_1=1$, $C_2=1$) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy AdSwitch($T=2000$, $C_1=1$, $C_2=1$) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #11/27: LM-DSEE($\gamma=200$, $\rho=0.6$, $\ell=2.59e+03$, $a=1$, $b=0.25$) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...
A LM-DSEE($\gamma=200$, $\rho=0.6$, $\ell=2.59e+03$, $a=1$, $b=0.25$) player at time 1297 and batch number 1 observed the mean rewards = [0.10493827160493827, 0.0] (for pulls [1296, 1]) and will play 0 for this exploitation phase.
A LM-DSEE($\gamma=200$, $\rho=0.6$, $\ell=2.59e+03$, $a=1$, $b=0.25$) player at time 1297 and batch number 1 observed the mean rewards = [0.09027777777777778, 1.0] (for pulls [1296, 1]) and will play 1 for this exploitation phase.
A LM-DSEE($\gamma=200$, $\rho=0.6$, $\ell=2.59e+03$, $a=1$, $b=0.25$) player at time 1297 and batch number 1 observed the mean rewards = [0.09722222222222222, 1.0] (for pulls [1296, 1]) and will play 1 for this exploitation phase.

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...
A LM-DSEE($\gamma=200$, $\rho=0.6$, $\ell=2.59e+03$, $a=1$, $b=0.25$) player at time 1297 and batch number 1 observed the mean rewards = [0.1095679012345679, 0.0] (for pulls [1296, 1]) and will play 0 for this exploitation phase.

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy LM-DSEE($\gamma=200$, $\rho=0.6$, $\ell=2.59e+03$, $a=1$, $b=0.25$) after 2000 steps: [1 0] ...
  ==> Optimal arm identification: 33.33% (relative success)...
  ==> Mean distance from optimal ordering: 25.00% (relative success)...



- Evaluating policy #12/27: CUSUM-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Per-Arm) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy CUSUM-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Per-Arm) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #13/27: CUSUM-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Global) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy CUSUM-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Global) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #14/27: PHT-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Per-Arm) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy PHT-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Per-Arm) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #15/27: PHT-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Global) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy PHT-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Global) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #16/27: BernoulliGLR-klUCB(Per-Arm) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy BernoulliGLR-klUCB(Per-Arm) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #17/27: BernoulliGLR-klUCB(Global) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy BernoulliGLR-klUCB(Global) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #18/27: GaussianGLR-klUCB(Per-Arm) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy GaussianGLR-klUCB(Per-Arm) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #19/27: GaussianGLR-klUCB(Global) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy GaussianGLR-klUCB(Global) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #20/27: SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 62 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 88 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 139 for arm 0 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 151 for arm 1 after seeing reward = 0.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 162 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 244 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 240 for arm 0 after seeing reward = 1.0!

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 398 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 452 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 552 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 596 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 756 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 825 for arm 0 after seeing reward = 1.0!

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 610 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 1083 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 1241 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 1035 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 1068 for arm 1 after seeing reward = 0.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 1084 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 1449 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 1487 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 1594 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 1755 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 1764 for arm 1 after seeing reward = 0.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 1825 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) a change was detected at time 1853 for arm 1 after seeing reward = 1.0!

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #21/27: SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 33 for arm 0 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 52 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 91 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 138 for arm 1 after seeing reward = 0.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 131 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 244 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 162 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 305 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 257 for arm 0 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 379 for arm 1 after seeing reward = 1.0!

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 402 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 705 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 730 for arm 1 after seeing reward = 1.0!

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 840 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 757 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 1095 for arm 0 after seeing reward = 1.0!

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 1080 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 610 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 1500 for arm 1 after seeing reward = 1.0!

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 1036 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 1080 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 1090 for arm 1 after seeing reward = 0.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 1994 for arm 1 after seeing reward = 1.0!

Estimated order by the policy SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 1756 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 1485 for arm 1 after seeing reward = 1.0!
For a player SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global) a change was detected at time 1525 for arm 1 after seeing reward = 0.0!



- Evaluating policy #22/27: M-klUCB($w=80$, $b=26.0719$, $\gamma=0.0099$, Per-Arm) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy M-klUCB($w=80$, $b=26.0719$, $\gamma=0.0099$, Per-Arm) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #23/27: M-klUCB($w=80$, $b=26.0719$, $\gamma=0.0099$, Global) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy M-klUCB($w=80$, $b=26.0719$, $\gamma=0.0099$, Global) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #24/27: SW-UCB#($\lambda=1$, $\alpha=1$) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy SW-UCB#($\lambda=1$, $\alpha=1$) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #25/27: SW-UCB+($\tau=493$, $\alpha=1$) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy SW-UCB+($\tau=493$, $\alpha=1$) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #26/27: D-UCB+($\alpha=1$, $\gamma=0.9875$) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...

Estimated order by the policy D-UCB+($\alpha=1$, $\gamma=0.9875$) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #27/27: OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) ...

New means vector = [0.1 0.2], best arm(s) = [1], at time t = 0 ...
For a player OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) a change was detected at time 400 for arm 1, because this time step is in its list of change points! Still 3 change points to go!
For a player OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) a change was detected at time 400 for arm 1, because this time step is in its list of change points! Still 3 change points to go!
For a player OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) a change was detected at time 400 for arm 1, because this time step is in its list of change points! Still 3 change points to go!

New means vector = [0.1 0.3], best arm(s) = [1], at time t = 400 ...
For a player OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) a change was detected at time 400 for arm 1, because this time step is in its list of change points! Still 3 change points to go!
For a player OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) a change was detected at time 800 for arm 1, because this time step is in its list of change points! Still 2 change points to go!
For a player OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) a change was detected at time 800 for arm 1, because this time step is in its list of change points! Still 2 change points to go!
For a player OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) a change was detected at time 800 for arm 1, because this time step is in its list of change points! Still 2 change points to go!

New means vector = [0.5 0.3], best arm(s) = [0], at time t = 800 ...
For a player OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) a change was detected at time 800 for arm 1, because this time step is in its list of change points! Still 2 change points to go!
For a player OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) a change was detected at time 1200 for arm 1, because this time step is in its list of change points! Still 1 change points to go!
For a player OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) a change was detected at time 1200 for arm 1, because this time step is in its list of change points! Still 1 change points to go!
For a player OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) a change was detected at time 1200 for arm 1, because this time step is in its list of change points! Still 1 change points to go!

New means vector = [0.4 0.3], best arm(s) = [0], at time t = 1200 ...
For a player OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) a change was detected at time 1200 for arm 1, because this time step is in its list of change points! Still 1 change points to go!
For a player OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) a change was detected at time 1600 for arm 1, because this time step is in its list of change points! Still 0 change points to go!
For a player OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) a change was detected at time 1600 for arm 1, because this time step is in its list of change points! Still 0 change points to go!

New means vector = [0.3 0.9], best arm(s) = [1], at time t = 1600 ...
For a player OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) a change was detected at time 1600 for arm 0, because this time step is in its list of change points! Still 0 change points to go!

Estimated order by the policy OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) after 2000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...
For a player OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm) a change was detected at time 1600 for arm 1, because this time step is in its list of change points! Still 0 change points to go!

Giving the vector of final regrets ...

  For policy #0 called 'Exp3++' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 200
Mean of   last regrets R_T = 200
Median of last regrets R_T = 200
Max of    last regrets R_T = 200
STD of    last regrets R_T = 0

  For policy #1 called 'UCB($\alpha=1$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 150
Mean of   last regrets R_T = 150
Median of last regrets R_T = 150
Max of    last regrets R_T = 150
STD of    last regrets R_T = 0

  For policy #2 called 'kl-UCB' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 132
Mean of   last regrets R_T = 132
Median of last regrets R_T = 132
Max of    last regrets R_T = 132
STD of    last regrets R_T = 0

  For policy #3 called 'Thompson' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 135
Mean of   last regrets R_T = 135
Median of last regrets R_T = 135
Max of    last regrets R_T = 135
STD of    last regrets R_T = 0

  For policy #4 called 'DiscountedThompson($\gamma=0.99$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 146
Mean of   last regrets R_T = 146
Median of last regrets R_T = 146
Max of    last regrets R_T = 146
STD of    last regrets R_T = 0

  For policy #5 called 'DiscountedThompson($\gamma=0.9$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 219
Mean of   last regrets R_T = 219
Median of last regrets R_T = 219
Max of    last regrets R_T = 219
STD of    last regrets R_T = 0

  For policy #6 called 'DiscountedThompson($\gamma=0.7$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 230
Mean of   last regrets R_T = 230
Median of last regrets R_T = 230
Max of    last regrets R_T = 230
STD of    last regrets R_T = 0

  For policy #7 called 'Exp3R($T=2000$, $c=3.18$, $\alpha=0.01$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 206
Mean of   last regrets R_T = 206
Median of last regrets R_T = 206
Max of    last regrets R_T = 206
STD of    last regrets R_T = 0

  For policy #8 called 'Exp3R++($T=2000$, $c=0.586$, $\alpha=0.294$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 174
Mean of   last regrets R_T = 174
Median of last regrets R_T = 174
Max of    last regrets R_T = 174
STD of    last regrets R_T = 0

  For policy #9 called 'AdSwitch($T=2000$, $C_1=1$, $C_2=1$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 241
Mean of   last regrets R_T = 241
Median of last regrets R_T = 241
Max of    last regrets R_T = 241
STD of    last regrets R_T = 0

  For policy #10 called 'LM-DSEE($\gamma=200$, $\rho=0.6$, $\ell=2.59e+03$, $a=1$, $b=0.25$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 360
Mean of   last regrets R_T = 360
Median of last regrets R_T = 360
Max of    last regrets R_T = 360
STD of    last regrets R_T = 0

  For policy #11 called 'CUSUM-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Per-Arm)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 128
Mean of   last regrets R_T = 128
Median of last regrets R_T = 128
Max of    last regrets R_T = 128
STD of    last regrets R_T = 0

  For policy #12 called 'CUSUM-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Global)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 126
Mean of   last regrets R_T = 126
Median of last regrets R_T = 126
Max of    last regrets R_T = 126
STD of    last regrets R_T = 0

  For policy #13 called 'PHT-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Per-Arm)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 134
Mean of   last regrets R_T = 134
Median of last regrets R_T = 134
Max of    last regrets R_T = 134
STD of    last regrets R_T = 0

  For policy #14 called 'PHT-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Global)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 130
Mean of   last regrets R_T = 130
Median of last regrets R_T = 130
Max of    last regrets R_T = 130
STD of    last regrets R_T = 0

  For policy #15 called 'BernoulliGLR-klUCB(Per-Arm)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 132
Mean of   last regrets R_T = 132
Median of last regrets R_T = 132
Max of    last regrets R_T = 132
STD of    last regrets R_T = 0

  For policy #16 called 'BernoulliGLR-klUCB(Global)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 131
Mean of   last regrets R_T = 131
Median of last regrets R_T = 131
Max of    last regrets R_T = 131
STD of    last regrets R_T = 0

  For policy #17 called 'GaussianGLR-klUCB(Per-Arm)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 132
Mean of   last regrets R_T = 132
Median of last regrets R_T = 132
Max of    last regrets R_T = 132
STD of    last regrets R_T = 0

  For policy #18 called 'GaussianGLR-klUCB(Global)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 135
Mean of   last regrets R_T = 135
Median of last regrets R_T = 135
Max of    last regrets R_T = 135
STD of    last regrets R_T = 0

  For policy #19 called 'SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 152
Mean of   last regrets R_T = 152
Median of last regrets R_T = 152
Max of    last regrets R_T = 152
STD of    last regrets R_T = 0

  For policy #20 called 'SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 183
Mean of   last regrets R_T = 183
Median of last regrets R_T = 183
Max of    last regrets R_T = 183
STD of    last regrets R_T = 0

  For policy #21 called 'M-klUCB($w=80$, $b=26.0719$, $\gamma=0.0099$, Per-Arm)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 138
Mean of   last regrets R_T = 138
Median of last regrets R_T = 138
Max of    last regrets R_T = 138
STD of    last regrets R_T = 0

  For policy #22 called 'M-klUCB($w=80$, $b=26.0719$, $\gamma=0.0099$, Global)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 138
Mean of   last regrets R_T = 138
Median of last regrets R_T = 138
Max of    last regrets R_T = 138
STD of    last regrets R_T = 0

  For policy #23 called 'SW-UCB#($\lambda=1$, $\alpha=1$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 171
Mean of   last regrets R_T = 171
Median of last regrets R_T = 171
Max of    last regrets R_T = 171
STD of    last regrets R_T = 0

  For policy #24 called 'SW-UCB+($\tau=493$, $\alpha=1$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 176
Mean of   last regrets R_T = 176
Median of last regrets R_T = 176
Max of    last regrets R_T = 176
STD of    last regrets R_T = 0

  For policy #25 called 'D-UCB+($\alpha=1$, $\gamma=0.9875$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 180
Mean of   last regrets R_T = 180
Median of last regrets R_T = 180
Max of    last regrets R_T = 180
STD of    last regrets R_T = 0

  For policy #26 called 'OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 130
Mean of   last regrets R_T = 130
Median of last regrets R_T = 130
Max of    last regrets R_T = 130
STD of    last regrets R_T = 0

Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'CUSUM-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Global)'	was ranked	1 / 27 for this simulation (last regret = 125.92).
- Policy 'CUSUM-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Per-Arm)'	was ranked	2 / 27 for this simulation (last regret = 127.77).
- Policy 'OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm)'	was ranked	3 / 27 for this simulation (last regret = 129.47).
- Policy 'PHT-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Global)'	was ranked	4 / 27 for this simulation (last regret = 130.47).
- Policy 'BernoulliGLR-klUCB(Global)'	was ranked	5 / 27 for this simulation (last regret = 131).
- Policy 'GaussianGLR-klUCB(Per-Arm)'	was ranked	6 / 27 for this simulation (last regret = 132.1).
- Policy 'kl-UCB'	was ranked	7 / 27 for this simulation (last regret = 132.1).
- Policy 'BernoulliGLR-klUCB(Per-Arm)'	was ranked	8 / 27 for this simulation (last regret = 132.27).
- Policy 'PHT-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Per-Arm)'	was ranked	9 / 27 for this simulation (last regret = 134.15).
- Policy 'Thompson'	was ranked	10 / 27 for this simulation (last regret = 134.95).
- Policy 'GaussianGLR-klUCB(Global)'	was ranked	11 / 27 for this simulation (last regret = 135.12).
- Policy 'M-klUCB($w=80$, $b=26.0719$, $\gamma=0.0099$, Global)'	was ranked	12 / 27 for this simulation (last regret = 137.65).
- Policy 'M-klUCB($w=80$, $b=26.0719$, $\gamma=0.0099$, Per-Arm)'	was ranked	13 / 27 for this simulation (last regret = 137.65).
- Policy 'DiscountedThompson($\gamma=0.99$)'	was ranked	14 / 27 for this simulation (last regret = 144.33).
- Policy 'UCB($\alpha=1$)'	was ranked	15 / 27 for this simulation (last regret = 148.87).
- Policy 'SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm)'	was ranked	16 / 27 for this simulation (last regret = 151.13).
- Policy 'SW-UCB#($\lambda=1$, $\alpha=1$)'	was ranked	17 / 27 for this simulation (last regret = 169.83).
- Policy 'Exp3R++($T=2000$, $c=0.586$, $\alpha=0.294$)'	was ranked	18 / 27 for this simulation (last regret = 172.03).
- Policy 'SW-UCB+($\tau=493$, $\alpha=1$)'	was ranked	19 / 27 for this simulation (last regret = 172.28).
- Policy 'D-UCB+($\alpha=1$, $\gamma=0.9875$)'	was ranked	20 / 27 for this simulation (last regret = 177.43).
- Policy 'SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global)'	was ranked	21 / 27 for this simulation (last regret = 180.95).
- Policy 'Exp3++'	was ranked	22 / 27 for this simulation (last regret = 196.2).
- Policy 'Exp3R($T=2000$, $c=3.18$, $\alpha=0.01$)'	was ranked	23 / 27 for this simulation (last regret = 202.95).
- Policy 'DiscountedThompson($\gamma=0.9$)'	was ranked	24 / 27 for this simulation (last regret = 213.6).
- Policy 'DiscountedThompson($\gamma=0.7$)'	was ranked	25 / 27 for this simulation (last regret = 224.1).
- Policy 'AdSwitch($T=2000$, $C_1=1$, $C_2=1$)'	was ranked	26 / 27 for this simulation (last regret = 235.3).
- Policy 'LM-DSEE($\gamma=200$, $\rho=0.6$, $\ell=2.59e+03$, $a=1$, $b=0.25$)'	was ranked	27 / 27 for this simulation (last regret = 348.8).

Giving the mean and std running times ...

For policy #10 called 'LM-DSEE($\gamma=200$, $\rho=0.6$, $\ell=2.59e+03$, $a=1$, $b=0.25$)' ...
    18.1 ms ± 3.07 ms per loop (mean ± std. dev. of 4 runs)

For policy #3 called 'Thompson' ...
    122 ms ± 19.7 ms per loop (mean ± std. dev. of 4 runs)

For policy #6 called 'DiscountedThompson($\gamma=0.7$)' ...
    132 ms ± 21.3 ms per loop (mean ± std. dev. of 4 runs)

For policy #5 called 'DiscountedThompson($\gamma=0.9$)' ...
    135 ms ± 24.5 ms per loop (mean ± std. dev. of 4 runs)

For policy #4 called 'DiscountedThompson($\gamma=0.99$)' ...
    144 ms ± 6.92 ms per loop (mean ± std. dev. of 4 runs)

For policy #1 called 'UCB($\alpha=1$)' ...
    175 ms ± 20.9 ms per loop (mean ± std. dev. of 4 runs)

For policy #25 called 'D-UCB+($\alpha=1$, $\gamma=0.9875$)' ...
    186 ms ± 9.27 ms per loop (mean ± std. dev. of 4 runs)

For policy #24 called 'SW-UCB+($\tau=493$, $\alpha=1$)' ...
    202 ms ± 11.7 ms per loop (mean ± std. dev. of 4 runs)

For policy #26 called 'OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm)' ...
    267 ms ± 15.6 ms per loop (mean ± std. dev. of 4 runs)

For policy #2 called 'kl-UCB' ...
    320 ms ± 54.6 ms per loop (mean ± std. dev. of 4 runs)

For policy #22 called 'M-klUCB($w=80$, $b=26.0719$, $\gamma=0.0099$, Global)' ...
    360 ms ± 9.33 ms per loop (mean ± std. dev. of 4 runs)

For policy #21 called 'M-klUCB($w=80$, $b=26.0719$, $\gamma=0.0099$, Per-Arm)' ...
    374 ms ± 10.7 ms per loop (mean ± std. dev. of 4 runs)

For policy #0 called 'Exp3++' ...
    1.03 s ± 72.1 ms per loop (mean ± std. dev. of 4 runs)

For policy #7 called 'Exp3R($T=2000$, $c=3.18$, $\alpha=0.01$)' ...
    1.65 s ± 140 ms per loop (mean ± std. dev. of 4 runs)

For policy #8 called 'Exp3R++($T=2000$, $c=0.586$, $\alpha=0.294$)' ...
    2 s ± 89.9 ms per loop (mean ± std. dev. of 4 runs)

For policy #23 called 'SW-UCB#($\lambda=1$, $\alpha=1$)' ...
    2.23 s ± 15.6 ms per loop (mean ± std. dev. of 4 runs)

For policy #20 called 'SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global)' ...
    4.68 s ± 1.01 s per loop (mean ± std. dev. of 4 runs)

For policy #12 called 'CUSUM-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Global)' ...
    7.26 s ± 202 ms per loop (mean ± std. dev. of 4 runs)

For policy #11 called 'CUSUM-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Per-Arm)' ...
    7.29 s ± 169 ms per loop (mean ± std. dev. of 4 runs)

For policy #19 called 'SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm)' ...
    7.56 s ± 3.27 s per loop (mean ± std. dev. of 4 runs)

For policy #13 called 'PHT-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Per-Arm)' ...
    9.9 s ± 213 ms per loop (mean ± std. dev. of 4 runs)

For policy #14 called 'PHT-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Global)' ...
    11.9 s ± 291 ms per loop (mean ± std. dev. of 4 runs)

For policy #9 called 'AdSwitch($T=2000$, $C_1=1$, $C_2=1$)' ...
    33.8 s ± 33.8 s per loop (mean ± std. dev. of 4 runs)

For policy #17 called 'GaussianGLR-klUCB(Per-Arm)' ...
    34.7 s ± 538 ms per loop (mean ± std. dev. of 4 runs)

For policy #15 called 'BernoulliGLR-klUCB(Per-Arm)' ...
    36.5 s ± 697 ms per loop (mean ± std. dev. of 4 runs)

For policy #18 called 'GaussianGLR-klUCB(Global)' ...
    37.3 s ± 984 ms per loop (mean ± std. dev. of 4 runs)

For policy #16 called 'BernoulliGLR-klUCB(Global)' ...
    37.4 s ± 546 ms per loop (mean ± std. dev. of 4 runs)

Giving the mean and std memory consumption ...

For policy #0 called 'Exp3++' ...
    206 B ± 202.1 B (mean ± std. dev. of 4 runs)

For policy #1 called 'UCB($\alpha=1$)' ...
    529 B ± 123 B (mean ± std. dev. of 4 runs)

For policy #3 called 'Thompson' ...
    638.2 B ± 188.4 B (mean ± std. dev. of 4 runs)

For policy #5 called 'DiscountedThompson($\gamma=0.9$)' ...
    742.5 B ± 255.5 B (mean ± std. dev. of 4 runs)

For policy #25 called 'D-UCB+($\alpha=1$, $\gamma=0.9875$)' ...
    841 B ± 483.2 B (mean ± std. dev. of 4 runs)

For policy #6 called 'DiscountedThompson($\gamma=0.7$)' ...
    890 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #4 called 'DiscountedThompson($\gamma=0.99$)' ...
    891 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #2 called 'kl-UCB' ...
    959 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #15 called 'BernoulliGLR-klUCB(Per-Arm)' ...
    4.9 KiB ± 50.5 B (mean ± std. dev. of 4 runs)

For policy #24 called 'SW-UCB+($\tau=493$, $\alpha=1$)' ...
    8.5 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #20 called 'SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Global)' ...
    13.3 KiB ± 10.5 KiB (mean ± std. dev. of 4 runs)

For policy #10 called 'LM-DSEE($\gamma=200$, $\rho=0.6$, $\ell=2.59e+03$, $a=1$, $b=0.25$)' ...
    16.3 KiB ± 9.4 KiB (mean ± std. dev. of 4 runs)

For policy #19 called 'SubGaussian-GLR-klUCB($\delta=0.001$, $\sigma=0.25$, joint, Per-Arm)' ...
    17.1 KiB ± 12.7 KiB (mean ± std. dev. of 4 runs)

For policy #26 called 'OracleRestart-klUCB($\Upsilon_T=4$, Per-Arm)' ...
    19.2 KiB ± 3.9 KiB (mean ± std. dev. of 4 runs)

For policy #7 called 'Exp3R($T=2000$, $c=3.18$, $\alpha=0.01$)' ...
    30.9 KiB ± 30.5 KiB (mean ± std. dev. of 4 runs)

For policy #17 called 'GaussianGLR-klUCB(Per-Arm)' ...
    31.5 KiB ± 31.2 KiB (mean ± std. dev. of 4 runs)

For policy #8 called 'Exp3R++($T=2000$, $c=0.586$, $\alpha=0.294$)' ...
    46.3 KiB ± 26.5 KiB (mean ± std. dev. of 4 runs)

For policy #14 called 'PHT-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Global)' ...
    46.4 KiB ± 26.8 KiB (mean ± std. dev. of 4 runs)

For policy #16 called 'BernoulliGLR-klUCB(Global)' ...
    47.1 KiB ± 27.1 KiB (mean ± std. dev. of 4 runs)

For policy #18 called 'GaussianGLR-klUCB(Global)' ...
    47.1 KiB ± 27 KiB (mean ± std. dev. of 4 runs)

For policy #22 called 'M-klUCB($w=80$, $b=26.0719$, $\gamma=0.0099$, Global)' ...
    61.8 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #21 called 'M-klUCB($w=80$, $b=26.0719$, $\gamma=0.0099$, Per-Arm)' ...
    61.8 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #13 called 'PHT-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Per-Arm)' ...
    61.8 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #12 called 'CUSUM-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Global)' ...
    61.8 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #11 called 'CUSUM-klUCB($\varepsilon=0.5$, $\Upsilon_T=5$, $M=100$, Per-Arm)' ...
    61.8 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #9 called 'AdSwitch($T=2000$, $C_1=1$, $C_2=1$)' ...
    66.8 KiB ± 0.5 B (mean ± std. dev. of 4 runs)

For policy #23 called 'SW-UCB#($\lambda=1$, $\alpha=1$)' ...
    121.5 KiB ± 0 B (mean ± std. dev. of 4 runs)
Warning: forcing to use putatright = False because there is 2 items in the legend.

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 2.73 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 45.00% ...
Warning: forcing to use putatright = True because there is 28 items in the legend.

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 2.73 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 45.00% ...
Warning: forcing to use putatright = True because there is 27 items in the legend.

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 2.73 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 45.00% ...
Warning: forcing to use putatright = True because there is 27 items in the legend.
Warning: forcing to use putatright = True because there is 27 items in the legend.
Warning: forcing to use putatright = True because there is 27 items in the legend.
Warning: forcing to use putatright = True because there is 27 items in the legend.
Warning: forcing to use putatright = True because there is 27 items in the legend.

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 2.73 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 45.00% ...
Warning: forcing to use putatright = True because there is 27 items in the legend.
Warning: forcing to use putatright = True because there is 27 items in the legend.
