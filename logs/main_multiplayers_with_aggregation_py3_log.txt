Using cli env variable to use MEANS = [0.01, 0.02, 0.03, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2].
  Using this already created player 'children[0]' = #1<RhoRand-UCB> ...
  Using this already created player 'children[1]' = #1<RandTopM-UCB> ...
  Using this already created player 'children[2]' = #1<MCTopM-UCB> ...
  Using this already created player 'children[3]' = #1<Selfish-UCB> ...
  Using this already created player 'children[0]' = #2<RhoRand-UCB> ...
  Using this already created player 'children[1]' = #2<RandTopM-UCB> ...
  Using this already created player 'children[2]' = #2<MCTopM-UCB> ...
  Using this already created player 'children[3]' = #2<Selfish-UCB> ...
  Using this already created player 'children[0]' = #3<RhoRand-UCB> ...
  Using this already created player 'children[1]' = #3<RandTopM-UCB> ...
  Using this already created player 'children[2]' = #3<MCTopM-UCB> ...
  Using this already created player 'children[3]' = #3<Selfish-UCB> ...
  Using this already created player 'children[0]' = #4<RhoRand-UCB> ...
  Using this already created player 'children[1]' = #4<RandTopM-UCB> ...
  Using this already created player 'children[2]' = #4<MCTopM-UCB> ...
  Using this already created player 'children[3]' = #4<Selfish-UCB> ...
  Using this already created player 'children[0]' = #5<RhoRand-UCB> ...
  Using this already created player 'children[1]' = #5<RandTopM-UCB> ...
  Using this already created player 'children[2]' = #5<MCTopM-UCB> ...
  Using this already created player 'children[3]' = #5<Selfish-UCB> ...
  Using this already created player 'children[0]' = #6<RhoRand-UCB> ...
  Using this already created player 'children[1]' = #6<RandTopM-UCB> ...
  Using this already created player 'children[2]' = #6<MCTopM-UCB> ...
  Using this already created player 'children[3]' = #6<Selfish-UCB> ...
 - One new child, of index 0, and class #1<CentralizedMultiplePlay(UCB)> ...
 - One new child, of index 1, and class #2<CentralizedMultiplePlay(UCB)> ...
 - One new child, of index 2, and class #3<CentralizedMultiplePlay(UCB)> ...
 - One new child, of index 3, and class #4<CentralizedMultiplePlay(UCB)> ...
 - One new child, of index 4, and class #5<CentralizedMultiplePlay(UCB)> ...
 - One new child, of index 5, and class #6<CentralizedMultiplePlay(UCB)> ...
Loaded experiments configuration from 'configuration.py' :
configuration = {'horizon': 10000, 'repetitions': 4, 'n_jobs': -1, 'verbosity': 6, 'collisionModel': <function onlyUniqUserGetsReward at 0x7f89706c5840>, 'finalRanksOnAverage': True, 'averageOn': 0.001, 'plot_lowerbounds': False, 'environment': [{'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.01, 0.02, 0.03, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2]}], 'change_labels': {0: 'Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB', 1: 'rhoRand UCB', 2: 'RandTopM UCB', 3: 'MCTopM UCB', 4: 'Selfish UCB', 5: 'Centralized UCB'}, 'successive_players': [[<Policies.Aggregator.Aggregator object at 0x7f896a753da0>, <Policies.Aggregator.Aggregator object at 0x7f896a75d978>, <Policies.Aggregator.Aggregator object at 0x7f896a768550>, <Policies.Aggregator.Aggregator object at 0x7f896a772128>, <Policies.Aggregator.Aggregator object at 0x7f896a772cc0>, <Policies.Aggregator.Aggregator object at 0x7f896a676898>], [rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB)], [RandTopM(UCB), RandTopM(UCB), RandTopM(UCB), RandTopM(UCB), RandTopM(UCB), RandTopM(UCB)], [MCTopM(UCB), MCTopM(UCB), MCTopM(UCB), MCTopM(UCB), MCTopM(UCB), MCTopM(UCB)], [Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB)], [CentralizedMultiplePlay(UCB), CentralizedMultiplePlay(UCB), CentralizedMultiplePlay(UCB), CentralizedMultiplePlay(UCB), CentralizedMultiplePlay(UCB), CentralizedMultiplePlay(UCB)]], 'players': [Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB)]}
====> SAVING FIGURES <=====
plots/ is already a directory here...



Considering the list of players :
 [<Policies.Aggregator.Aggregator object at 0x7f896a753da0>, <Policies.Aggregator.Aggregator object at 0x7f896a75d978>, <Policies.Aggregator.Aggregator object at 0x7f896a768550>, <Policies.Aggregator.Aggregator object at 0x7f896a772128>, <Policies.Aggregator.Aggregator object at 0x7f896a772cc0>, <Policies.Aggregator.Aggregator object at 0x7f896a676898>]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 4
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: -1
Using collision model onlyUniqUserGetsReward (function <function onlyUniqUserGetsReward at 0x7f89706c5840>).
More details:
 Simple collision model where only the players alone on one arm samples it and receives the reward.

    - This is the default collision model, cf. [[Multi-Player Bandits Revisited, Lilian Besson and Emilie Kaufmann, 2017]](https://hal.inria.fr/hal-01629733).
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Using accurate regrets and last regrets ? True


Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.01, 0.02, 0.03, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.01, 0.02, 0.03, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2]
 - with 'arms' = [B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)]
 - with 'means' = [0.01 0.02 0.03 0.1  0.12 0.14 0.16 0.18 0.2 ]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.2
 - with 'minArm' = 0.01

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 37.9 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 79.56% ...
 - with 'arms' represented as: $[B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)^*]$
Number of environments to try: 1


Evaluating environment: MAB(nbArms: 9, arms: [B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)], minArm: 0.01, maxArm: 0.2)
- Adding player # 1 = Aggregator($N=4$) ...
  Using this already created player 'player' = Aggregator($N=4$) ...
- Adding player # 2 = Aggregator($N=4$) ...
  Using this already created player 'player' = Aggregator($N=4$) ...
- Adding player # 3 = Aggregator($N=4$) ...
  Using this already created player 'player' = Aggregator($N=4$) ...
- Adding player # 4 = Aggregator($N=4$) ...
  Using this already created player 'player' = Aggregator($N=4$) ...
- Adding player # 5 = Aggregator($N=4$) ...
  Using this already created player 'player' = Aggregator($N=4$) ...
- Adding player # 6 = Aggregator($N=4$) ...
  Using this already created player 'player' = Aggregator($N=4$) ...

Estimated order by the policy Aggregator($N=4$) after 10000 steps: [2 0 1 3 4 5 8 6 7] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 79.01% (relative success)...

Estimated order by the policy Aggregator($N=4$) after 10000 steps: [2 0 3 1 4 8 6 5 7] ...
  ==> Optimal arm identification: 91.11% (relative success)...
  ==> Mean distance from optimal ordering: 57.41% (relative success)...

Estimated order by the policy Aggregator($N=4$) after 10000 steps: [2 0 1 3 4 5 6 8 7] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 81.48% (relative success)...

Estimated order by the policy Aggregator($N=4$) after 10000 steps: [2 1 0 3 4 6 7 5 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 73.46% (relative success)...

Estimated order by the policy Aggregator($N=4$) after 10000 steps: [0 1 2 3 4 5 8 6 7] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 89.51% (relative success)...

Estimated order by the policy Aggregator($N=4$) after 10000 steps: [0 2 1 3 4 5 8 6 7] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 81.48% (relative success)...

Giving the final ranks ...

Final ranking for this environment # 0 : Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB ...
- Player # 2 / 6, Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB	was ranked	1 / 6 for this simulation (last rewards = 663.75).
- Player # 1 / 6, Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB	was ranked	2 / 6 for this simulation (last rewards = 623.25).
- Player # 4 / 6, Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB	was ranked	3 / 6 for this simulation (last rewards = 611.5).
- Player # 3 / 6, Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB	was ranked	4 / 6 for this simulation (last rewards = 579).
- Player # 5 / 6, Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB	was ranked	5 / 6 for this simulation (last rewards = 575.75).
- Player # 6 / 6, Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB	was ranked	6 / 6 for this simulation (last rewards = 567).

Giving the vector of final regrets ...

For evaluator # 1/1 : <Environment.EvaluatorMultiPlayers.EvaluatorMultiPlayers object at 0x7f8988803748> (players Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB) ...
  Last regrets vector (for all repetitions) is:
Min of    last regrets R_T = 5241.09
Mean of   last regrets R_T = 5379.845
Median of last regrets R_T = 5396.165
Max of    last regrets R_T = 5485.96
STD of    last regrets R_T = 91.72203402127539

Giving the mean and std running times ...

For players called 'Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB' ...
    3min 47s ± 290 ms per loop (mean ± std. dev. of 4 runs)

Giving the mean and std memory consumption ...

For players called 'Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB' ...
    33 KiB ± 136.1 B (mean ± std. dev. of 4 runs)
Error: when saving the Evaluator object to a HDF5 file, the attribute named change_labels (value {0: 'Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB', 1: 'rhoRand UCB', 2: 'RandTopM UCB', 3: 'MCTopM UCB', 4: 'Selfish UCB', 5: 'Centralized UCB'} of type <class 'dict'>) couldn't be saved. Skipping...
Error: when saving the Evaluator object to a HDF5 file, the attribute named append_labels (value {} of type <class 'dict'>) couldn't be saved. Skipping...
Error: when saving the Evaluator object to a HDF5 file, the attribute named _sparsity (value None of type <class 'NoneType'>) couldn't be saved. Skipping...



Considering the list of players :
 [rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB)]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 4
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: -1
Using collision model onlyUniqUserGetsReward (function <function onlyUniqUserGetsReward at 0x7f89706c5840>).
More details:
 Simple collision model where only the players alone on one arm samples it and receives the reward.

    - This is the default collision model, cf. [[Multi-Player Bandits Revisited, Lilian Besson and Emilie Kaufmann, 2017]](https://hal.inria.fr/hal-01629733).
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Using accurate regrets and last regrets ? True


Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.01, 0.02, 0.03, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.01, 0.02, 0.03, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2]
 - with 'arms' = [B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)]
 - with 'means' = [0.01 0.02 0.03 0.1  0.12 0.14 0.16 0.18 0.2 ]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.2
 - with 'minArm' = 0.01

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 37.9 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 79.56% ...
 - with 'arms' represented as: $[B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)^*]$
Number of environments to try: 1


Evaluating environment: MAB(nbArms: 9, arms: [B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)], minArm: 0.01, maxArm: 0.2)
- Adding player # 1 = #1<RhoRand-UCB> ...
  Using this already created player 'player' = #1<RhoRand-UCB> ...
- Adding player # 2 = #2<RhoRand-UCB> ...
  Using this already created player 'player' = #2<RhoRand-UCB> ...
- Adding player # 3 = #3<RhoRand-UCB> ...
  Using this already created player 'player' = #3<RhoRand-UCB> ...
- Adding player # 4 = #4<RhoRand-UCB> ...
  Using this already created player 'player' = #4<RhoRand-UCB> ...
- Adding player # 5 = #5<RhoRand-UCB> ...
  Using this already created player 'player' = #5<RhoRand-UCB> ...
- Adding player # 6 = #6<RhoRand-UCB> ...
  Using this already created player 'player' = #6<RhoRand-UCB> ...

Estimated order by the policy #1<RhoRand-UCB> after 10000 steps: [1 2 0 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 89.51% (relative success)...

Estimated order by the policy #2<RhoRand-UCB> after 10000 steps: [0 6 4 2 1 3 5 7 8] ...
  ==> Optimal arm identification: 74.44% (relative success)...
  ==> Mean distance from optimal ordering: 54.94% (relative success)...

Estimated order by the policy #3<RhoRand-UCB> after 10000 steps: [0 3 1 2 4 5 6 7 8] ...
  ==> Optimal arm identification: 92.22% (relative success)...
  ==> Mean distance from optimal ordering: 89.51% (relative success)...

Estimated order by the policy #4<RhoRand-UCB> after 10000 steps: [2 0 1 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 89.51% (relative success)...

Estimated order by the policy #5<RhoRand-UCB> after 10000 steps: [2 1 0 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 83.95% (relative success)...

Estimated order by the policy #6<RhoRand-UCB> after 10000 steps: [2 0 1 3 4 5 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 81.48% (relative success)...

Giving the final ranks ...

Final ranking for this environment # 0 : rhoRand UCB ...
- Player # 4 / 6, rhoRand UCB	was ranked	1 / 6 for this simulation (last rewards = 968.5).
- Player # 1 / 6, rhoRand UCB	was ranked	2 / 6 for this simulation (last rewards = 931.75).
- Player # 3 / 6, rhoRand UCB	was ranked	3 / 6 for this simulation (last rewards = 926).
- Player # 2 / 6, rhoRand UCB	was ranked	4 / 6 for this simulation (last rewards = 920).
- Player # 6 / 6, rhoRand UCB	was ranked	5 / 6 for this simulation (last rewards = 905.5).
- Player # 5 / 6, rhoRand UCB	was ranked	6 / 6 for this simulation (last rewards = 902.75).

Giving the vector of final regrets ...

For evaluator # 1/1 : <Environment.EvaluatorMultiPlayers.EvaluatorMultiPlayers object at 0x7f89886ad748> (players rhoRand UCB) ...
  Last regrets vector (for all repetitions) is:
Min of    last regrets R_T = 3365.1099999999997
Mean of   last regrets R_T = 3495.4475
Median of last regrets R_T = 3462.2100000000005
Max of    last regrets R_T = 3692.2599999999993
STD of    last regrets R_T = 130.12463936069125

Giving the mean and std running times ...

For players called 'rhoRand UCB' ...
    29.7 s ± 146 ms per loop (mean ± std. dev. of 4 runs)

Giving the mean and std memory consumption ...

For players called 'rhoRand UCB' ...
    nan YiB ± nan YiB (mean ± std. dev. of 4 runs)
plots/MP__K9_M6_T10000_N4__6_algos is already a directory here...
Error: when saving the Evaluator object to a HDF5 file, the attribute named change_labels (value {0: 'Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB', 1: 'rhoRand UCB', 2: 'RandTopM UCB', 3: 'MCTopM UCB', 4: 'Selfish UCB', 5: 'Centralized UCB'} of type <class 'dict'>) couldn't be saved. Skipping...
Error: when saving the Evaluator object to a HDF5 file, the attribute named append_labels (value {} of type <class 'dict'>) couldn't be saved. Skipping...
Error: when saving the Evaluator object to a HDF5 file, the attribute named _sparsity (value None of type <class 'NoneType'>) couldn't be saved. Skipping...



Considering the list of players :
 [RandTopM(UCB), RandTopM(UCB), RandTopM(UCB), RandTopM(UCB), RandTopM(UCB), RandTopM(UCB)]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 4
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: -1
Using collision model onlyUniqUserGetsReward (function <function onlyUniqUserGetsReward at 0x7f89706c5840>).
More details:
 Simple collision model where only the players alone on one arm samples it and receives the reward.

    - This is the default collision model, cf. [[Multi-Player Bandits Revisited, Lilian Besson and Emilie Kaufmann, 2017]](https://hal.inria.fr/hal-01629733).
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Using accurate regrets and last regrets ? True


Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.01, 0.02, 0.03, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.01, 0.02, 0.03, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2]
 - with 'arms' = [B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)]
 - with 'means' = [0.01 0.02 0.03 0.1  0.12 0.14 0.16 0.18 0.2 ]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.2
 - with 'minArm' = 0.01

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 37.9 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 79.56% ...
 - with 'arms' represented as: $[B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)^*]$
Number of environments to try: 1


Evaluating environment: MAB(nbArms: 9, arms: [B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)], minArm: 0.01, maxArm: 0.2)
- Adding player # 1 = #1<RandTopM-UCB> ...
  Using this already created player 'player' = #1<RandTopM-UCB> ...
- Adding player # 2 = #2<RandTopM-UCB> ...
  Using this already created player 'player' = #2<RandTopM-UCB> ...
- Adding player # 3 = #3<RandTopM-UCB> ...
  Using this already created player 'player' = #3<RandTopM-UCB> ...
- Adding player # 4 = #4<RandTopM-UCB> ...
  Using this already created player 'player' = #4<RandTopM-UCB> ...
- Adding player # 5 = #5<RandTopM-UCB> ...
  Using this already created player 'player' = #5<RandTopM-UCB> ...
- Adding player # 6 = #6<RandTopM-UCB> ...
  Using this already created player 'player' = #6<RandTopM-UCB> ...

Estimated order by the policy #1<RandTopM-UCB, $M$-best: $[3, 5, 4, 6, 7, 8]$, arm: $7$> after 10000 steps: [2 1 0 3 5 4 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 75.93% (relative success)...

Estimated order by the policy #2<RandTopM-UCB, $M$-best: $[0, 5, 4, 6, 7, 8]$, arm: $5$> after 10000 steps: [2 3 1 0 5 4 6 7 8] ...
  ==> Optimal arm identification: 90.00% (relative success)...
  ==> Mean distance from optimal ordering: 70.99% (relative success)...

Estimated order by the policy #3<RandTopM-UCB, $M$-best: $[0, 5, 6, 4, 7, 8]$, arm: $4$> after 10000 steps: [3 2 1 0 5 6 4 7 8] ...
  ==> Optimal arm identification: 90.00% (relative success)...
  ==> Mean distance from optimal ordering: 62.96% (relative success)...

Estimated order by the policy #4<RandTopM-UCB, $M$-best: $[3, 4, 5, 7, 6, 8]$, arm: $3$> after 10000 steps: [0 1 2 3 4 5 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 91.98% (relative success)...

Estimated order by the policy #5<RandTopM-UCB, $M$-best: $[1, 4, 5, 7, 6, 8]$, arm: $8$> after 10000 steps: [0 2 3 1 4 5 7 6 8] ...
  ==> Optimal arm identification: 91.11% (relative success)...
  ==> Mean distance from optimal ordering: 81.48% (relative success)...

Estimated order by the policy #6<RandTopM-UCB, $M$-best: $[5, 2, 4, 6, 7, 8]$, arm: $6$> after 10000 steps: [1 3 0 5 2 4 6 7 8] ...
  ==> Optimal arm identification: 92.22% (relative success)...
  ==> Mean distance from optimal ordering: 70.99% (relative success)...

Giving the final ranks ...

Final ranking for this environment # 0 : RandTopM UCB ...
- Player # 5 / 6, RandTopM UCB	was ranked	1 / 6 for this simulation (last rewards = 1083).
- Player # 6 / 6, RandTopM UCB	was ranked	2 / 6 for this simulation (last rewards = 1083).
- Player # 4 / 6, RandTopM UCB	was ranked	3 / 6 for this simulation (last rewards = 1071).
- Player # 2 / 6, RandTopM UCB	was ranked	4 / 6 for this simulation (last rewards = 1068.2).
- Player # 1 / 6, RandTopM UCB	was ranked	5 / 6 for this simulation (last rewards = 1057.2).
- Player # 3 / 6, RandTopM UCB	was ranked	6 / 6 for this simulation (last rewards = 1036.2).

Giving the vector of final regrets ...

For evaluator # 1/1 : <Environment.EvaluatorMultiPlayers.EvaluatorMultiPlayers object at 0x7f8969f8e7b8> (players RandTopM UCB) ...
  Last regrets vector (for all repetitions) is:
Min of    last regrets R_T = 2553.9000000000005
Mean of   last regrets R_T = 2654.285
Median of last regrets R_T = 2661.5799999999995
Max of    last regrets R_T = 2740.079999999999
STD of    last regrets R_T = 85.70416282188347

Giving the mean and std running times ...

For players called 'RandTopM UCB' ...
    58.8 s ± 203 ms per loop (mean ± std. dev. of 4 runs)

Giving the mean and std memory consumption ...

For players called 'RandTopM UCB' ...
    nan YiB ± nan YiB (mean ± std. dev. of 4 runs)
plots/MP__K9_M6_T10000_N4__6_algos is already a directory here...
Error: when saving the Evaluator object to a HDF5 file, the attribute named change_labels (value {0: 'Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB', 1: 'rhoRand UCB', 2: 'RandTopM UCB', 3: 'MCTopM UCB', 4: 'Selfish UCB', 5: 'Centralized UCB'} of type <class 'dict'>) couldn't be saved. Skipping...
Error: when saving the Evaluator object to a HDF5 file, the attribute named append_labels (value {} of type <class 'dict'>) couldn't be saved. Skipping...
Error: when saving the Evaluator object to a HDF5 file, the attribute named _sparsity (value None of type <class 'NoneType'>) couldn't be saved. Skipping...



Considering the list of players :
 [MCTopM(UCB), MCTopM(UCB), MCTopM(UCB), MCTopM(UCB), MCTopM(UCB), MCTopM(UCB)]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 4
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: -1
Using collision model onlyUniqUserGetsReward (function <function onlyUniqUserGetsReward at 0x7f89706c5840>).
More details:
 Simple collision model where only the players alone on one arm samples it and receives the reward.

    - This is the default collision model, cf. [[Multi-Player Bandits Revisited, Lilian Besson and Emilie Kaufmann, 2017]](https://hal.inria.fr/hal-01629733).
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Using accurate regrets and last regrets ? True


Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.01, 0.02, 0.03, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.01, 0.02, 0.03, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2]
 - with 'arms' = [B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)]
 - with 'means' = [0.01 0.02 0.03 0.1  0.12 0.14 0.16 0.18 0.2 ]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.2
 - with 'minArm' = 0.01

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 37.9 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 79.56% ...
 - with 'arms' represented as: $[B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)^*]$
Number of environments to try: 1


Evaluating environment: MAB(nbArms: 9, arms: [B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)], minArm: 0.01, maxArm: 0.2)
- Adding player # 1 = #1<MCTopM-UCB> ...
  Using this already created player 'player' = #1<MCTopM-UCB> ...
- Adding player # 2 = #2<MCTopM-UCB> ...
  Using this already created player 'player' = #2<MCTopM-UCB> ...
- Adding player # 3 = #3<MCTopM-UCB> ...
  Using this already created player 'player' = #3<MCTopM-UCB> ...
- Adding player # 4 = #4<MCTopM-UCB> ...
  Using this already created player 'player' = #4<MCTopM-UCB> ...
- Adding player # 5 = #5<MCTopM-UCB> ...
  Using this already created player 'player' = #5<MCTopM-UCB> ...
- Adding player # 6 = #6<MCTopM-UCB> ...
  Using this already created player 'player' = #6<MCTopM-UCB> ...

Estimated order by the policy #1<MCTopM-UCB, $M$-best: $[4, 0, 5, 8, 6, 7]$, arm: $8$> after 10000 steps: [3 1 2 4 0 5 8 6 7] ...
  ==> Optimal arm identification: 90.00% (relative success)...
  ==> Mean distance from optimal ordering: 68.52% (relative success)...

Estimated order by the policy #2<MCTopM-UCB, $M$-best: $[2, 4, 5, 6, 8, 7]$, arm: $4$> after 10000 steps: [1 0 3 2 4 5 6 8 7] ...
  ==> Optimal arm identification: 92.22% (relative success)...
  ==> Mean distance from optimal ordering: 75.93% (relative success)...

Estimated order by the policy #3<MCTopM-UCB, $M$-best: $[1, 4, 5, 6, 7, 8]$, arm: $3$> after 10000 steps: [2 3 0 1 4 5 6 7 8] ...
  ==> Optimal arm identification: 91.11% (relative success)...
  ==> Mean distance from optimal ordering: 79.01% (relative success)...

Estimated order by the policy #4<MCTopM-UCB, $M$-best: $[0, 4, 5, 6, 7, 8]$, arm: $5$> after 10000 steps: [3 1 2 0 4 5 6 7 8] ...
  ==> Optimal arm identification: 90.00% (relative success)...
  ==> Mean distance from optimal ordering: 81.48% (relative success)...

Estimated order by the policy #5<MCTopM-UCB, $M$-best: $[1, 6, 4, 7, 5, 8]$, arm: $6$> after 10000 steps: [3 2 0 1 6 4 7 5 8] ...
  ==> Optimal arm identification: 91.11% (relative success)...
  ==> Mean distance from optimal ordering: 60.49% (relative success)...

Estimated order by the policy #6<MCTopM-UCB, $M$-best: $[1, 3, 5, 7, 8, 6]$, arm: $7$> after 10000 steps: [4 2 0 1 3 5 7 8 6] ...
  ==> Optimal arm identification: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 66.05% (relative success)...

Giving the final ranks ...

Final ranking for this environment # 0 : MCTopM UCB ...
- Player # 1 / 6, MCTopM UCB	was ranked	1 / 6 for this simulation (last rewards = 1244.8).
- Player # 6 / 6, MCTopM UCB	was ranked	2 / 6 for this simulation (last rewards = 1232).
- Player # 5 / 6, MCTopM UCB	was ranked	3 / 6 for this simulation (last rewards = 1165.5).
- Player # 3 / 6, MCTopM UCB	was ranked	4 / 6 for this simulation (last rewards = 1153.2).
- Player # 2 / 6, MCTopM UCB	was ranked	5 / 6 for this simulation (last rewards = 1151).
- Player # 4 / 6, MCTopM UCB	was ranked	6 / 6 for this simulation (last rewards = 1109.5).

Giving the vector of final regrets ...

For evaluator # 1/1 : <Environment.EvaluatorMultiPlayers.EvaluatorMultiPlayers object at 0x7f8969f35240> (players MCTopM UCB) ...
  Last regrets vector (for all repetitions) is:
Min of    last regrets R_T = 1892.0199999999995
Mean of   last regrets R_T = 1963.5324999999998
Median of last regrets R_T = 1977.0499999999997
Max of    last regrets R_T = 2008.0100000000002
STD of    last regrets R_T = 46.5803055888431

Giving the mean and std running times ...

For players called 'MCTopM UCB' ...
    51 s ± 195 ms per loop (mean ± std. dev. of 4 runs)

Giving the mean and std memory consumption ...

For players called 'MCTopM UCB' ...
    nan YiB ± nan YiB (mean ± std. dev. of 4 runs)
plots/MP__K9_M6_T10000_N4__6_algos is already a directory here...
Error: when saving the Evaluator object to a HDF5 file, the attribute named change_labels (value {0: 'Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB', 1: 'rhoRand UCB', 2: 'RandTopM UCB', 3: 'MCTopM UCB', 4: 'Selfish UCB', 5: 'Centralized UCB'} of type <class 'dict'>) couldn't be saved. Skipping...
Error: when saving the Evaluator object to a HDF5 file, the attribute named append_labels (value {} of type <class 'dict'>) couldn't be saved. Skipping...
Error: when saving the Evaluator object to a HDF5 file, the attribute named _sparsity (value None of type <class 'NoneType'>) couldn't be saved. Skipping...



Considering the list of players :
 [Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB)]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 4
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: -1
Using collision model onlyUniqUserGetsReward (function <function onlyUniqUserGetsReward at 0x7f89706c5840>).
More details:
 Simple collision model where only the players alone on one arm samples it and receives the reward.

    - This is the default collision model, cf. [[Multi-Player Bandits Revisited, Lilian Besson and Emilie Kaufmann, 2017]](https://hal.inria.fr/hal-01629733).
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Using accurate regrets and last regrets ? True


Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.01, 0.02, 0.03, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.01, 0.02, 0.03, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2]
 - with 'arms' = [B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)]
 - with 'means' = [0.01 0.02 0.03 0.1  0.12 0.14 0.16 0.18 0.2 ]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.2
 - with 'minArm' = 0.01

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 37.9 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 79.56% ...
 - with 'arms' represented as: $[B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)^*]$
Number of environments to try: 1


Evaluating environment: MAB(nbArms: 9, arms: [B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)], minArm: 0.01, maxArm: 0.2)
- Adding player # 1 = #1<Selfish-UCB> ...
  Using this already created player 'player' = #1<Selfish-UCB> ...
- Adding player # 2 = #2<Selfish-UCB> ...
  Using this already created player 'player' = #2<Selfish-UCB> ...
- Adding player # 3 = #3<Selfish-UCB> ...
  Using this already created player 'player' = #3<Selfish-UCB> ...
- Adding player # 4 = #4<Selfish-UCB> ...
  Using this already created player 'player' = #4<Selfish-UCB> ...
- Adding player # 5 = #5<Selfish-UCB> ...
  Using this already created player 'player' = #5<Selfish-UCB> ...
- Adding player # 6 = #6<Selfish-UCB> ...
  Using this already created player 'player' = #6<Selfish-UCB> ...

Estimated order by the policy #1<Selfish-UCB> after 10000 steps: [0 2 8 7 3 5 6 1 4] ...
  ==> Optimal arm identification: 80.00% (relative success)...
  ==> Mean distance from optimal ordering: 50.62% (relative success)...

Estimated order by the policy #2<Selfish-UCB> after 10000 steps: [7 8 4 3 2 5 0 1 6] ...
  ==> Optimal arm identification: 51.11% (relative success)...
  ==> Mean distance from optimal ordering: 21.60% (relative success)...

Estimated order by the policy #3<Selfish-UCB> after 10000 steps: [8 4 6 1 3 5 2 0 7] ...
  ==> Optimal arm identification: 53.33% (relative success)...
  ==> Mean distance from optimal ordering: 18.52% (relative success)...

Estimated order by the policy #4<Selfish-UCB> after 10000 steps: [1 5 6 7 3 2 0 8 4] ...
  ==> Optimal arm identification: 71.11% (relative success)...
  ==> Mean distance from optimal ordering: 43.21% (relative success)...

Estimated order by the policy #5<Selfish-UCB> after 10000 steps: [0 2 7 3 6 1 8 4 5] ...
  ==> Optimal arm identification: 82.22% (relative success)...
  ==> Mean distance from optimal ordering: 53.09% (relative success)...

Estimated order by the policy #6<Selfish-UCB> after 10000 steps: [7 5 6 4 3 2 1 0 8] ...
  ==> Optimal arm identification: 53.33% (relative success)...
  ==> Mean distance from optimal ordering: 27.16% (relative success)...

Giving the final ranks ...

Final ranking for this environment # 0 : Selfish UCB ...
- Player # 1 / 6, Selfish UCB	was ranked	1 / 6 for this simulation (last rewards = 774).
- Player # 6 / 6, Selfish UCB	was ranked	2 / 6 for this simulation (last rewards = 736.5).
- Player # 4 / 6, Selfish UCB	was ranked	3 / 6 for this simulation (last rewards = 736).
- Player # 5 / 6, Selfish UCB	was ranked	4 / 6 for this simulation (last rewards = 723.5).
- Player # 2 / 6, Selfish UCB	was ranked	5 / 6 for this simulation (last rewards = 721.5).
- Player # 3 / 6, Selfish UCB	was ranked	6 / 6 for this simulation (last rewards = 719.25).

Giving the vector of final regrets ...

For evaluator # 1/1 : <Environment.EvaluatorMultiPlayers.EvaluatorMultiPlayers object at 0x7f8969ec8748> (players Selfish UCB) ...
  Last regrets vector (for all repetitions) is:
Min of    last regrets R_T = 4463.1
Mean of   last regrets R_T = 4565.2375
Median of last regrets R_T = 4572.055
Max of    last regrets R_T = 4653.74
STD of    last regrets R_T = 73.72287513892803

Giving the mean and std running times ...

For players called 'Selfish UCB' ...
    27 s ± 90.9 ms per loop (mean ± std. dev. of 4 runs)

Giving the mean and std memory consumption ...

For players called 'Selfish UCB' ...
    nan YiB ± nan YiB (mean ± std. dev. of 4 runs)
plots/MP__K9_M6_T10000_N4__6_algos is already a directory here...
Error: when saving the Evaluator object to a HDF5 file, the attribute named change_labels (value {0: 'Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB', 1: 'rhoRand UCB', 2: 'RandTopM UCB', 3: 'MCTopM UCB', 4: 'Selfish UCB', 5: 'Centralized UCB'} of type <class 'dict'>) couldn't be saved. Skipping...
Error: when saving the Evaluator object to a HDF5 file, the attribute named append_labels (value {} of type <class 'dict'>) couldn't be saved. Skipping...
Error: when saving the Evaluator object to a HDF5 file, the attribute named _sparsity (value None of type <class 'NoneType'>) couldn't be saved. Skipping...



Considering the list of players :
 [CentralizedMultiplePlay(UCB), CentralizedMultiplePlay(UCB), CentralizedMultiplePlay(UCB), CentralizedMultiplePlay(UCB), CentralizedMultiplePlay(UCB), CentralizedMultiplePlay(UCB)]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 4
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: -1
Using collision model onlyUniqUserGetsReward (function <function onlyUniqUserGetsReward at 0x7f89706c5840>).
More details:
 Simple collision model where only the players alone on one arm samples it and receives the reward.

    - This is the default collision model, cf. [[Multi-Player Bandits Revisited, Lilian Besson and Emilie Kaufmann, 2017]](https://hal.inria.fr/hal-01629733).
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Using accurate regrets and last regrets ? True


Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.01, 0.02, 0.03, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.01, 0.02, 0.03, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2]
 - with 'arms' = [B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)]
 - with 'means' = [0.01 0.02 0.03 0.1  0.12 0.14 0.16 0.18 0.2 ]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.2
 - with 'minArm' = 0.01

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 37.9 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 79.56% ...
 - with 'arms' represented as: $[B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)^*]$
Number of environments to try: 1


Evaluating environment: MAB(nbArms: 9, arms: [B(0.01), B(0.02), B(0.03), B(0.1), B(0.12), B(0.14), B(0.16), B(0.18), B(0.2)], minArm: 0.01, maxArm: 0.2)
- Adding player # 1 = #1<CentralizedMultiplePlay(UCB)> ...
  Using this already created player 'player' = #1<CentralizedMultiplePlay(UCB)> ...
- Adding player # 2 = #2<CentralizedMultiplePlay(UCB)> ...
  Using this already created player 'player' = #2<CentralizedMultiplePlay(UCB)> ...
- Adding player # 3 = #3<CentralizedMultiplePlay(UCB)> ...
  Using this already created player 'player' = #3<CentralizedMultiplePlay(UCB)> ...
- Adding player # 4 = #4<CentralizedMultiplePlay(UCB)> ...
  Using this already created player 'player' = #4<CentralizedMultiplePlay(UCB)> ...
- Adding player # 5 = #5<CentralizedMultiplePlay(UCB)> ...
  Using this already created player 'player' = #5<CentralizedMultiplePlay(UCB)> ...
- Adding player # 6 = #6<CentralizedMultiplePlay(UCB)> ...
  Using this already created player 'player' = #6<CentralizedMultiplePlay(UCB)> ...

Estimated order by the policy #1<CentralizedMultiplePlay(UCB)> after 10000 steps: [0 1 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...

Estimated order by the policy #2<CentralizedMultiplePlay(UCB)> after 10000 steps: [0 1 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...

Estimated order by the policy #3<CentralizedMultiplePlay(UCB)> after 10000 steps: [0 1 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...

Estimated order by the policy #4<CentralizedMultiplePlay(UCB)> after 10000 steps: [0 1 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...

Estimated order by the policy #5<CentralizedMultiplePlay(UCB)> after 10000 steps: [0 1 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...

Estimated order by the policy #6<CentralizedMultiplePlay(UCB)> after 10000 steps: [0 1 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...

Giving the final ranks ...

Final ranking for this environment # 0 : Centralized UCB ...
- Player # 2 / 6, Centralized UCB	was ranked	1 / 6 for this simulation (last rewards = 1914.8).
- Player # 3 / 6, Centralized UCB	was ranked	2 / 6 for this simulation (last rewards = 1534.2).
- Player # 5 / 6, Centralized UCB	was ranked	3 / 6 for this simulation (last rewards = 1489.2).
- Player # 6 / 6, Centralized UCB	was ranked	4 / 6 for this simulation (last rewards = 1404).
- Player # 1 / 6, Centralized UCB	was ranked	5 / 6 for this simulation (last rewards = 1376.8).
- Player # 4 / 6, Centralized UCB	was ranked	6 / 6 for this simulation (last rewards = 949).

Giving the vector of final regrets ...

For evaluator # 1/1 : <Environment.EvaluatorMultiPlayers.EvaluatorMultiPlayers object at 0x7f8969ec83c8> (players Centralized UCB) ...
  Last regrets vector (for all repetitions) is:
Min of    last regrets R_T = 289.9899999999998
Mean of   last regrets R_T = 311.8024999999998
Median of last regrets R_T = 308.96000000000004
Max of    last regrets R_T = 339.2999999999993
STD of    last regrets R_T = 18.591243066293085

Giving the mean and std running times ...

For players called 'Centralized UCB' ...
    15.6 s ± 72.5 ms per loop (mean ± std. dev. of 4 runs)

Giving the mean and std memory consumption ...

For players called 'Centralized UCB' ...
    nan YiB ± nan YiB (mean ± std. dev. of 4 runs)
plots/MP__K9_M6_T10000_N4__6_algos is already a directory here...
Error: when saving the Evaluator object to a HDF5 file, the attribute named change_labels (value {0: 'Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB', 1: 'rhoRand UCB', 2: 'RandTopM UCB', 3: 'MCTopM UCB', 4: 'Selfish UCB', 5: 'Centralized UCB'} of type <class 'dict'>) couldn't be saved. Skipping...
Error: when saving the Evaluator object to a HDF5 file, the attribute named append_labels (value {} of type <class 'dict'>) couldn't be saved. Skipping...
Error: when saving the Evaluator object to a HDF5 file, the attribute named _sparsity (value None of type <class 'NoneType'>) couldn't be saved. Skipping...

Giving all the vector of final regrets ...

For evaluator # 1/6 : <Environment.EvaluatorMultiPlayers.EvaluatorMultiPlayers object at 0x7f8988803748> (players Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB) ...
  Last regrets vector (for all repetitions) is:
Min of    last regrets R_T = 5241.09
Mean of   last regrets R_T = 5379.845
Median of last regrets R_T = 5396.165
Max of    last regrets R_T = 5485.96
STD of    last regrets R_T = 91.72203402127539

For evaluator # 2/6 : <Environment.EvaluatorMultiPlayers.EvaluatorMultiPlayers object at 0x7f89886ad748> (players rhoRand UCB) ...
  Last regrets vector (for all repetitions) is:
Min of    last regrets R_T = 3365.1099999999997
Mean of   last regrets R_T = 3495.4475
Median of last regrets R_T = 3462.2100000000005
Max of    last regrets R_T = 3692.2599999999993
STD of    last regrets R_T = 130.12463936069125

For evaluator # 3/6 : <Environment.EvaluatorMultiPlayers.EvaluatorMultiPlayers object at 0x7f8969f8e7b8> (players RandTopM UCB) ...
  Last regrets vector (for all repetitions) is:
Min of    last regrets R_T = 2553.9000000000005
Mean of   last regrets R_T = 2654.285
Median of last regrets R_T = 2661.5799999999995
Max of    last regrets R_T = 2740.079999999999
STD of    last regrets R_T = 85.70416282188347

For evaluator # 4/6 : <Environment.EvaluatorMultiPlayers.EvaluatorMultiPlayers object at 0x7f8969f35240> (players MCTopM UCB) ...
  Last regrets vector (for all repetitions) is:
Min of    last regrets R_T = 1892.0199999999995
Mean of   last regrets R_T = 1963.5324999999998
Median of last regrets R_T = 1977.0499999999997
Max of    last regrets R_T = 2008.0100000000002
STD of    last regrets R_T = 46.5803055888431

For evaluator # 5/6 : <Environment.EvaluatorMultiPlayers.EvaluatorMultiPlayers object at 0x7f8969ec8748> (players Selfish UCB) ...
  Last regrets vector (for all repetitions) is:
Min of    last regrets R_T = 4463.1
Mean of   last regrets R_T = 4565.2375
Median of last regrets R_T = 4572.055
Max of    last regrets R_T = 4653.74
STD of    last regrets R_T = 73.72287513892803

For evaluator # 6/6 : <Environment.EvaluatorMultiPlayers.EvaluatorMultiPlayers object at 0x7f8969ec83c8> (players Centralized UCB) ...
  Last regrets vector (for all repetitions) is:
Min of    last regrets R_T = 289.9899999999998
Mean of   last regrets R_T = 311.8024999999998
Median of last regrets R_T = 308.96000000000004
Max of    last regrets R_T = 339.2999999999993
STD of    last regrets R_T = 18.591243066293085

Giving the final ranking ...
- Group of players # 6 / 6, Centralized UCB	was ranked	1 / 6 for this simulation (last rewards = 8668).
- Group of players # 4 / 6, MCTopM UCB	was ranked	2 / 6 for this simulation (last rewards = 7056).
- Group of players # 3 / 6, RandTopM UCB	was ranked	3 / 6 for this simulation (last rewards = 6398.8).
- Group of players # 2 / 6, rhoRand UCB	was ranked	4 / 6 for this simulation (last rewards = 5554.5).
- Group of players # 5 / 6, Selfish UCB	was ranked	5 / 6 for this simulation (last rewards = 4410.8).
- Group of players # 1 / 6, Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB	was ranked	6 / 6 for this simulation (last rewards = 3620.2).

Giving the mean and std running times ...

For players called 'Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB' ...
    3min 47s ± 290 ms per loop (mean ± std. dev. of 4 runs)

For players called 'rhoRand UCB' ...
    29.7 s ± 146 ms per loop (mean ± std. dev. of 4 runs)

For players called 'RandTopM UCB' ...
    58.8 s ± 203 ms per loop (mean ± std. dev. of 4 runs)

For players called 'MCTopM UCB' ...
    51 s ± 195 ms per loop (mean ± std. dev. of 4 runs)

For players called 'Selfish UCB' ...
    27 s ± 90.9 ms per loop (mean ± std. dev. of 4 runs)

For players called 'Centralized UCB' ...
    15.6 s ± 72.5 ms per loop (mean ± std. dev. of 4 runs)

Giving the mean and std memory consumption ...

For players called 'Aggr(rhoRand, RandTopM, MCTopM, Selfish) UCB' ...
    33 KiB ± 136.1 B (mean ± std. dev. of 4 runs)

For players called 'rhoRand UCB' ...
    nan YiB ± nan YiB (mean ± std. dev. of 4 runs)

For players called 'RandTopM UCB' ...
    nan YiB ± nan YiB (mean ± std. dev. of 4 runs)

For players called 'MCTopM UCB' ...
    nan YiB ± nan YiB (mean ± std. dev. of 4 runs)

For players called 'Selfish UCB' ...
    nan YiB ± nan YiB (mean ± std. dev. of 4 runs)

For players called 'Centralized UCB' ...
    nan YiB ± nan YiB (mean ± std. dev. of 4 runs)
 - Plotting the running times, and saving the plot to plots/MP__K9_M6_T10000_N4__6_algos/all_RunningTimes____env1-1_1790697744797155451 ...
Warning: forcing to use putatright = False because there is 0 items in the legend.
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_RunningTimes____env1-1_1790697744797155451.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_RunningTimes____env1-1_1790697744797155451.pickle' created of size '170010b', at 'Wed Oct 10 10:58:19 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_RunningTimes____env1-1_1790697744797155451.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_RunningTimes____env1-1_1790697744797155451.png' created of size '159924b', at 'Wed Oct 10 10:58:19 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_RunningTimes____env1-1_1790697744797155451.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_RunningTimes____env1-1_1790697744797155451.pdf' created of size '29046b', at 'Wed Oct 10 10:58:20 2018' ...
 - Plotting the memory consumption, and saving the plot to plots/MP__K9_M6_T10000_N4__6_algos/all_MemoryConsumption____env1-1_1790697744797155451 ...
Warning: forcing to use putatright = False because there is 0 items in the legend.
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_MemoryConsumption____env1-1_1790697744797155451.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_MemoryConsumption____env1-1_1790697744797155451.pickle' created of size '170016b', at 'Wed Oct 10 10:58:20 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_MemoryConsumption____env1-1_1790697744797155451.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_MemoryConsumption____env1-1_1790697744797155451.png' created of size '161761b', at 'Wed Oct 10 10:58:20 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_MemoryConsumption____env1-1_1790697744797155451.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_MemoryConsumption____env1-1_1790697744797155451.pdf' created of size '29461b', at 'Wed Oct 10 10:58:20 2018' ...


- Plotting the centralized regret for all 'players' values
  and saving the plot to plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized____env1-1_1790697744797155451 ...
 -  For 6 players, Anandtharam et al. centralized lower-bound gave = 4.74 ...
 -  For 6 players, our lower bound gave = 28.4 ...
 -  For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 16.4 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 37.9 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 79.56% ...
 - [Anandtharam et al] centralized lower-bound = 4.74,
 - [Anandkumar et al] decentralized lower-bound = 16.4
 - Our better (larger) decentralized lower-bound = 28.4,
Warning: forcing to use putatright = False because there is 6 items in the legend.
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized____env1-1_1790697744797155451.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized____env1-1_1790697744797155451.pickle' created of size '3936938b', at 'Wed Oct 10 10:58:21 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized____env1-1_1790697744797155451.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized____env1-1_1790697744797155451.png' created of size '241905b', at 'Wed Oct 10 10:58:21 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized____env1-1_1790697744797155451.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized____env1-1_1790697744797155451.pdf' created of size '62186b', at 'Wed Oct 10 10:58:21 2018' ...


- Plotting the centralized regret for all 'players' values, in semilogx scale
  and saving the plot to plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_semilogx____env1-1_1790697744797155451 ...
 -  For 6 players, Anandtharam et al. centralized lower-bound gave = 4.74 ...
 -  For 6 players, our lower bound gave = 28.4 ...
 -  For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 16.4 ...
 - [Anandtharam et al] centralized lower-bound = 4.74,
 - [Anandkumar et al] decentralized lower-bound = 16.4
 - Our better (larger) decentralized lower-bound = 28.4,
Warning: forcing to use putatright = False because there is 6 items in the legend.
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_semilogx____env1-1_1790697744797155451.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_semilogx____env1-1_1790697744797155451.pickle' created of size '3898363b', at 'Wed Oct 10 10:58:22 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_semilogx____env1-1_1790697744797155451.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_semilogx____env1-1_1790697744797155451.png' created of size '222283b', at 'Wed Oct 10 10:58:22 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_semilogx____env1-1_1790697744797155451.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_semilogx____env1-1_1790697744797155451.pdf' created of size '58722b', at 'Wed Oct 10 10:58:22 2018' ...


- Plotting the centralized regret for all 'players' values, in semilogy scale
  and saving the plot to plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_semilogy____env1-1_1790697744797155451 ...
 -  For 6 players, Anandtharam et al. centralized lower-bound gave = 4.74 ...
 -  For 6 players, our lower bound gave = 28.4 ...
 -  For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 16.4 ...
 - [Anandtharam et al] centralized lower-bound = 4.74,
 - [Anandkumar et al] decentralized lower-bound = 16.4
 - Our better (larger) decentralized lower-bound = 28.4,
Warning: forcing to use putatright = False because there is 6 items in the legend.
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_semilogy____env1-1_1790697744797155451.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_semilogy____env1-1_1790697744797155451.pickle' created of size '3936763b', at 'Wed Oct 10 10:58:23 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_semilogy____env1-1_1790697744797155451.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_semilogy____env1-1_1790697744797155451.png' created of size '205776b', at 'Wed Oct 10 10:58:23 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_semilogy____env1-1_1790697744797155451.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_semilogy____env1-1_1790697744797155451.pdf' created of size '53935b', at 'Wed Oct 10 10:58:24 2018' ...


- Plotting the centralized regret for all 'players' values, in loglog scale
  and saving the plot to plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_loglog____env1-1_1790697744797155451 ...
 -  For 6 players, Anandtharam et al. centralized lower-bound gave = 4.74 ...
 -  For 6 players, our lower bound gave = 28.4 ...
 -  For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 16.4 ...
 - [Anandtharam et al] centralized lower-bound = 4.74,
 - [Anandkumar et al] decentralized lower-bound = 16.4
 - Our better (larger) decentralized lower-bound = 28.4,
Warning: forcing to use putatright = False because there is 6 items in the legend.
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_loglog____env1-1_1790697744797155451.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_loglog____env1-1_1790697744797155451.pickle' created of size '3897973b', at 'Wed Oct 10 10:58:24 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_loglog____env1-1_1790697744797155451.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_loglog____env1-1_1790697744797155451.png' created of size '232069b', at 'Wed Oct 10 10:58:24 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_loglog____env1-1_1790697744797155451.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_RegretCentralized_loglog____env1-1_1790697744797155451.pdf' created of size '66172b', at 'Wed Oct 10 10:58:25 2018' ...


- Plotting the centralized fairness (STD)
  and saving the plot to plots/MP__K9_M6_T10000_N4__6_algos/all_FairnessSTD____env1-1_1790697744797155451 ...
Warning: forcing to use putatright = False because there is 6 items in the legend.
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_FairnessSTD____env1-1_1790697744797155451.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_FairnessSTD____env1-1_1790697744797155451.pickle' created of size '3936191b', at 'Wed Oct 10 10:58:25 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_FairnessSTD____env1-1_1790697744797155451.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_FairnessSTD____env1-1_1790697744797155451.png' created of size '256899b', at 'Wed Oct 10 10:58:25 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_FairnessSTD____env1-1_1790697744797155451.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_FairnessSTD____env1-1_1790697744797155451.pdf' created of size '295523b', at 'Wed Oct 10 10:58:26 2018' ...

- Plotting the total nb of collision as a function of time for all 'players' values
  and saving the plot to plots/MP__K9_M6_T10000_N4__6_algos/all_NbCollisions____env1-1_1790697744797155451 ...
No upper bound for the non-cumulated number of collisions...
Warning: forcing to use putatright = False because there is 6 items in the legend.
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_NbCollisions____env1-1_1790697744797155451.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_NbCollisions____env1-1_1790697744797155451.pickle' created of size '3936135b', at 'Wed Oct 10 10:58:26 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_NbCollisions____env1-1_1790697744797155451.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_NbCollisions____env1-1_1790697744797155451.png' created of size '544309b', at 'Wed Oct 10 10:58:27 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_NbCollisions____env1-1_1790697744797155451.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_NbCollisions____env1-1_1790697744797155451.pdf' created of size '86019b', at 'Wed Oct 10 10:58:27 2018' ...

- Plotting the cumulated total nb of collision as a function of time for all 'players' values
  and saving the plot to plots/MP__K9_M6_T10000_N4__6_algos/all_CumNbCollisions____env1-1_1790697744797155451 ...
No upper bound for the non-cumulated number of collisions...
Warning: forcing to use putatright = False because there is 6 items in the legend.
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_CumNbCollisions____env1-1_1790697744797155451.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_CumNbCollisions____env1-1_1790697744797155451.pickle' created of size '3937022b', at 'Wed Oct 10 10:58:29 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_CumNbCollisions____env1-1_1790697744797155451.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_CumNbCollisions____env1-1_1790697744797155451.png' created of size '228037b', at 'Wed Oct 10 10:58:30 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_CumNbCollisions____env1-1_1790697744797155451.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_CumNbCollisions____env1-1_1790697744797155451.pdf' created of size '59120b', at 'Wed Oct 10 10:58:30 2018' ...


- Plotting the number of switches as a function of time for all 'players' values
  and saving the plot to plots/MP__K9_M6_T10000_N4__6_algos/all_CumNbSwitchs____env1-1_1790697744797155451 ...
Warning: forcing to use putatright = False because there is 6 items in the legend.
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_CumNbSwitchs____env1-1_1790697744797155451.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_CumNbSwitchs____env1-1_1790697744797155451.pickle' created of size '3936890b', at 'Wed Oct 10 10:58:30 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_CumNbSwitchs____env1-1_1790697744797155451.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_CumNbSwitchs____env1-1_1790697744797155451.png' created of size '248027b', at 'Wed Oct 10 10:58:30 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_CumNbSwitchs____env1-1_1790697744797155451.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_CumNbSwitchs____env1-1_1790697744797155451.pdf' created of size '53473b', at 'Wed Oct 10 10:58:31 2018' ...

- Plotting the histograms of regrets
Warning: forcing to use putatright = False because there is 6 items in the legend.
  and saving the plot to plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareX_shareY____env1-1_1790697744797155451 ...
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareX_shareY____env1-1_1790697744797155451.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareX_shareY____env1-1_1790697744797155451.pickle' created of size '746036b', at 'Wed Oct 10 10:58:31 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareX_shareY____env1-1_1790697744797155451.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareX_shareY____env1-1_1790697744797155451.png' created of size '129009b', at 'Wed Oct 10 10:58:32 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareX_shareY____env1-1_1790697744797155451.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareX_shareY____env1-1_1790697744797155451.pdf' created of size '34410b', at 'Wed Oct 10 10:58:32 2018' ...
  and saving the plot to plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareX____env1-1_1790697744797155451 ...
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareX____env1-1_1790697744797155451.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareX____env1-1_1790697744797155451.pickle' created of size '748691b', at 'Wed Oct 10 10:58:33 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareX____env1-1_1790697744797155451.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareX____env1-1_1790697744797155451.png' created of size '135923b', at 'Wed Oct 10 10:58:33 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareX____env1-1_1790697744797155451.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareX____env1-1_1790697744797155451.pdf' created of size '34626b', at 'Wed Oct 10 10:58:33 2018' ...
  and saving the plot to plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareY____env1-1_1790697744797155451 ...
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareY____env1-1_1790697744797155451.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareY____env1-1_1790697744797155451.pickle' created of size '748691b', at 'Wed Oct 10 10:58:34 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareY____env1-1_1790697744797155451.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareY____env1-1_1790697744797155451.png' created of size '143604b', at 'Wed Oct 10 10:58:34 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareY____env1-1_1790697744797155451.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret_shareY____env1-1_1790697744797155451.pdf' created of size '35374b', at 'Wed Oct 10 10:58:35 2018' ...
  and saving the plot to plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451 ...
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451.pickle' created of size '751355b', at 'Wed Oct 10 10:58:35 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451.png' created of size '152650b', at 'Wed Oct 10 10:58:36 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451.pdf' created of size '35604b', at 'Wed Oct 10 10:58:36 2018' ...

 - Plotting the histograms of regrets for each algorithm separately, and saving the plots...
  and saving the plot to plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451 ...
Warning: forcing to use putatright = False because there is 0 items in the legend.
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_1_7.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_1_7.pickle' created of size '117316b', at 'Wed Oct 10 10:58:36 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_1_7.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_1_7.png' created of size '85221b', at 'Wed Oct 10 10:58:37 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_1_7.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_1_7.pdf' created of size '31620b', at 'Wed Oct 10 10:58:37 2018' ...
Warning: forcing to use putatright = False because there is 0 items in the legend.
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_2_7.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_2_7.pickle' created of size '117283b', at 'Wed Oct 10 10:58:37 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_2_7.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_2_7.png' created of size '83009b', at 'Wed Oct 10 10:58:37 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_2_7.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_2_7.pdf' created of size '30456b', at 'Wed Oct 10 10:58:38 2018' ...
Warning: forcing to use putatright = False because there is 0 items in the legend.
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_3_7.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_3_7.pickle' created of size '115862b', at 'Wed Oct 10 10:58:38 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_3_7.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_3_7.png' created of size '91888b', at 'Wed Oct 10 10:58:38 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_3_7.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_3_7.pdf' created of size '29540b', at 'Wed Oct 10 10:58:38 2018' ...
Warning: forcing to use putatright = False because there is 0 items in the legend.
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_4_7.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_4_7.pickle' created of size '117282b', at 'Wed Oct 10 10:58:39 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_4_7.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_4_7.png' created of size '82356b', at 'Wed Oct 10 10:58:39 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_4_7.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_4_7.pdf' created of size '30030b', at 'Wed Oct 10 10:58:39 2018' ...
Warning: forcing to use putatright = False because there is 0 items in the legend.
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_5_7.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_5_7.pickle' created of size '117283b', at 'Wed Oct 10 10:58:39 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_5_7.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_5_7.png' created of size '83272b', at 'Wed Oct 10 10:58:39 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_5_7.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_5_7.pdf' created of size '30461b', at 'Wed Oct 10 10:58:40 2018' ...
Warning: forcing to use putatright = False because there is 0 items in the legend.
Saving raw figure with format pickle, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_6_7.pickle'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_6_7.pickle' created of size '117287b', at 'Wed Oct 10 10:58:40 2018' ...
Saving figure with format png, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_6_7.png'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_6_7.png' created of size '82504b', at 'Wed Oct 10 10:58:40 2018' ...
Saving figure with format pdf, to file 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_6_7.pdf'...
       Saved! 'plots/MP__K9_M6_T10000_N4__6_algos/all_HistogramsRegret____env1-1_1790697744797155451__Algo_6_7.pdf' created of size '30493b', at 'Wed Oct 10 10:58:41 2018' ...


==> To see the figures, do :
eog plots/MP__K9_M6_T10000_N4__6_algos/all*1790697744797155451.png
Done for simulations main_multiplayers.py ...
