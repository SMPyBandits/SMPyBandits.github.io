 - Setting dpi of all figures to 110 ...
 - Setting 'figsize' of all figures to (19.8, 10.8) ...
Info: Using the regular tqdm() decorator ...
Info: numba.jit seems to be available.
Info: numba.jit seems to be available.
Loaded experiments configuration from 'configuration.py' :
configuration = {'horizon': 1000, 'repetitions': 4, 'activations': (0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05), 'n_jobs': 4, 'verbosity': 6, 'finalRanksOnAverage': True, 'averageOn': 0.001, 'environment': [{'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': (0.9, 0.8, 0.7, 0.6, 0.5)}], 'successive_players': [[Selfish(U(1..5)), Selfish(U(1..5)), Selfish(U(1..5)), Selfish(U(1..5)), Selfish(U(1..5)), Selfish(U(1..5)), Selfish(U(1..5)), Selfish(U(1..5)), Selfish(U(1..5)), Selfish(U(1..5))], [Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB)], [Selfish(Thompson), Selfish(Thompson), Selfish(Thompson), Selfish(Thompson), Selfish(Thompson), Selfish(Thompson), Selfish(Thompson), Selfish(Thompson), Selfish(Thompson), Selfish(Thompson)], [Selfish(KLUCB), Selfish(KLUCB), Selfish(KLUCB), Selfish(KLUCB), Selfish(KLUCB), Selfish(KLUCB), Selfish(KLUCB), Selfish(KLUCB), Selfish(KLUCB), Selfish(KLUCB)], [Selfish(Exp3++), Selfish(Exp3++), Selfish(Exp3++), Selfish(Exp3++), Selfish(Exp3++), Selfish(Exp3++), Selfish(Exp3++), Selfish(Exp3++), Selfish(Exp3++), Selfish(Exp3++)]], 'players': [Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB), Selfish(UCB)]}
====> TURNING DEBUG MODE ON <=====
plots/ is already a directory here...



Considering the list of players :
 [Selfish(U(1..5)), Selfish(U(1..5)), Selfish(U(1..5)), Selfish(U(1..5)), Selfish(U(1..5)), Selfish(U(1..5)), Selfish(U(1..5)), Selfish(U(1..5)), Selfish(U(1..5)), Selfish(U(1..5))]
Number of players in the multi-players game: 10
Time horizon: 1000
Number of repetitions: 4
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: 4
Using collision model onlyUniqUserGetsReward (function <function onlyUniqUserGetsReward at 0x7f6fb4955400>).
More details:
 Simple collision model where only the players alone on one arm samples it and receives the reward.

    - This is the default collision model, cf. [[Multi-Player Bandits Models Revisited, Lilian Besson and Emilie Kaufmann, 2017]](https://hal.inria.fr/hal-01629733).
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Using accurate regrets and last regrets ? True


Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': (0.9, 0.8, 0.7, 0.6, 0.5)} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = (0.9, 0.8, 0.7, 0.6, 0.5)
 - with 'arms' = [B(0.9), B(0.8), B(0.7), B(0.6), B(0.5)]
 - with 'means' = [ 0.9  0.8  0.7  0.6  0.5]
 - with 'nbArms' = 5
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.5

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 5.3 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...
 - with 'arms' represented as: $[B(0.9)^*, B(0.8), B(0.7), B(0.6), B(0.5)]$
Number of environments to try: 1
Using collision model onlyUniqUserGetsRewardSparse (function <function onlyUniqUserGetsRewardSparse at 0x7f6fb4955488>).
More details:
 Simple collision model where only the players alone on one arm samples it and receives the reward.

    - This is the default collision model, cf. [[Multi-Player Bandits Models Revisited, Lilian Besson and Emilie Kaufmann, 2017]](https://hal.inria.fr/hal-01629733).
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    - Support for player non activated, by choosing a negative index.
    


Evaluating environment: MAB(nbArms: 5, arms: [B(0.9), B(0.8), B(0.7), B(0.6), B(0.5)], minArm: 0.5, maxArm: 0.9)
- Adding player # 1 = #1<Selfish-U(1..5)> ...
  Using this already created player 'player' = #1<Selfish-U(1..5)> ...
- Adding player # 2 = #2<Selfish-U(1..5)> ...
  Using this already created player 'player' = #2<Selfish-U(1..5)> ...
- Adding player # 3 = #3<Selfish-U(1..5)> ...
  Using this already created player 'player' = #3<Selfish-U(1..5)> ...
- Adding player # 4 = #4<Selfish-U(1..5)> ...
  Using this already created player 'player' = #4<Selfish-U(1..5)> ...
- Adding player # 5 = #5<Selfish-U(1..5)> ...
  Using this already created player 'player' = #5<Selfish-U(1..5)> ...
- Adding player # 6 = #6<Selfish-U(1..5)> ...
  Using this already created player 'player' = #6<Selfish-U(1..5)> ...
- Adding player # 7 = #7<Selfish-U(1..5)> ...
  Using this already created player 'player' = #7<Selfish-U(1..5)> ...
- Adding player # 8 = #8<Selfish-U(1..5)> ...
  Using this already created player 'player' = #8<Selfish-U(1..5)> ...
- Adding player # 9 = #9<Selfish-U(1..5)> ...
  Using this already created player 'player' = #9<Selfish-U(1..5)> ...
- Adding player #10 = #10<Selfish-U(1..5)> ...
  Using this already created player 'player' = #10<Selfish-U(1..5)> ...

Number of activations by players:

The policy #1<Selfish-U(1..5)> was activated 35 times after 1000 steps...
Estimated order by the policy #1<Selfish-U(1..5)> after 1000 steps: [2 0 1 3 4] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 78.76% (relative success)...

The policy #2<Selfish-U(1..5)> was activated 42 times after 1000 steps...
Estimated order by the policy #2<Selfish-U(1..5)> after 1000 steps: [2 4 0 1 3] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 23.18% (relative success)...

The policy #3<Selfish-U(1..5)> was activated 39 times after 1000 steps...
Estimated order by the policy #3<Selfish-U(1..5)> after 1000 steps: [4 2 3 1 0] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 58.81% (relative success)...

The policy #4<Selfish-U(1..5)> was activated 54 times after 1000 steps...
Estimated order by the policy #4<Selfish-U(1..5)> after 1000 steps: [4 0 1 3 2] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 27.18% (relative success)...

The policy #5<Selfish-U(1..5)> was activated 51 times after 1000 steps...
Estimated order by the policy #5<Selfish-U(1..5)> after 1000 steps: [1 4 0 2 3] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 39.72% (relative success)...

The policy #6<Selfish-U(1..5)> was activated 48 times after 1000 steps...
Estimated order by the policy #6<Selfish-U(1..5)> after 1000 steps: [3 1 4 0 2] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 33.80% (relative success)...

The policy #7<Selfish-U(1..5)> was activated 54 times after 1000 steps...
Estimated order by the policy #7<Selfish-U(1..5)> after 1000 steps: [0 4 1 3 2] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 41.80% (relative success)...

The policy #8<Selfish-U(1..5)> was activated 56 times after 1000 steps...
Estimated order by the policy #8<Selfish-U(1..5)> after 1000 steps: [2 3 4 0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 40.62% (relative success)...

The policy #9<Selfish-U(1..5)> was activated 55 times after 1000 steps...
Estimated order by the policy #9<Selfish-U(1..5)> after 1000 steps: [0 4 1 2 3] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 62.20% (relative success)...

The policy #10<Selfish-U(1..5)> was activated 51 times after 1000 steps...
Estimated order by the policy #10<Selfish-U(1..5)> after 1000 steps: [0 1 4 2 3] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 78.76% (relative success)...


Giving the final ranks ...

Final ranking for this environment # 0 : 10 x Selfish-U(1..5), p=0.05 ...
- Player # 8 / 10, Selfish-U(1..5)	was ranked	1 / 10 for this simulation (last rewards = 34.75).
- Player # 6 / 10, Selfish-U(1..5)	was ranked	2 / 10 for this simulation (last rewards = 33.5).
- Player #10 / 10, Selfish-U(1..5)	was ranked	3 / 10 for this simulation (last rewards = 32.5).
- Player # 7 / 10, Selfish-U(1..5)	was ranked	4 / 10 for this simulation (last rewards = 32.25).
- Player # 3 / 10, Selfish-U(1..5)	was ranked	5 / 10 for this simulation (last rewards = 32).
- Player # 5 / 10, Selfish-U(1..5)	was ranked	6 / 10 for this simulation (last rewards = 31.75).
- Player # 2 / 10, Selfish-U(1..5)	was ranked	7 / 10 for this simulation (last rewards = 31).
- Player # 4 / 10, Selfish-U(1..5)	was ranked	8 / 10 for this simulation (last rewards = 30.5).
- Player # 9 / 10, Selfish-U(1..5)	was ranked	9 / 10 for this simulation (last rewards = 29).
- Player # 1 / 10, Selfish-U(1..5)	was ranked	10 / 10 for this simulation (last rewards = 27).


Giving the vector of final regrets ...

For evaluator # 1/1 : <Environment.EvaluatorSparseMultiPlayers.EvaluatorSparseMultiPlayers object at 0x7f6fda531a58> (players 10 x Selfish-U(1..5), p=0.05) ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (4,)
Min of    last regrets R_T = 2672.0
Mean of   last regrets R_T = 2686.025
Median of last regrets R_T = 2687.5
Max of    last regrets R_T = 2697.1
STD of    last regrets R_T = 10.4427905753


- Plotting the decentralized rewards


- Plotting the centralized fairness (STD)


- Plotting the centralized regret
Error: Unable to compute and display the lower-bound...

- Plotting the cumulated total nb of collision as a function of time
No upper bound for the non-cumulated number of collisions...
