Reading argument from command line, importing the configuration from arg = configuration_multiplayers_nonstationary (module = configuration_multiplayers_nonstationary)...

Using Upsilon_T = 3 break-points (time when at least one arm changes), and C_T = 9 change-points (number of changes of all arms).
For this problem, we compute the Delta^change and Delta^opt...
min_change_on_mean = 0.09999999999999998
min_optimality_gap = 0.09999999999999998
DELTA_for_MUCB = 0.1
EPSILON_for_CUSUM = 0.1
Warning: using the default value for the GAP (Bayesian environment maybe?)
 - One new child, of index 0, and class #1<CentralizedMultiplePlay(UCB)> ...
 - One new child, of index 0, and class #1<CentralizedMultiplePlay(kl-UCB)> ...
 - One new child, of index 0, and class #1<CentralizedMultiplePlay(GLR-UCB(Local))> ...
Loaded experiments configuration from 'configuration_multiplayers_nonstationary.py' :
configuration = {'horizon': 2000, 'repetitions': 100, 'n_jobs': -1, 'verbosity': 6, 'collisionModel': <function onlyUniqUserGetsReward at 0x7f0d15f609d8>, 'finalRanksOnAverage': True, 'averageOn': 0.001, 'nb_break_points': 3, 'plot_lowerbounds': True, 'environment': [{'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.1, 0.2, 0.9], [0.4, 0.9, 0.1], [0.5, 0.1, 0.2], [0.2, 0.2, 0.3]], 'changePoints': [0, 500, 1000, 1500]}}], 'successive_players': [[rhoRand(UCB)], [rhoRand(kl-UCB)], [RandTopM(UCB)], [RandTopM(kl-UCB)], [MCTopM(UCB)], [MCTopM(kl-UCB)], [Selfish(UCB)], [Selfish(kl-UCB)], [CentralizedMultiplePlay(UCB)], [CentralizedMultiplePlay(kl-UCB)], [CentralizedMultiplePlay(GLR-UCB(Local))], [<Policies.MusicalChair.MusicalChair object at 0x7f0d0d77e8d0>], [<Policies.MusicalChair.MusicalChair object at 0x7f0d0d77e9b0>], [<Policies.MusicalChair.MusicalChair object at 0x7f0d0d77ea58>]], 'players': [Selfish(DiscountedThompson($\gamma=0.99$))]}
====> TURNING DEBUG MODE ON <=====
plots/ is already a directory here...
Number of players in the multi-players game: 1
Time horizon: 2000
Number of repetitions: 100
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: -1
Using collision model onlyUniqUserGetsReward (function <function onlyUniqUserGetsReward at 0x7f0d15f609d8>).
More details:
 Simple collision model where only the players alone on one arm samples it and receives the reward.

    - This is the default collision model, cf. [[Multi-Player Bandits Revisited, Lilian Besson and Emilie Kaufmann, 2017]](https://hal.inria.fr/hal-01629733).
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Using accurate regrets and last regrets ? True
Using this dictionary to create a new environment:
 {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.1, 0.2, 0.9], [0.4, 0.9, 0.1], [0.5, 0.1, 0.2], [0.2, 0.2, 0.3]], 'changePoints': [0, 500, 1000, 1500]}}
  Special MAB problem, with arm (possibly) changing at every time step, read from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.1, 0.2, 0.9], [0.4, 0.9, 0.1], [0.5, 0.1, 0.2], [0.2, 0.2, 0.3]], 'changePoints': [0, 500, 1000, 1500]}} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = {'listOfMeans': [[0.1, 0.2, 0.9], [0.4, 0.9, 0.1], [0.5, 0.1, 0.2], [0.2, 0.2, 0.3]], 'changePoints': [0, 500, 1000, 1500]}
 - with 'listOfMeans' = [[0.1 0.2 0.9]
 [0.4 0.9 0.1]
 [0.5 0.1 0.2]
 [0.2 0.2 0.3]]
 - with 'changePoints' = [0, 500, 1000, 1500]


 ==> Creating the dynamic arms ...
   - with 'nbArms' = 3
   - with 'arms' = [B(0.1), B(0.2), B(0.9)]
 - Initial draw of 'means' = [0.1 0.2 0.9]
Number of environments to try: 1


Evaluating environment: PieceWiseStationaryMAB(nbArms: 3, arms: [B(0.1), B(0.2), B(0.9)])
- Adding player # 1 = #1<Selfish-DiscountedThompson($\gamma=0.99$)> ...
  Using this already created player 'player' = #1<Selfish-DiscountedThompson($\gamma=0.99$)> ...

New means vector = [0.1 0.2 0.9], at time t = 0 ...

New means vector = [0.4 0.9 0.1], at time t = 500 ...

New means vector = [0.5 0.1 0.2], at time t = 1000 ...

New means vector = [0.2 0.2 0.3], at time t = 1500 ...

Estimated order by the policy #1<Selfish-DiscountedThompson($\gamma=0.99$)> after 2000 steps: [0 1 2] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...

Giving the final ranks ...

Final ranking for this environment # 0 : Selfish-DiscountedThompson($\gamma=0.99$) ...
- Player # 1 / 1, Selfish-DiscountedThompson($\gamma=0.99$)	was ranked	1 / 1 for this simulation (last rewards = 1205.4).

Giving the vector of final regrets ...

For evaluator # 1/1 : <Environment.EvaluatorMultiPlayers.EvaluatorMultiPlayers object at 0x7f0d3aa16080> (players Selfish-DiscountedThompson($\gamma=0.99$)) ...
  Last regrets vector (for all repetitions) is:
Min of    last regrets R_T = 784.4
Mean of   last regrets R_T = 870.3539999999998
Median of last regrets R_T = 866.3499999999999
Max of    last regrets R_T = 1017.3
Variance  last regrets R_T = 2049.957684

Giving the mean and var running times ...

For players called 'Selfish-DiscountedThompson($\gamma=0.99$)' ...
    318 ms ± 5.58 ms per loop (mean ± var. dev. of 100 run)

Giving the mean and var memory consumption ...

For players called 'Selfish-DiscountedThompson($\gamma=0.99$)' ...
    1.1 KiB ± 32.2 KiB (mean ± var. dev. of 100 runs)


- Plotting the decentralized rewards


- Plotting the centralized fairness (STD)


- Plotting the centralized regret
For the empirical centralized regret, first term has shape (2000,)
For the empirical centralized regret, second term has shape (2000,)
For the empirical centralized regret, third term has shape (2000,)
 -  For 1 players, Anandtharam et al. centralized lower-bound gave = 0.969 ...
 -  For 1 players, our lower bound gave = 0.969 ...
 -  For 1 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 0.969 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 0.969 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 16.67% ...
 - [Anandtharam et al] centralized lower-bound = 0.969,
 - [Anandkumar et al] decentralized lower-bound = 0.969
 - Our better (larger) decentralized lower-bound = 0.969,


- Plotting the centralized regret
For the empirical centralized regret, first term has shape (2000,)
For the empirical centralized regret, second term has shape (2000,)
For the empirical centralized regret, third term has shape (2000,)
 -  For 1 players, Anandtharam et al. centralized lower-bound gave = 0.969 ...
 -  For 1 players, our lower bound gave = 0.969 ...
 -  For 1 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 0.969 ...
 - [Anandtharam et al] centralized lower-bound = 0.969,
 - [Anandkumar et al] decentralized lower-bound = 0.969
 - Our better (larger) decentralized lower-bound = 0.969,
