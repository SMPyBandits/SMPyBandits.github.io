Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'archtype': <class 'Policies.UCB.UCB'>, 'params': {}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}}, {'archtype': <class 'Policies.MOSSH.MOSSH'>, 'params': {'horizon': 1000}}, {'archtype': <class 'Policies.MOSSAnytime.MOSSAnytime'>, 'params': {'alpha': 1.35}}, {'archtype': <class 'Policies.OCUCB.OCUCB'>, 'params': {'eta': 1.1, 'rho': 1}}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}}, {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 1000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}}, {'archtype': <class 'Policies.AdBandits.AdBandits'>, 'params': {'alpha': 0.5, 'horizon': 1000}}, {'archtype': <class 'Policies.ApproximatedFHGittins.ApproximatedFHGittins'>, 'params': {'alpha': 0.5, 'horizon': 1100}}, {'archtype': <class 'Policies.UCBoost.UCB_bq'>, 'params': {}}, {'archtype': <class 'Policies.UCBoost.UCB_h'>, 'params': {}}, {'archtype': <class 'Policies.UCBoost.UCB_lb'>, 'params': {}}, {'archtype': <class 'Policies.UCBoost.UCBoost_bq_h_lb'>, 'params': {}}, {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.1}}, {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.05}}, {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.01}}, {'archtype': <class 'Policies.UCBoost_faster.UCB_bq'>, 'params': {}}, {'archtype': <class 'Policies.UCBoost_faster.UCB_h'>, 'params': {}}, {'archtype': <class 'Policies.UCBoost_faster.UCB_lb'>, 'params': {}}, {'archtype': <class 'Policies.UCBoost_faster.UCBoost_bq_h_lb'>, 'params': {}}, {'archtype': <class 'Policies.UCBoost_faster.UCBoostEpsilon'>, 'params': {'epsilon': 0.1}}, {'archtype': <class 'Policies.UCBoost_faster.UCBoostEpsilon'>, 'params': {'epsilon': 0.05}}, {'archtype': <class 'Policies.UCBoost_faster.UCBoostEpsilon'>, 'params': {'epsilon': 0.01}}, {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCB_bq'>, 'params': {}}, {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCB_h'>, 'params': {}}, {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCB_lb'>, 'params': {}}, {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCBoost_bq_h_lb'>, 'params': {}}, {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.1}}, {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.05}}, {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.01}}]
====> TURNING DEBUG MODE ON <=====
plots/ is already a directory here...
Number of policies in this comparison: 33
Time horizon: 1000
Number of repetitions: 500
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: -1
  Special MAB problem, changing at every repetitions, read from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'function': <function randomMeans at 0x7f75a114b510>, 'args': {'nbArms': 9, 'mingap': None, 'lower': 0.0, 'amplitude': 1.0, 'isSorted': True}}} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = {'function': <function randomMeans at 0x7f75a114b510>, 'args': {'nbArms': 9, 'mingap': None, 'lower': 0.0, 'amplitude': 1.0, 'isSorted': True}}
 - with 'function' = <function randomMeans at 0x7f75a114b510>
 - with 'args' = {'nbArms': 9, 'mingap': None, 'lower': 0.0, 'amplitude': 1.0, 'isSorted': True}


 ==> Creating the dynamic arms ...
   - drawing a random set of arms
   - with 'nbArms' = 9
   - with 'arms' = [B(0.295), B(0.454), B(0.551), B(0.625), B(0.745), B(0.876), B(0.91), B(0.912), B(0.917)]
 - Example of initial draw of 'means' = [0.29538729 0.45424432 0.55100109 0.62505502 0.74450988 0.87648497
 0.90967041 0.91168104 0.9172123 ]
   - with 'maxArm' = 0.9172122970702685
   - with 'minArm' = 0.2953872882323476
Number of environments to try: 1


Evaluating environment: DynamicMAB(nbArms: 9, arms: [B(0.295), B(0.454), B(0.551), B(0.625), B(0.745), B(0.876), B(0.91), B(0.912), B(0.917)], minArm: 0.295, maxArm: 0.917)
- Adding policy #1 = {'archtype': <class 'Policies.UCB.UCB'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'archtype': <class 'Policies.UCB.UCB'>, 'params': {}} ...
- Adding policy #2 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
- Adding policy #3 = {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}} ...
- Adding policy #4 = {'archtype': <class 'Policies.MOSSH.MOSSH'>, 'params': {'horizon': 1000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'archtype': <class 'Policies.MOSSH.MOSSH'>, 'params': {'horizon': 1000}} ...
- Adding policy #5 = {'archtype': <class 'Policies.MOSSAnytime.MOSSAnytime'>, 'params': {'alpha': 1.35}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][4]' = {'archtype': <class 'Policies.MOSSAnytime.MOSSAnytime'>, 'params': {'alpha': 1.35}} ...
- Adding policy #6 = {'archtype': <class 'Policies.OCUCB.OCUCB'>, 'params': {'eta': 1.1, 'rho': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][5]' = {'archtype': <class 'Policies.OCUCB.OCUCB'>, 'params': {'eta': 1.1, 'rho': 1}} ...
- Adding policy #7 = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][6]' = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}} ...
- Adding policy #8 = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][7]' = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
- Adding policy #9 = {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 1000, 'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][8]' = {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 1000, 'klucb': <built-in function klucbBern>}} ...
- Adding policy #10 = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][9]' = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}} ...
- Adding policy #11 = {'archtype': <class 'Policies.AdBandits.AdBandits'>, 'params': {'alpha': 0.5, 'horizon': 1000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][10]' = {'archtype': <class 'Policies.AdBandits.AdBandits'>, 'params': {'alpha': 0.5, 'horizon': 1000}} ...
- Adding policy #12 = {'archtype': <class 'Policies.ApproximatedFHGittins.ApproximatedFHGittins'>, 'params': {'alpha': 0.5, 'horizon': 1100}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][11]' = {'archtype': <class 'Policies.ApproximatedFHGittins.ApproximatedFHGittins'>, 'params': {'alpha': 0.5, 'horizon': 1100}} ...
- Adding policy #13 = {'archtype': <class 'Policies.UCBoost.UCB_bq'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][12]' = {'archtype': <class 'Policies.UCBoost.UCB_bq'>, 'params': {}} ...
- Adding policy #14 = {'archtype': <class 'Policies.UCBoost.UCB_h'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][13]' = {'archtype': <class 'Policies.UCBoost.UCB_h'>, 'params': {}} ...
- Adding policy #15 = {'archtype': <class 'Policies.UCBoost.UCB_lb'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][14]' = {'archtype': <class 'Policies.UCBoost.UCB_lb'>, 'params': {}} ...
- Adding policy #16 = {'archtype': <class 'Policies.UCBoost.UCBoost_bq_h_lb'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][15]' = {'archtype': <class 'Policies.UCBoost.UCBoost_bq_h_lb'>, 'params': {}} ...
- Adding policy #17 = {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][16]' = {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.1}} ...
- Adding policy #18 = {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.05}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][17]' = {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.05}} ...
- Adding policy #19 = {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.01}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][18]' = {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.01}} ...
- Adding policy #20 = {'archtype': <class 'Policies.UCBoost_faster.UCB_bq'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][19]' = {'archtype': <class 'Policies.UCBoost_faster.UCB_bq'>, 'params': {}} ...
- Adding policy #21 = {'archtype': <class 'Policies.UCBoost_faster.UCB_h'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][20]' = {'archtype': <class 'Policies.UCBoost_faster.UCB_h'>, 'params': {}} ...
- Adding policy #22 = {'archtype': <class 'Policies.UCBoost_faster.UCB_lb'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][21]' = {'archtype': <class 'Policies.UCBoost_faster.UCB_lb'>, 'params': {}} ...
- Adding policy #23 = {'archtype': <class 'Policies.UCBoost_faster.UCBoost_bq_h_lb'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][22]' = {'archtype': <class 'Policies.UCBoost_faster.UCBoost_bq_h_lb'>, 'params': {}} ...
- Adding policy #24 = {'archtype': <class 'Policies.UCBoost_faster.UCBoostEpsilon'>, 'params': {'epsilon': 0.1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][23]' = {'archtype': <class 'Policies.UCBoost_faster.UCBoostEpsilon'>, 'params': {'epsilon': 0.1}} ...
- Adding policy #25 = {'archtype': <class 'Policies.UCBoost_faster.UCBoostEpsilon'>, 'params': {'epsilon': 0.05}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][24]' = {'archtype': <class 'Policies.UCBoost_faster.UCBoostEpsilon'>, 'params': {'epsilon': 0.05}} ...
- Adding policy #26 = {'archtype': <class 'Policies.UCBoost_faster.UCBoostEpsilon'>, 'params': {'epsilon': 0.01}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][25]' = {'archtype': <class 'Policies.UCBoost_faster.UCBoostEpsilon'>, 'params': {'epsilon': 0.01}} ...
- Adding policy #27 = {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCB_bq'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][26]' = {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCB_bq'>, 'params': {}} ...
- Adding policy #28 = {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCB_h'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][27]' = {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCB_h'>, 'params': {}} ...
- Adding policy #29 = {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCB_lb'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][28]' = {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCB_lb'>, 'params': {}} ...
- Adding policy #30 = {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCBoost_bq_h_lb'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][29]' = {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCBoost_bq_h_lb'>, 'params': {}} ...
- Adding policy #31 = {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][30]' = {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.1}} ...
- Adding policy #32 = {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.05}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][31]' = {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.05}} ...
- Adding policy #33 = {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.01}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][32]' = {'archtype': <class 'SMPyBandits.Policies.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.01}} ...



- Evaluating policy #1/33: UCB ...

Estimated order by the policy UCB after 1000 steps: [1 3 2 0 5 4 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 65.43% (relative success)...



- Evaluating policy #2/33: UCB($\alpha=1$) ...

Estimated order by the policy UCB($\alpha=1$) after 1000 steps: [0 1 2 4 3 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 95.06% (relative success)...
  ==> Gestalt     distance from optimal ordering: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 91.98% (relative success)...



- Evaluating policy #3/33: MOSS ...

Estimated order by the policy MOSS after 1000 steps: [2 5 1 4 0 3 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 65.43% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 60.49% (relative success)...



- Evaluating policy #4/33: MOSS-H($T=1000$) ...

Estimated order by the policy MOSS-H($T=1000$) after 1000 steps: [3 2 4 1 0 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 70.37% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 68.52% (relative success)...



- Evaluating policy #5/33: MOSS-Anytime($\alpha=1.35$) ...

Estimated order by the policy MOSS-Anytime($\alpha=1.35$) after 1000 steps: [2 0 3 1 4 5 6 8 7] ...
  ==> Optimal arm identification: 91.82% (relative success)...
  ==> Manhattan   distance from optimal ordering: 80.25% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 73.46% (relative success)...



- Evaluating policy #6/33: OC-UCB($\eta=1.1$, $\rho=1$) ...

Estimated order by the policy OC-UCB($\eta=1.1$, $\rho=1$) after 1000 steps: [0 1 2 5 4 3 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 75.93% (relative success)...



- Evaluating policy #7/33: Thompson ...

Estimated order by the policy Thompson after 1000 steps: [0 1 2 3 6 5 4 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 90.12% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 83.95% (relative success)...



- Evaluating policy #8/33: kl-UCB ...

Estimated order by the policy kl-UCB after 1000 steps: [0 4 1 2 3 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Gestalt     distance from optimal ordering: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 87.04% (relative success)...



- Evaluating policy #9/33: kl-UCB$^{++}$($T=1000$) ...

Estimated order by the policy kl-UCB$^{++}$($T=1000$) after 1000 steps: [0 1 3 4 2 5 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 81.48% (relative success)...



- Evaluating policy #10/33: BayesUCB ...

Estimated order by the policy BayesUCB after 1000 steps: [6 0 1 2 3 8 4 5 7] ...
  ==> Optimal arm identification: 99.53% (relative success)...
  ==> Manhattan   distance from optimal ordering: 55.56% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 66.67% (relative success)...



- Evaluating policy #11/33: AdBandits($T=1000$, $\alpha=0.5$) ...

Estimated order by the policy AdBandits($T=1000$, $\alpha=0.5$) after 1000 steps: [0 3 6 7 5 4 2 1 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 45.68% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 50.62% (relative success)...



- Evaluating policy #12/33: ApprFHG($T=1100$) ...

Estimated order by the policy ApprFHG($T=1100$) after 1000 steps: [0 1 2 3 6 5 4 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 90.12% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 83.95% (relative success)...



- Evaluating policy #13/33: $\mathrm{UCB}_{d=d_{bq}}$($c=0$) ...

Estimated order by the policy $\mathrm{UCB}_{d=d_{bq}}$($c=0$) after 1000 steps: [2 0 1 5 3 4 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 70.99% (relative success)...



- Evaluating policy #14/33: $\mathrm{UCB}_{d=d_h}$($c=0$) ...

Estimated order by the policy $\mathrm{UCB}_{d=d_h}$($c=0$) after 1000 steps: [0 1 2 3 5 4 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 95.06% (relative success)...
  ==> Gestalt     distance from optimal ordering: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 91.98% (relative success)...



- Evaluating policy #15/33: $\mathrm{UCB}_{d=d_{lb}}$($c=0$) ...

Estimated order by the policy $\mathrm{UCB}_{d=d_{lb}}$($c=0$) after 1000 steps: [0 1 4 2 5 7 6 3 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 70.99% (relative success)...



- Evaluating policy #16/33: UCBoost($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$) ...

Estimated order by the policy UCBoost($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$) after 1000 steps: [0 1 2 3 4 5 8 6 7] ...
  ==> Optimal arm identification: 97.25% (relative success)...
  ==> Manhattan   distance from optimal ordering: 90.12% (relative success)...
  ==> Gestalt     distance from optimal ordering: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 89.51% (relative success)...



- Evaluating policy #17/33: UCBoost($\varepsilon=0.1$, $c=0$) ...

Estimated order by the policy UCBoost($\varepsilon=0.1$, $c=0$) after 1000 steps: [0 2 1 3 5 7 4 8 6] ...
  ==> Optimal arm identification: 91.80% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 70.99% (relative success)...



- Evaluating policy #18/33: UCBoost($\varepsilon=0.05$, $c=0$) ...

Estimated order by the policy UCBoost($\varepsilon=0.05$, $c=0$) after 1000 steps: [0 1 3 5 2 4 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 81.48% (relative success)...



- Evaluating policy #19/33: UCBoost($\varepsilon=0.01$, $c=0$) ...

Estimated order by the policy UCBoost($\varepsilon=0.01$, $c=0$) after 1000 steps: [0 1 4 5 6 2 7 3 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 65.43% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 71.60% (relative success)...



- Evaluating policy #20/33: $\mathrm{UCBfaster}_{d=d_{bq}}$($c=0$) ...

Estimated order by the policy $\mathrm{UCBfaster}_{d=d_{bq}}$($c=0$) after 1000 steps: [0 1 5 4 3 2 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 80.25% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 73.46% (relative success)...



- Evaluating policy #21/33: $\mathrm{UCBfaster}_{d=d_h}$($c=0$) ...

Estimated order by the policy $\mathrm{UCBfaster}_{d=d_h}$($c=0$) after 1000 steps: [0 1 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #22/33: $\mathrm{UCBfaster}_{d=d_{lb}}$($c=0$) ...

Estimated order by the policy $\mathrm{UCBfaster}_{d=d_{lb}}$($c=0$) after 1000 steps: [0 1 2 4 6 8 3 5 7] ...
  ==> Optimal arm identification: 97.99% (relative success)...
  ==> Manhattan   distance from optimal ordering: 70.37% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 68.52% (relative success)...



- Evaluating policy #23/33: UCBoostFaster($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$) ...

Estimated order by the policy UCBoostFaster($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$) after 1000 steps: [1 3 4 0 2 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 76.54% (relative success)...



- Evaluating policy #24/33: UCBoostFaster($\varepsilon=0.1$, $c=0$) ...

Estimated order by the policy UCBoostFaster($\varepsilon=0.1$, $c=0$) after 1000 steps: [5 6 0 1 2 3 4 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 50.62% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 64.20% (relative success)...



- Evaluating policy #25/33: UCBoostFaster($\varepsilon=0.05$, $c=0$) ...

Estimated order by the policy UCBoostFaster($\varepsilon=0.05$, $c=0$) after 1000 steps: [3 4 5 7 0 1 2 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 35.80% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 45.68% (relative success)...



- Evaluating policy #26/33: UCBoostFaster($\varepsilon=0.01$, $c=0$) ...

Estimated order by the policy UCBoostFaster($\varepsilon=0.01$, $c=0$) after 1000 steps: [0 1 3 4 6 2 7 5 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 76.54% (relative success)...



- Evaluating policy #27/33: $\mathrm{UCBcython}_{d=d_{bq}}$($c=0$) ...

Estimated order by the policy $\mathrm{UCBcython}_{d=d_{bq}}$($c=0$) after 1000 steps: [2 3 4 5 0 1 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 55.56% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 61.11% (relative success)...



- Evaluating policy #28/33: $\mathrm{UCBcython}_{d=d_h}$($c=0$) ...

Estimated order by the policy $\mathrm{UCBcython}_{d=d_h}$($c=0$) after 1000 steps: [0 5 1 2 3 6 7 4 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 70.37% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 74.07% (relative success)...



- Evaluating policy #29/33: $\mathrm{UCBcython}_{d=d_{lb}}$($c=0$) ...

Estimated order by the policy $\mathrm{UCBcython}_{d=d_{lb}}$($c=0$) after 1000 steps: [2 6 0 1 3 4 5 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 65.43% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 71.60% (relative success)...



- Evaluating policy #30/33: UCBoostcython($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$) ...

Estimated order by the policy UCBoostcython($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$) after 1000 steps: [0 1 2 4 5 6 3 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Gestalt     distance from optimal ordering: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 87.04% (relative success)...



- Evaluating policy #31/33: UCBoostcython($\varepsilon=0.1$, $c=0$) ...

Estimated order by the policy UCBoostcython($\varepsilon=0.1$, $c=0$) after 1000 steps: [0 1 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #32/33: UCBoostcython($\varepsilon=0.05$, $c=0$) ...

Estimated order by the policy UCBoostcython($\varepsilon=0.05$, $c=0$) after 1000 steps: [7 0 1 2 6 3 4 5 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 55.56% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 66.67% (relative success)...



- Evaluating policy #33/33: UCBoostcython($\varepsilon=0.01$, $c=0$) ...

Estimated order by the policy UCBoostcython($\varepsilon=0.01$, $c=0$) after 1000 steps: [4 5 0 2 3 1 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 60.49% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 58.02% (relative success)...


Giving the vector of final regrets ...

  For policy #0 called 'UCB' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 26.2
Mean of   last regrets R_T = 52.6
Median of last regrets R_T = 48.6
Max of    last regrets R_T = 138
STD of    last regrets R_T = 17.3

  For policy #1 called 'UCB($\alpha=1$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 8.2
Mean of   last regrets R_T = 21.6
Median of last regrets R_T = 18.3
Max of    last regrets R_T = 81.7
STD of    last regrets R_T = 10.9

  For policy #2 called 'MOSS' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 7.93
Mean of   last regrets R_T = 18.8
Median of last regrets R_T = 16.9
Max of    last regrets R_T = 65.4
STD of    last regrets R_T = 8.05

  For policy #3 called 'MOSS-H($T=1000$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 8.73
Mean of   last regrets R_T = 19.6
Median of last regrets R_T = 17.7
Max of    last regrets R_T = 63.9
STD of    last regrets R_T = 8.54

  For policy #4 called 'MOSS-Anytime($\alpha=1.35$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 9.07
Mean of   last regrets R_T = 20.9
Median of last regrets R_T = 18.9
Max of    last regrets R_T = 62.6
STD of    last regrets R_T = 8.56

  For policy #5 called 'OC-UCB($\eta=1.1$, $\rho=1$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 25.3
Mean of   last regrets R_T = 52.9
Median of last regrets R_T = 50.2
Max of    last regrets R_T = 120
STD of    last regrets R_T = 16.5

  For policy #6 called 'Thompson' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 2.28
Mean of   last regrets R_T = 11.2
Median of last regrets R_T = 9.32
Max of    last regrets R_T = 54.6
STD of    last regrets R_T = 7.28

  For policy #7 called 'kl-UCB' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 2.11
Mean of   last regrets R_T = 13.2
Median of last regrets R_T = 10.9
Max of    last regrets R_T = 89.4
STD of    last regrets R_T = 9.57

  For policy #8 called 'kl-UCB$^{++}$($T=1000$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 2.16
Mean of   last regrets R_T = 13.5
Median of last regrets R_T = 11.5
Max of    last regrets R_T = 60.3
STD of    last regrets R_T = 7.63

  For policy #9 called 'BayesUCB' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 0.083
Mean of   last regrets R_T = 9.63
Median of last regrets R_T = 7.94
Max of    last regrets R_T = 78.9
STD of    last regrets R_T = 8.08

  For policy #10 called 'AdBandits($T=1000$, $\alpha=0.5$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 2.05
Mean of   last regrets R_T = 8.17
Median of last regrets R_T = 6.92
Max of    last regrets R_T = 29.9
STD of    last regrets R_T = 4.43

  For policy #11 called 'ApprFHG($T=1100$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 9.72
Mean of   last regrets R_T = 21
Median of last regrets R_T = 19.1
Max of    last regrets R_T = 74.3
STD of    last regrets R_T = 8.34

  For policy #12 called '$\mathrm{UCB}_{d=d_{bq}}$($c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 6.14
Mean of   last regrets R_T = 20.9
Median of last regrets R_T = 17.9
Max of    last regrets R_T = 69.1
STD of    last regrets R_T = 11.1

  For policy #13 called '$\mathrm{UCB}_{d=d_h}$($c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 1.97
Mean of   last regrets R_T = 12.9
Median of last regrets R_T = 3.43
Max of    last regrets R_T = 461
STD of    last regrets R_T = 45.4

  For policy #14 called '$\mathrm{UCB}_{d=d_{lb}}$($c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 1.98
Mean of   last regrets R_T = 10.1
Median of last regrets R_T = 8.61
Max of    last regrets R_T = 70.1
STD of    last regrets R_T = 6.71

  For policy #15 called 'UCBoost($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 1.97
Mean of   last regrets R_T = 13.5
Median of last regrets R_T = 3.35
Max of    last regrets R_T = 618
STD of    last regrets R_T = 49.1

  For policy #16 called 'UCBoost($\varepsilon=0.1$, $c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 2.07
Mean of   last regrets R_T = 12.6
Median of last regrets R_T = 10.2
Max of    last regrets R_T = 81
STD of    last regrets R_T = 9.41

  For policy #17 called 'UCBoost($\varepsilon=0.05$, $c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 2
Mean of   last regrets R_T = 11.9
Median of last regrets R_T = 9.09
Max of    last regrets R_T = 100
STD of    last regrets R_T = 8.94

  For policy #18 called 'UCBoost($\varepsilon=0.01$, $c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 1.98
Mean of   last regrets R_T = 13.4
Median of last regrets R_T = 11.3
Max of    last regrets R_T = 63.8
STD of    last regrets R_T = 8.73

  For policy #19 called '$\mathrm{UCBfaster}_{d=d_{bq}}$($c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 6.55
Mean of   last regrets R_T = 20.5
Median of last regrets R_T = 17.1
Max of    last regrets R_T = 99.5
STD of    last regrets R_T = 11.4

  For policy #20 called '$\mathrm{UCBfaster}_{d=d_h}$($c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 1.97
Mean of   last regrets R_T = 14.8
Median of last regrets R_T = 3.64
Max of    last regrets R_T = 461
STD of    last regrets R_T = 47.2

  For policy #21 called '$\mathrm{UCBfaster}_{d=d_{lb}}$($c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 2.03
Mean of   last regrets R_T = 10.1
Median of last regrets R_T = 8.88
Max of    last regrets R_T = 42.3
STD of    last regrets R_T = 5.78

  For policy #22 called 'UCBoostFaster($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 1.97
Mean of   last regrets R_T = 12
Median of last regrets R_T = 3.53
Max of    last regrets R_T = 522
STD of    last regrets R_T = 37.8

  For policy #23 called 'UCBoostFaster($\varepsilon=0.1$, $c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 2.2
Mean of   last regrets R_T = 12.3
Median of last regrets R_T = 9.53
Max of    last regrets R_T = 68.2
STD of    last regrets R_T = 9.21

  For policy #24 called 'UCBoostFaster($\varepsilon=0.05$, $c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 1.97
Mean of   last regrets R_T = 12.7
Median of last regrets R_T = 10.2
Max of    last regrets R_T = 90.3
STD of    last regrets R_T = 9.31

  For policy #25 called 'UCBoostFaster($\varepsilon=0.01$, $c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 2
Mean of   last regrets R_T = 13.5
Median of last regrets R_T = 11
Max of    last regrets R_T = 68.4
STD of    last regrets R_T = 9.61

  For policy #26 called '$\mathrm{UCBcython}_{d=d_{bq}}$($c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 6.45
Mean of   last regrets R_T = 20.5
Median of last regrets R_T = 17.8
Max of    last regrets R_T = 94.3
STD of    last regrets R_T = 11.1

  For policy #27 called '$\mathrm{UCBcython}_{d=d_h}$($c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 1.97
Mean of   last regrets R_T = 14.5
Median of last regrets R_T = 3.34
Max of    last regrets R_T = 461
STD of    last regrets R_T = 44.5

  For policy #28 called '$\mathrm{UCBcython}_{d=d_{lb}}$($c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 2.08
Mean of   last regrets R_T = 10.4
Median of last regrets R_T = 9.3
Max of    last regrets R_T = 50.2
STD of    last regrets R_T = 5.96

  For policy #29 called 'UCBoostcython($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 1.97
Mean of   last regrets R_T = 10.9
Median of last regrets R_T = 3.42
Max of    last regrets R_T = 362
STD of    last regrets R_T = 32.3

  For policy #30 called 'UCBoostcython($\varepsilon=0.1$, $c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 4.15
Mean of   last regrets R_T = 16.4
Median of last regrets R_T = 13.7
Max of    last regrets R_T = 100
STD of    last regrets R_T = 10.7

  For policy #31 called 'UCBoostcython($\varepsilon=0.05$, $c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 3.98
Mean of   last regrets R_T = 17.5
Median of last regrets R_T = 15.5
Max of    last regrets R_T = 99.2
STD of    last regrets R_T = 9.93

  For policy #32 called 'UCBoostcython($\varepsilon=0.01$, $c=0$)' ...
  Last regrets vector (for all repetitions) is:
Shape of  last regrets R_T = (500,)
Min of    last regrets R_T = 5.38
Mean of   last regrets R_T = 15.6
Median of last regrets R_T = 12.9
Max of    last regrets R_T = 85.6
STD of    last regrets R_T = 9.25


Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'AdBandits($T=1000$, $\alpha=0.5$)'	was ranked	1 / 33 for this simulation (last regret = 8.1689).
- Policy 'BayesUCB'	was ranked	2 / 33 for this simulation (last regret = 9.5932).
- Policy '$\mathrm{UCB}_{d=d_{lb}}$($c=0$)'	was ranked	3 / 33 for this simulation (last regret = 10.046).
- Policy '$\mathrm{UCBfaster}_{d=d_{lb}}$($c=0$)'	was ranked	4 / 33 for this simulation (last regret = 10.088).
- Policy '$\mathrm{UCBcython}_{d=d_{lb}}$($c=0$)'	was ranked	5 / 33 for this simulation (last regret = 10.396).
- Policy 'UCBoostcython($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)'	was ranked	6 / 33 for this simulation (last regret = 10.873).
- Policy 'Thompson'	was ranked	7 / 33 for this simulation (last regret = 11.157).
- Policy 'UCBoost($\varepsilon=0.05$, $c=0$)'	was ranked	8 / 33 for this simulation (last regret = 11.858).
- Policy 'UCBoostFaster($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)'	was ranked	9 / 33 for this simulation (last regret = 11.971).
- Policy 'UCBoostFaster($\varepsilon=0.1$, $c=0$)'	was ranked	10 / 33 for this simulation (last regret = 12.291).
- Policy 'UCBoost($\varepsilon=0.1$, $c=0$)'	was ranked	11 / 33 for this simulation (last regret = 12.562).
- Policy 'UCBoostFaster($\varepsilon=0.05$, $c=0$)'	was ranked	12 / 33 for this simulation (last regret = 12.724).
- Policy '$\mathrm{UCB}_{d=d_h}$($c=0$)'	was ranked	13 / 33 for this simulation (last regret = 12.794).
- Policy 'kl-UCB'	was ranked	14 / 33 for this simulation (last regret = 13.173).
- Policy 'UCBoost($\varepsilon=0.01$, $c=0$)'	was ranked	15 / 33 for this simulation (last regret = 13.389).
- Policy 'UCBoost($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)'	was ranked	16 / 33 for this simulation (last regret = 13.402).
- Policy 'UCBoostFaster($\varepsilon=0.01$, $c=0$)'	was ranked	17 / 33 for this simulation (last regret = 13.477).
- Policy 'kl-UCB$^{++}$($T=1000$)'	was ranked	18 / 33 for this simulation (last regret = 13.525).
- Policy '$\mathrm{UCBcython}_{d=d_h}$($c=0$)'	was ranked	19 / 33 for this simulation (last regret = 14.393).
- Policy '$\mathrm{UCBfaster}_{d=d_h}$($c=0$)'	was ranked	20 / 33 for this simulation (last regret = 14.7).
- Policy 'UCBoostcython($\varepsilon=0.01$, $c=0$)'	was ranked	21 / 33 for this simulation (last regret = 15.525).
- Policy 'UCBoostcython($\varepsilon=0.1$, $c=0$)'	was ranked	22 / 33 for this simulation (last regret = 16.303).
- Policy 'UCBoostcython($\varepsilon=0.05$, $c=0$)'	was ranked	23 / 33 for this simulation (last regret = 17.457).
- Policy 'MOSS'	was ranked	24 / 33 for this simulation (last regret = 18.769).
- Policy 'MOSS-H($T=1000$)'	was ranked	25 / 33 for this simulation (last regret = 19.631).
- Policy '$\mathrm{UCBcython}_{d=d_{bq}}$($c=0$)'	was ranked	26 / 33 for this simulation (last regret = 20.498).
- Policy '$\mathrm{UCBfaster}_{d=d_{bq}}$($c=0$)'	was ranked	27 / 33 for this simulation (last regret = 20.502).
- Policy '$\mathrm{UCB}_{d=d_{bq}}$($c=0$)'	was ranked	28 / 33 for this simulation (last regret = 20.812).
- Policy 'MOSS-Anytime($\alpha=1.35$)'	was ranked	29 / 33 for this simulation (last regret = 20.871).
- Policy 'ApprFHG($T=1100$)'	was ranked	30 / 33 for this simulation (last regret = 20.954).
- Policy 'UCB($\alpha=1$)'	was ranked	31 / 33 for this simulation (last regret = 21.546).
- Policy 'UCB'	was ranked	32 / 33 for this simulation (last regret = 52.415).
- Policy 'OC-UCB($\eta=1.1$, $\rho=1$)'	was ranked	33 / 33 for this simulation (last regret = 52.672).


Giving the mean and std running times ...

For policy #5 called 'OC-UCB($\eta=1.1$, $\rho=1$)' ...
    57.3 ms ± 8.15 ms per loop (mean ± std. dev. of 500 runs)

For policy #1 called 'UCB($\alpha=1$)' ...
    61.3 ms ± 9.82 ms per loop (mean ± std. dev. of 500 runs)

For policy #4 called 'MOSS-Anytime($\alpha=1.35$)' ...
    63.9 ms ± 9.67 ms per loop (mean ± std. dev. of 500 runs)

For policy #2 called 'MOSS' ...
    65.2 ms ± 12.2 ms per loop (mean ± std. dev. of 500 runs)

For policy #6 called 'Thompson' ...
    65.8 ms ± 7.82 ms per loop (mean ± std. dev. of 500 runs)

For policy #3 called 'MOSS-H($T=1000$)' ...
    65.8 ms ± 11.2 ms per loop (mean ± std. dev. of 500 runs)

For policy #11 called 'ApprFHG($T=1100$)' ...
    75 ms ± 11.2 ms per loop (mean ± std. dev. of 500 runs)

For policy #0 called 'UCB' ...
    96.9 ms ± 43.2 ms per loop (mean ± std. dev. of 500 runs)

For policy #27 called '$\mathrm{UCBcython}_{d=d_h}$($c=0$)' ...
    102 ms ± 15.1 ms per loop (mean ± std. dev. of 500 runs)

For policy #26 called '$\mathrm{UCBcython}_{d=d_{bq}}$($c=0$)' ...
    104 ms ± 16.1 ms per loop (mean ± std. dev. of 500 runs)

For policy #28 called '$\mathrm{UCBcython}_{d=d_{lb}}$($c=0$)' ...
    104 ms ± 14.9 ms per loop (mean ± std. dev. of 500 runs)

For policy #20 called '$\mathrm{UCBfaster}_{d=d_h}$($c=0$)' ...
    112 ms ± 15.9 ms per loop (mean ± std. dev. of 500 runs)

For policy #19 called '$\mathrm{UCBfaster}_{d=d_{bq}}$($c=0$)' ...
    112 ms ± 18.1 ms per loop (mean ± std. dev. of 500 runs)

For policy #21 called '$\mathrm{UCBfaster}_{d=d_{lb}}$($c=0$)' ...
    113 ms ± 15.4 ms per loop (mean ± std. dev. of 500 runs)

For policy #29 called 'UCBoostcython($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)' ...
    124 ms ± 16.3 ms per loop (mean ± std. dev. of 500 runs)

For policy #12 called '$\mathrm{UCB}_{d=d_{bq}}$($c=0$)' ...
    125 ms ± 23.4 ms per loop (mean ± std. dev. of 500 runs)

For policy #13 called '$\mathrm{UCB}_{d=d_h}$($c=0$)' ...
    127 ms ± 25.8 ms per loop (mean ± std. dev. of 500 runs)

For policy #14 called '$\mathrm{UCB}_{d=d_{lb}}$($c=0$)' ...
    128 ms ± 23.7 ms per loop (mean ± std. dev. of 500 runs)

For policy #9 called 'BayesUCB' ...
    142 ms ± 20.5 ms per loop (mean ± std. dev. of 500 runs)

For policy #30 called 'UCBoostcython($\varepsilon=0.1$, $c=0$)' ...
    142 ms ± 18.6 ms per loop (mean ± std. dev. of 500 runs)

For policy #10 called 'AdBandits($T=1000$, $\alpha=0.5$)' ...
    142 ms ± 22.6 ms per loop (mean ± std. dev. of 500 runs)

For policy #7 called 'kl-UCB' ...
    145 ms ± 28 ms per loop (mean ± std. dev. of 500 runs)

For policy #22 called 'UCBoostFaster($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)' ...
    150 ms ± 19.1 ms per loop (mean ± std. dev. of 500 runs)

For policy #8 called 'kl-UCB$^{++}$($T=1000$)' ...
    169 ms ± 28.7 ms per loop (mean ± std. dev. of 500 runs)

For policy #23 called 'UCBoostFaster($\varepsilon=0.1$, $c=0$)' ...
    180 ms ± 24.1 ms per loop (mean ± std. dev. of 500 runs)

For policy #15 called 'UCBoost($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)' ...
    184 ms ± 36.5 ms per loop (mean ± std. dev. of 500 runs)

For policy #31 called 'UCBoostcython($\varepsilon=0.05$, $c=0$)' ...
    195 ms ± 29.7 ms per loop (mean ± std. dev. of 500 runs)

For policy #24 called 'UCBoostFaster($\varepsilon=0.05$, $c=0$)' ...
    238 ms ± 41 ms per loop (mean ± std. dev. of 500 runs)

For policy #16 called 'UCBoost($\varepsilon=0.1$, $c=0$)' ...
    328 ms ± 60.1 ms per loop (mean ± std. dev. of 500 runs)

For policy #17 called 'UCBoost($\varepsilon=0.05$, $c=0$)' ...
    569 ms ± 152 ms per loop (mean ± std. dev. of 500 runs)

For policy #32 called 'UCBoostcython($\varepsilon=0.01$, $c=0$)' ...
    934 ms ± 240 ms per loop (mean ± std. dev. of 500 runs)

For policy #25 called 'UCBoostFaster($\varepsilon=0.01$, $c=0$)' ...
    935 ms ± 269 ms per loop (mean ± std. dev. of 500 runs)

For policy #18 called 'UCBoost($\varepsilon=0.01$, $c=0$)' ...
    3.56 s ± 930 ms per loop (mean ± std. dev. of 500 runs)

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 57.1 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 67.00% ...
Warning: forcing to use putatright = True because there is 34 items in the legend.
Warning: forcing to use putatright = True because there is 34 items in the legend.
Warning: forcing to use putatright = True because there is 34 items in the legend.
Warning: forcing to use putatright = True because there is 34 items in the legend.

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 57.1 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 67.00% ...
Warning: forcing to use putatright = True because there is 34 items in the legend.
Warning: forcing to use putatright = True because there is 33 items in the legend.
Warning: forcing to use putatright = True because there is 33 items in the legend.
Done for simulations main.py ...
