Loaded experiments configuration from 'configuration_all_singleplayer.py' :
configuration['policies'] = [{'archtype': <class 'Policies.Uniform.Uniform'>, 'params': {}}, {'archtype': <class 'Policies.UniformOnSome.UniformOnSome'>, 'params': {'armIndexes': [0, 1]}}, {'archtype': <class 'Policies.EmpiricalMeans.EmpiricalMeans'>, 'params': {}}, {'archtype': <class 'Policies.TakeRandomFixedArm.TakeRandomFixedArm'>, 'params': {}}, {'archtype': <class 'Policies.UniformOnSome.UniformOnSome'>, 'params': {'armIndexes': [0, 1]}}, {'archtype': <class 'Policies.TakeFixedArm.TakeFixedArm'>, 'params': {'armIndex': 1}}, {'archtype': <class 'Policies.TakeFixedArm.TakeFixedArm'>, 'params': {'armIndex': 0}}, {'archtype': <class 'Policies.TakeFixedArm.TakeFixedArm'>, 'params': {'armIndex': 0}}, {'archtype': <class 'Policies.TakeFixedArm.TakeFixedArm'>, 'params': {'armIndex': 1}}, {'archtype': <class 'Policies.EpsilonGreedy.EpsilonGreedy'>, 'params': {'epsilon': 0.1}}, {'archtype': <class 'Policies.EpsilonGreedy.EpsilonDecreasing'>, 'params': {'epsilon': 0.1}}, {'archtype': <class 'Policies.EpsilonGreedy.EpsilonExpDecreasing'>, 'params': {'epsilon': 0.1, 'decreasingRate': 0.005}}, {'archtype': <class 'Policies.EpsilonGreedy.EpsilonFirst'>, 'params': {'epsilon': 0.1, 'horizon': 1000}}, {'archtype': <class 'Policies.ExploreThenCommit.ETC_KnownGap'>, 'params': {'horizon': 1000, 'gap': 0.05}}, {'archtype': <class 'Policies.ExploreThenCommit.ETC_RandomStop'>, 'params': {'horizon': 1000}}, {'archtype': <class 'Policies.Softmax.Softmax'>, 'params': {'temperature': 0.05}}, {'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>, 'params': {}}, {'archtype': <class 'Policies.Softmax.SoftMix'>, 'params': {}}, {'archtype': <class 'Policies.Softmax.SoftmaxWithHorizon'>, 'params': {'horizon': 1000}}, {'archtype': <class 'Policies.BoltzmannGumbel.BoltzmannGumbel'>, 'params': {'C': 0.5}}, {'archtype': <class 'Policies.Exp3.Exp3'>, 'params': {'gamma': 0.001}}, {'archtype': <class 'Policies.Exp3.Exp3Decreasing'>, 'params': {'gamma': 0.001}}, {'archtype': <class 'Policies.Exp3.Exp3SoftMix'>, 'params': {}}, {'archtype': <class 'Policies.Exp3.Exp3WithHorizon'>, 'params': {'horizon': 1000}}, {'archtype': <class 'Policies.Exp3.Exp3ELM'>, 'params': {'delta': 0.1}}, {'archtype': <class 'Policies.Exp3PlusPlus.Exp3PlusPlus'>, 'params': {}}, {'archtype': <class 'Policies.ProbabilityPursuit.ProbabilityPursuit'>, 'params': {'beta': 0.5}}, {'archtype': <class 'Policies.Hedge.Hedge'>, 'params': {'epsilon': 0.5}}, {'archtype': <class 'Policies.Hedge.HedgeDecreasing'>, 'params': {}}, {'archtype': <class 'Policies.Hedge.HedgeWithHorizon'>, 'params': {'horizon': 1000}}, {'archtype': <class 'Policies.UCB.UCB'>, 'params': {}}, {'archtype': <class 'Policies.Experimentals.UCBlog10.UCBlog10'>, 'params': {}}, {'archtype': <class 'Policies.Experimentals.UCBwrong.UCBwrong'>, 'params': {}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.Experimentals.UCBlog10alpha.UCBlog10alpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.UCBmin.UCBmin'>, 'params': {}}, {'archtype': <class 'Policies.UCBplus.UCBplus'>, 'params': {}}, {'archtype': <class 'Policies.UCBrandomInit.UCBrandomInit'>, 'params': {}}, {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBcython.UCBcython'>, 'params': {}}, {'archtype': <class 'Policies.UCBV.UCBV'>, 'params': {}}, {'archtype': <class 'Policies.UCBVtuned.UCBVtuned'>, 'params': {}}, {'archtype': <class 'Policies.SlidingWindowUCB.SWUCB'>, 'params': {}}, {'archtype': <class 'Policies.SlidingWindowUCB.SWUCBPlus'>, 'params': {}}, {'archtype': <class 'Policies.DiscountedUCB.DiscountedUCB'>, 'params': {}}, {'archtype': <class 'Policies.DiscountedUCB.DiscountedUCBPlus'>, 'params': {}}, {'archtype': <class 'Policies.SlidingWindowRestart.SWR_UCB'>, 'params': {}}, {'archtype': <class 'Policies.SlidingWindowRestart.SWR_UCBalpha'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.SparseUCB.SparseUCB'>, 'params': {'alpha': 4, 'sparsity': 2}}, {'archtype': <class 'Policies.SparseklUCB.SparseklUCB'>, 'params': {'sparsity': 2}}, {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}}, {'archtype': <class 'Policies.MOSSH.MOSSH'>, 'params': {'horizon': 1000}}, {'archtype': <class 'Policies.MOSSAnytime.MOSSAnytime'>, 'params': {'alpha': 1.35}}, {'archtype': <class 'Policies.MOSSExperimental.MOSSExperimental'>, 'params': {}}, {'archtype': <class 'Policies.OCUCB.OCUCB'>, 'params': {'eta': 1.1, 'rho': 1}}, {'archtype': <class 'Policies.CPUCB.CPUCB'>, 'params': {}}, {'archtype': <class 'Policies.DMED.DMEDPlus'>, 'params': {}}, {'archtype': <class 'Policies.DMED.DMED'>, 'params': {}}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Gauss.Gauss'>}}, {'archtype': <class 'Policies.Experimentals.ThompsonRobust.ThompsonRobust'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}}, {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.SlidingWindowRestart.SWR_klUCB'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBloglog.klUCBloglog'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.Experimentals.klUCBlog10.klUCBlog10'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.Experimentals.klUCBloglog10.klUCBloglog10'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBH.klUCBH'>, 'params': {'horizon': 1000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>, 'params': {'horizon': 1000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 1000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}}, {'archtype': <class 'Policies.AdBandits.AdBandits'>, 'params': {'alpha': 0.5, 'horizon': 1000}}, {'archtype': <class 'Policies.ApproximatedFHGittins.ApproximatedFHGittins'>, 'params': {'alpha': 0.5, 'horizon': 1100}}, {'archtype': <class 'Policies.Experimentals.UnsupervisedLearning.UnsupervisedLearning'>, 'params': {}}, {'archtype': <class 'Policies.OSSB.OSSB'>, 'params': {'epsilon': 0.01, 'gamma': 0.0}}, {'archtype': <class 'Policies.BESA.BESA'>, 'params': {'horizon': 1000, 'minPullsOfEachArm': 1, 'randomized_tournament': True, 'random_subsample': True, 'non_binary': False, 'non_recursive': False}}, {'archtype': <class 'Policies.UCBdagger.UCBdagger'>, 'params': {'horizon': 1000}}, {'archtype': <class 'Policies.UCBoost.UCB_bq'>, 'params': {}}, {'archtype': <class 'Policies.UCBoost.UCB_h'>, 'params': {}}, {'archtype': <class 'Policies.UCBoost.UCB_lb'>, 'params': {}}, {'archtype': <class 'Policies.UCBoost.UCBoost_bq_h_lb'>, 'params': {}}, {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.1}}, {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.05}}, {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.01}}, {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCB_bq'>, 'params': {}}, {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCB_h'>, 'params': {}}, {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCB_lb'>, 'params': {}}, {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCBoost_bq_h_lb'>, 'params': {}}, {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.1}}, {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.05}}, {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.01}}, {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBcython.UCBcython'>, 'params': {'alpha': 4.0}}, {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBcython.UCBcython'>, 'params': {'alpha': 1.0}}, {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBcython.UCBcython'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.Exp3PlusPlus.Exp3PlusPlus'>, 'params': {}}, {'archtype': <class 'Policies.DiscountedThompson.DiscountedThompson'>, 'params': {'posterior': <class 'Policies.Posterior.DiscountedBeta.DiscountedBeta'>, 'gamma': 0.99}}, {'archtype': <class 'Policies.Exp3R.Exp3R'>, 'params': {'horizon': 1000}}, {'archtype': <class 'Policies.Exp3R.Exp3RPlusPlus'>, 'params': {'horizon': 1000}}, {'archtype': <class 'Policies.AdSwitch.AdSwitch'>, 'params': {'horizon': 1000}}, {'archtype': <class 'Policies.LM_DSEE.LM_DSEE'>, 'params': {'nu': 0.25, 'DeltaMin': 0.1, 'a': 1, 'b': 0.25}}, {'archtype': <class 'Policies.CD_UCB.CUSUM_IndexPolicy'>, 'params': {'horizon': 1000, 'max_nb_random_events': 1, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True}}, {'archtype': <class 'Policies.CD_UCB.PHT_IndexPolicy'>, 'params': {'horizon': 1000, 'max_nb_random_events': 1, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True}}, {'archtype': <class 'Policies.SWHash_UCB.SWHash_IndexPolicy'>, 'params': {'alpha': 1, 'lmbda': 1, 'policy': <class 'Policies.UCB.UCB'>}}, {'archtype': <class 'Policies.SlidingWindowRestart.SlidingWindowRestart'>, 'params': {'policy': <class 'Policies.UCB.UCB'>}}, {'archtype': <class 'Policies.SlidingWindowUCB.SWUCB'>, 'params': {'alpha': 1, 'tau': 500}}, {'archtype': <class 'Policies.SlidingWindowUCB.SWUCBPlus'>, 'params': {'horizon': 1000, 'alpha': 1}}, {'archtype': <class 'Policies.DiscountedUCB.DiscountedUCB'>, 'params': {'alpha': 1, 'gamma': 0.9}}, {'archtype': <class 'Policies.DiscountedUCB.DiscountedUCBPlus'>, 'params': {'max_nb_random_events': 1, 'alpha': 1, 'horizon': 1000}}, {'archtype': <class 'Policies.Monitored_UCB.Monitored_IndexPolicy'>, 'params': {'horizon': 1000, 'max_nb_random_events': 1, 'delta': 0.1, 'policy': <class 'Policies.UCB.UCB'>}}, {'archtype': <class 'Policies.Monitored_UCB.Monitored_IndexPolicy'>, 'params': {'policy': <class 'Policies.UCB.UCB'>, 'horizon': 1000, 'w': 80, 'b': 25.85024743855303}}, {'archtype': <class 'Policies.OracleSequentiallyRestartPolicy.OracleSequentiallyRestartPolicy'>, 'params': {'changePoints': [], 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True}}, {'archtype': <class 'Policies.CD_UCB.BernoulliGLR_IndexPolicy'>, 'params': {'horizon': 1000, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True, 'max_nb_random_events': 1}}, {'archtype': <class 'Policies.CD_UCB.BernoulliGLR_IndexPolicy_WithTracking'>, 'params': {'horizon': 1000, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True, 'max_nb_random_events': 1}}, {'archtype': <class 'Policies.CD_UCB.GaussianGLR_IndexPolicy'>, 'params': {'horizon': 1000, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True, 'max_nb_random_events': 1}}, {'archtype': <class 'Policies.CD_UCB.GaussianGLR_IndexPolicy_WithTracking'>, 'params': {'horizon': 1000, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True, 'max_nb_random_events': 1}}, {'archtype': <class 'Policies.CD_UCB.SubGaussianGLR_IndexPolicy'>, 'params': {'horizon': 1000, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True, 'max_nb_random_events': 1}}]
====> TURNING DEBUG MODE ON <=====
plots/ is already a directory here...
Number of policies in this comparison: 115
Time horizon: 1000
Number of repetitions: 4
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: -1
Using this dictionary to create a new environment:
 {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6, 0.7000000000000001, 0.8, 0.9]}


Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6, 0.7000000000000001, 0.8, 0.9]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6, 0.7000000000000001, 0.8, 0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'means' = [0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - with 'arms' represented as: $[B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)^*]$
Number of environments to try: 1
Warning: environment MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9) did not have a method plotHistoryOfMeans...


Evaluating environment: MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9)
- Adding policy #1 = {'archtype': <class 'Policies.Uniform.Uniform'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'archtype': <class 'Policies.Uniform.Uniform'>, 'params': {}} ...
- Adding policy #2 = {'archtype': <class 'Policies.UniformOnSome.UniformOnSome'>, 'params': {'armIndexes': [0, 1]}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'archtype': <class 'Policies.UniformOnSome.UniformOnSome'>, 'params': {'armIndexes': [0, 1]}} ...
- Adding policy #3 = {'archtype': <class 'Policies.EmpiricalMeans.EmpiricalMeans'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'archtype': <class 'Policies.EmpiricalMeans.EmpiricalMeans'>, 'params': {}} ...
- Adding policy #4 = {'archtype': <class 'Policies.TakeRandomFixedArm.TakeRandomFixedArm'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'archtype': <class 'Policies.TakeRandomFixedArm.TakeRandomFixedArm'>, 'params': {}} ...
- Adding policy #5 = {'archtype': <class 'Policies.UniformOnSome.UniformOnSome'>, 'params': {'armIndexes': [0, 1]}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][4]' = {'archtype': <class 'Policies.UniformOnSome.UniformOnSome'>, 'params': {'armIndexes': [0, 1]}} ...
- Adding policy #6 = {'archtype': <class 'Policies.TakeFixedArm.TakeFixedArm'>, 'params': {'armIndex': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][5]' = {'archtype': <class 'Policies.TakeFixedArm.TakeFixedArm'>, 'params': {'armIndex': 1}} ...
- Adding policy #7 = {'archtype': <class 'Policies.TakeFixedArm.TakeFixedArm'>, 'params': {'armIndex': 0}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][6]' = {'archtype': <class 'Policies.TakeFixedArm.TakeFixedArm'>, 'params': {'armIndex': 0}} ...
- Adding policy #8 = {'archtype': <class 'Policies.TakeFixedArm.TakeFixedArm'>, 'params': {'armIndex': 0}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][7]' = {'archtype': <class 'Policies.TakeFixedArm.TakeFixedArm'>, 'params': {'armIndex': 0}} ...
- Adding policy #9 = {'archtype': <class 'Policies.TakeFixedArm.TakeFixedArm'>, 'params': {'armIndex': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][8]' = {'archtype': <class 'Policies.TakeFixedArm.TakeFixedArm'>, 'params': {'armIndex': 1}} ...
- Adding policy #10 = {'archtype': <class 'Policies.EpsilonGreedy.EpsilonGreedy'>, 'params': {'epsilon': 0.1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][9]' = {'archtype': <class 'Policies.EpsilonGreedy.EpsilonGreedy'>, 'params': {'epsilon': 0.1}} ...
- Adding policy #11 = {'archtype': <class 'Policies.EpsilonGreedy.EpsilonDecreasing'>, 'params': {'epsilon': 0.1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][10]' = {'archtype': <class 'Policies.EpsilonGreedy.EpsilonDecreasing'>, 'params': {'epsilon': 0.1}} ...
- Adding policy #12 = {'archtype': <class 'Policies.EpsilonGreedy.EpsilonExpDecreasing'>, 'params': {'epsilon': 0.1, 'decreasingRate': 0.005}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][11]' = {'archtype': <class 'Policies.EpsilonGreedy.EpsilonExpDecreasing'>, 'params': {'epsilon': 0.1, 'decreasingRate': 0.005}} ...
- Adding policy #13 = {'archtype': <class 'Policies.EpsilonGreedy.EpsilonFirst'>, 'params': {'epsilon': 0.1, 'horizon': 1000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][12]' = {'archtype': <class 'Policies.EpsilonGreedy.EpsilonFirst'>, 'params': {'epsilon': 0.1, 'horizon': 1000}} ...
- Adding policy #14 = {'archtype': <class 'Policies.ExploreThenCommit.ETC_KnownGap'>, 'params': {'horizon': 1000, 'gap': 0.05}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][13]' = {'archtype': <class 'Policies.ExploreThenCommit.ETC_KnownGap'>, 'params': {'horizon': 1000, 'gap': 0.05}} ...
- Adding policy #15 = {'archtype': <class 'Policies.ExploreThenCommit.ETC_RandomStop'>, 'params': {'horizon': 1000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][14]' = {'archtype': <class 'Policies.ExploreThenCommit.ETC_RandomStop'>, 'params': {'horizon': 1000}} ...
- Adding policy #16 = {'archtype': <class 'Policies.Softmax.Softmax'>, 'params': {'temperature': 0.05}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][15]' = {'archtype': <class 'Policies.Softmax.Softmax'>, 'params': {'temperature': 0.05}} ...
- Adding policy #17 = {'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][16]' = {'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>, 'params': {}} ...
- Adding policy #18 = {'archtype': <class 'Policies.Softmax.SoftMix'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][17]' = {'archtype': <class 'Policies.Softmax.SoftMix'>, 'params': {}} ...
- Adding policy #19 = {'archtype': <class 'Policies.Softmax.SoftmaxWithHorizon'>, 'params': {'horizon': 1000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][18]' = {'archtype': <class 'Policies.Softmax.SoftmaxWithHorizon'>, 'params': {'horizon': 1000}} ...
- Adding policy #20 = {'archtype': <class 'Policies.BoltzmannGumbel.BoltzmannGumbel'>, 'params': {'C': 0.5}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][19]' = {'archtype': <class 'Policies.BoltzmannGumbel.BoltzmannGumbel'>, 'params': {'C': 0.5}} ...
- Adding policy #21 = {'archtype': <class 'Policies.Exp3.Exp3'>, 'params': {'gamma': 0.001}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][20]' = {'archtype': <class 'Policies.Exp3.Exp3'>, 'params': {'gamma': 0.001}} ...
- Adding policy #22 = {'archtype': <class 'Policies.Exp3.Exp3Decreasing'>, 'params': {'gamma': 0.001}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][21]' = {'archtype': <class 'Policies.Exp3.Exp3Decreasing'>, 'params': {'gamma': 0.001}} ...
- Adding policy #23 = {'archtype': <class 'Policies.Exp3.Exp3SoftMix'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][22]' = {'archtype': <class 'Policies.Exp3.Exp3SoftMix'>, 'params': {}} ...
- Adding policy #24 = {'archtype': <class 'Policies.Exp3.Exp3WithHorizon'>, 'params': {'horizon': 1000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][23]' = {'archtype': <class 'Policies.Exp3.Exp3WithHorizon'>, 'params': {'horizon': 1000}} ...
- Adding policy #25 = {'archtype': <class 'Policies.Exp3.Exp3ELM'>, 'params': {'delta': 0.1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][24]' = {'archtype': <class 'Policies.Exp3.Exp3ELM'>, 'params': {'delta': 0.1}} ...
- Adding policy #26 = {'archtype': <class 'Policies.Exp3PlusPlus.Exp3PlusPlus'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][25]' = {'archtype': <class 'Policies.Exp3PlusPlus.Exp3PlusPlus'>, 'params': {}} ...
- Adding policy #27 = {'archtype': <class 'Policies.ProbabilityPursuit.ProbabilityPursuit'>, 'params': {'beta': 0.5}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][26]' = {'archtype': <class 'Policies.ProbabilityPursuit.ProbabilityPursuit'>, 'params': {'beta': 0.5}} ...
- Adding policy #28 = {'archtype': <class 'Policies.Hedge.Hedge'>, 'params': {'epsilon': 0.5}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][27]' = {'archtype': <class 'Policies.Hedge.Hedge'>, 'params': {'epsilon': 0.5}} ...
- Adding policy #29 = {'archtype': <class 'Policies.Hedge.HedgeDecreasing'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][28]' = {'archtype': <class 'Policies.Hedge.HedgeDecreasing'>, 'params': {}} ...
- Adding policy #30 = {'archtype': <class 'Policies.Hedge.HedgeWithHorizon'>, 'params': {'horizon': 1000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][29]' = {'archtype': <class 'Policies.Hedge.HedgeWithHorizon'>, 'params': {'horizon': 1000}} ...
- Adding policy #31 = {'archtype': <class 'Policies.UCB.UCB'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][30]' = {'archtype': <class 'Policies.UCB.UCB'>, 'params': {}} ...
- Adding policy #32 = {'archtype': <class 'Policies.Experimentals.UCBlog10.UCBlog10'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][31]' = {'archtype': <class 'Policies.Experimentals.UCBlog10.UCBlog10'>, 'params': {}} ...
- Adding policy #33 = {'archtype': <class 'Policies.Experimentals.UCBwrong.UCBwrong'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][32]' = {'archtype': <class 'Policies.Experimentals.UCBwrong.UCBwrong'>, 'params': {}} ...
- Adding policy #34 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][33]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
- Adding policy #35 = {'archtype': <class 'Policies.Experimentals.UCBlog10alpha.UCBlog10alpha'>, 'params': {'alpha': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][34]' = {'archtype': <class 'Policies.Experimentals.UCBlog10alpha.UCBlog10alpha'>, 'params': {'alpha': 1}} ...
- Adding policy #36 = {'archtype': <class 'Policies.UCBmin.UCBmin'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][35]' = {'archtype': <class 'Policies.UCBmin.UCBmin'>, 'params': {}} ...
- Adding policy #37 = {'archtype': <class 'Policies.UCBplus.UCBplus'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][36]' = {'archtype': <class 'Policies.UCBplus.UCBplus'>, 'params': {}} ...
- Adding policy #38 = {'archtype': <class 'Policies.UCBrandomInit.UCBrandomInit'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][37]' = {'archtype': <class 'Policies.UCBrandomInit.UCBrandomInit'>, 'params': {}} ...
- Adding policy #39 = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBcython.UCBcython'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][38]' = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBcython.UCBcython'>, 'params': {}} ...
- Adding policy #40 = {'archtype': <class 'Policies.UCBV.UCBV'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][39]' = {'archtype': <class 'Policies.UCBV.UCBV'>, 'params': {}} ...
- Adding policy #41 = {'archtype': <class 'Policies.UCBVtuned.UCBVtuned'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][40]' = {'archtype': <class 'Policies.UCBVtuned.UCBVtuned'>, 'params': {}} ...
- Adding policy #42 = {'archtype': <class 'Policies.SlidingWindowUCB.SWUCB'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][41]' = {'archtype': <class 'Policies.SlidingWindowUCB.SWUCB'>, 'params': {}} ...
- Adding policy #43 = {'archtype': <class 'Policies.SlidingWindowUCB.SWUCBPlus'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][42]' = {'archtype': <class 'Policies.SlidingWindowUCB.SWUCBPlus'>, 'params': {}} ...
- Adding policy #44 = {'archtype': <class 'Policies.DiscountedUCB.DiscountedUCB'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][43]' = {'archtype': <class 'Policies.DiscountedUCB.DiscountedUCB'>, 'params': {}} ...
Warning: using DiscountedUCB with 'gamma' too close to 1 will result in UCBalpha, you should rather use it...
- Adding policy #45 = {'archtype': <class 'Policies.DiscountedUCB.DiscountedUCBPlus'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][44]' = {'archtype': <class 'Policies.DiscountedUCB.DiscountedUCBPlus'>, 'params': {}} ...
Warning: using DiscountedUCB with 'gamma' too close to 1 will result in UCBalpha, you should rather use it...
- Adding policy #46 = {'archtype': <class 'Policies.SlidingWindowRestart.SWR_UCB'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][45]' = {'archtype': <class 'Policies.SlidingWindowRestart.SWR_UCB'>, 'params': {}} ...
- Adding policy #47 = {'archtype': <class 'Policies.SlidingWindowRestart.SWR_UCBalpha'>, 'params': {'alpha': 0.5}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][46]' = {'archtype': <class 'Policies.SlidingWindowRestart.SWR_UCBalpha'>, 'params': {'alpha': 0.5}} ...
- Adding policy #48 = {'archtype': <class 'Policies.SparseUCB.SparseUCB'>, 'params': {'alpha': 4, 'sparsity': 2}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][47]' = {'archtype': <class 'Policies.SparseUCB.SparseUCB'>, 'params': {'alpha': 4, 'sparsity': 2}} ...
- Adding policy #49 = {'archtype': <class 'Policies.SparseklUCB.SparseklUCB'>, 'params': {'sparsity': 2}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][48]' = {'archtype': <class 'Policies.SparseklUCB.SparseklUCB'>, 'params': {'sparsity': 2}} ...
- Adding policy #50 = {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][49]' = {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}} ...
- Adding policy #51 = {'archtype': <class 'Policies.MOSSH.MOSSH'>, 'params': {'horizon': 1000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][50]' = {'archtype': <class 'Policies.MOSSH.MOSSH'>, 'params': {'horizon': 1000}} ...
- Adding policy #52 = {'archtype': <class 'Policies.MOSSAnytime.MOSSAnytime'>, 'params': {'alpha': 1.35}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][51]' = {'archtype': <class 'Policies.MOSSAnytime.MOSSAnytime'>, 'params': {'alpha': 1.35}} ...
- Adding policy #53 = {'archtype': <class 'Policies.MOSSExperimental.MOSSExperimental'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][52]' = {'archtype': <class 'Policies.MOSSExperimental.MOSSExperimental'>, 'params': {}} ...
- Adding policy #54 = {'archtype': <class 'Policies.OCUCB.OCUCB'>, 'params': {'eta': 1.1, 'rho': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][53]' = {'archtype': <class 'Policies.OCUCB.OCUCB'>, 'params': {'eta': 1.1, 'rho': 1}} ...
- Adding policy #55 = {'archtype': <class 'Policies.CPUCB.CPUCB'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][54]' = {'archtype': <class 'Policies.CPUCB.CPUCB'>, 'params': {}} ...
- Adding policy #56 = {'archtype': <class 'Policies.DMED.DMEDPlus'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][55]' = {'archtype': <class 'Policies.DMED.DMEDPlus'>, 'params': {}} ...
- Adding policy #57 = {'archtype': <class 'Policies.DMED.DMED'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][56]' = {'archtype': <class 'Policies.DMED.DMED'>, 'params': {}} ...
- Adding policy #58 = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][57]' = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}} ...
- Adding policy #59 = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Gauss.Gauss'>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][58]' = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Gauss.Gauss'>}} ...
- Adding policy #60 = {'archtype': <class 'Policies.Experimentals.ThompsonRobust.ThompsonRobust'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][59]' = {'archtype': <class 'Policies.Experimentals.ThompsonRobust.ThompsonRobust'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}} ...
- Adding policy #61 = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][60]' = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
- Adding policy #62 = {'archtype': <class 'Policies.SlidingWindowRestart.SWR_klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][61]' = {'archtype': <class 'Policies.SlidingWindowRestart.SWR_klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
- Adding policy #63 = {'archtype': <class 'Policies.klUCBloglog.klUCBloglog'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][62]' = {'archtype': <class 'Policies.klUCBloglog.klUCBloglog'>, 'params': {'klucb': <built-in function klucbBern>}} ...
- Adding policy #64 = {'archtype': <class 'Policies.Experimentals.klUCBlog10.klUCBlog10'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][63]' = {'archtype': <class 'Policies.Experimentals.klUCBlog10.klUCBlog10'>, 'params': {'klucb': <built-in function klucbBern>}} ...
- Adding policy #65 = {'archtype': <class 'Policies.Experimentals.klUCBloglog10.klUCBloglog10'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][64]' = {'archtype': <class 'Policies.Experimentals.klUCBloglog10.klUCBloglog10'>, 'params': {'klucb': <built-in function klucbBern>}} ...
- Adding policy #66 = {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][65]' = {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}} ...
- Adding policy #67 = {'archtype': <class 'Policies.klUCBH.klUCBH'>, 'params': {'horizon': 1000, 'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][66]' = {'archtype': <class 'Policies.klUCBH.klUCBH'>, 'params': {'horizon': 1000, 'klucb': <built-in function klucbBern>}} ...
- Adding policy #68 = {'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>, 'params': {'horizon': 1000, 'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][67]' = {'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>, 'params': {'horizon': 1000, 'klucb': <built-in function klucbBern>}} ...
- Adding policy #69 = {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 1000, 'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][68]' = {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 1000, 'klucb': <built-in function klucbBern>}} ...
- Adding policy #70 = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][69]' = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}} ...
- Adding policy #71 = {'archtype': <class 'Policies.AdBandits.AdBandits'>, 'params': {'alpha': 0.5, 'horizon': 1000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][70]' = {'archtype': <class 'Policies.AdBandits.AdBandits'>, 'params': {'alpha': 0.5, 'horizon': 1000}} ...
- Adding policy #72 = {'archtype': <class 'Policies.ApproximatedFHGittins.ApproximatedFHGittins'>, 'params': {'alpha': 0.5, 'horizon': 1100}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][71]' = {'archtype': <class 'Policies.ApproximatedFHGittins.ApproximatedFHGittins'>, 'params': {'alpha': 0.5, 'horizon': 1100}} ...
- Adding policy #73 = {'archtype': <class 'Policies.Experimentals.UnsupervisedLearning.UnsupervisedLearning'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][72]' = {'archtype': <class 'Policies.Experimentals.UnsupervisedLearning.UnsupervisedLearning'>, 'params': {}} ...
- Adding policy #74 = {'archtype': <class 'Policies.OSSB.OSSB'>, 'params': {'epsilon': 0.01, 'gamma': 0.0}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][73]' = {'archtype': <class 'Policies.OSSB.OSSB'>, 'params': {'epsilon': 0.01, 'gamma': 0.0}} ...
- Adding policy #75 = {'archtype': <class 'Policies.BESA.BESA'>, 'params': {'horizon': 1000, 'minPullsOfEachArm': 1, 'randomized_tournament': True, 'random_subsample': True, 'non_binary': False, 'non_recursive': False}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][74]' = {'archtype': <class 'Policies.BESA.BESA'>, 'params': {'horizon': 1000, 'minPullsOfEachArm': 1, 'randomized_tournament': True, 'random_subsample': True, 'non_binary': False, 'non_recursive': False}} ...
- Adding policy #76 = {'archtype': <class 'Policies.UCBdagger.UCBdagger'>, 'params': {'horizon': 1000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][75]' = {'archtype': <class 'Policies.UCBdagger.UCBdagger'>, 'params': {'horizon': 1000}} ...
- Adding policy #77 = {'archtype': <class 'Policies.UCBoost.UCB_bq'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][76]' = {'archtype': <class 'Policies.UCBoost.UCB_bq'>, 'params': {}} ...
- Adding policy #78 = {'archtype': <class 'Policies.UCBoost.UCB_h'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][77]' = {'archtype': <class 'Policies.UCBoost.UCB_h'>, 'params': {}} ...
- Adding policy #79 = {'archtype': <class 'Policies.UCBoost.UCB_lb'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][78]' = {'archtype': <class 'Policies.UCBoost.UCB_lb'>, 'params': {}} ...
- Adding policy #80 = {'archtype': <class 'Policies.UCBoost.UCBoost_bq_h_lb'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][79]' = {'archtype': <class 'Policies.UCBoost.UCBoost_bq_h_lb'>, 'params': {}} ...
- Adding policy #81 = {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][80]' = {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.1}} ...
- Adding policy #82 = {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.05}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][81]' = {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.05}} ...
- Adding policy #83 = {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.01}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][82]' = {'archtype': <class 'Policies.UCBoost.UCBoostEpsilon'>, 'params': {'epsilon': 0.01}} ...
- Adding policy #84 = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCB_bq'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][83]' = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCB_bq'>, 'params': {}} ...
- Adding policy #85 = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCB_h'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][84]' = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCB_h'>, 'params': {}} ...
- Adding policy #86 = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCB_lb'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][85]' = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCB_lb'>, 'params': {}} ...
- Adding policy #87 = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCBoost_bq_h_lb'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][86]' = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCBoost_bq_h_lb'>, 'params': {}} ...
- Adding policy #88 = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][87]' = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.1}} ...
- Adding policy #89 = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.05}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][88]' = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.05}} ...
- Adding policy #90 = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.01}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][89]' = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBoost_cython.UCBoostEpsilon'>, 'params': {'epsilon': 0.01}} ...
- Adding policy #91 = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBcython.UCBcython'>, 'params': {'alpha': 4.0}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][90]' = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBcython.UCBcython'>, 'params': {'alpha': 4.0}} ...
- Adding policy #92 = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBcython.UCBcython'>, 'params': {'alpha': 1.0}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][91]' = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBcython.UCBcython'>, 'params': {'alpha': 1.0}} ...
- Adding policy #93 = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBcython.UCBcython'>, 'params': {'alpha': 0.5}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][92]' = {'archtype': <class 'SMPyBandits.Policies.Experimentals.UCBcython.UCBcython'>, 'params': {'alpha': 0.5}} ...
- Adding policy #94 = {'archtype': <class 'Policies.Exp3PlusPlus.Exp3PlusPlus'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][93]' = {'archtype': <class 'Policies.Exp3PlusPlus.Exp3PlusPlus'>, 'params': {}} ...
- Adding policy #95 = {'archtype': <class 'Policies.DiscountedThompson.DiscountedThompson'>, 'params': {'posterior': <class 'Policies.Posterior.DiscountedBeta.DiscountedBeta'>, 'gamma': 0.99}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][94]' = {'archtype': <class 'Policies.DiscountedThompson.DiscountedThompson'>, 'params': {'posterior': <class 'Policies.Posterior.DiscountedBeta.DiscountedBeta'>, 'gamma': 0.99}} ...
- Adding policy #96 = {'archtype': <class 'Policies.Exp3R.Exp3R'>, 'params': {'horizon': 1000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][95]' = {'archtype': <class 'Policies.Exp3R.Exp3R'>, 'params': {'horizon': 1000}} ...
Warning: the policy Exp3R($T=1000$, $c=8.77$, $\alpha=0.01$) tried to use default value of gamma = 0.3695957346328987 but could not set attribute self.policy.gamma to gamma (maybe it's using an Exp3 with a non-constant value of gamma).
- Adding policy #97 = {'archtype': <class 'Policies.Exp3R.Exp3RPlusPlus'>, 'params': {'horizon': 1000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][96]' = {'archtype': <class 'Policies.Exp3R.Exp3RPlusPlus'>, 'params': {'horizon': 1000}} ...
Warning: the policy Exp3R++($T=1000$, $c=1.76$, $\alpha=0.247$) tried to use default value of gamma = 0.3695957346328987 but could not set attribute self.policy.gamma to gamma (maybe it's using an Exp3 with a non-constant value of gamma).
- Adding policy #98 = {'archtype': <class 'Policies.AdSwitch.AdSwitch'>, 'params': {'horizon': 1000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][97]' = {'archtype': <class 'Policies.AdSwitch.AdSwitch'>, 'params': {'horizon': 1000}} ...
WARNING: so far, for the AdSwitch algorithm, only the special case of K=2 arms was explained in the paper, but I generalized it. Maybe it does not work!
- Adding policy #99 = {'archtype': <class 'Policies.LM_DSEE.LM_DSEE'>, 'params': {'nu': 0.25, 'DeltaMin': 0.1, 'a': 1, 'b': 0.25}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][98]' = {'archtype': <class 'Policies.LM_DSEE.LM_DSEE'>, 'params': {'nu': 0.25, 'DeltaMin': 0.1, 'a': 1, 'b': 0.25}} ...
- Adding policy #100 = {'archtype': <class 'Policies.CD_UCB.CUSUM_IndexPolicy'>, 'params': {'horizon': 1000, 'max_nb_random_events': 1, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][99]' = {'archtype': <class 'Policies.CD_UCB.CUSUM_IndexPolicy'>, 'params': {'horizon': 1000, 'max_nb_random_events': 1, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True}} ...
- Adding policy #101 = {'archtype': <class 'Policies.CD_UCB.PHT_IndexPolicy'>, 'params': {'horizon': 1000, 'max_nb_random_events': 1, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][100]' = {'archtype': <class 'Policies.CD_UCB.PHT_IndexPolicy'>, 'params': {'horizon': 1000, 'max_nb_random_events': 1, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True}} ...
- Adding policy #102 = {'archtype': <class 'Policies.SWHash_UCB.SWHash_IndexPolicy'>, 'params': {'alpha': 1, 'lmbda': 1, 'policy': <class 'Policies.UCB.UCB'>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][101]' = {'archtype': <class 'Policies.SWHash_UCB.SWHash_IndexPolicy'>, 'params': {'alpha': 1, 'lmbda': 1, 'policy': <class 'Policies.UCB.UCB'>}} ...
- Adding policy #103 = {'archtype': <class 'Policies.SlidingWindowRestart.SlidingWindowRestart'>, 'params': {'policy': <class 'Policies.UCB.UCB'>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][102]' = {'archtype': <class 'Policies.SlidingWindowRestart.SlidingWindowRestart'>, 'params': {'policy': <class 'Policies.UCB.UCB'>}} ...
- Adding policy #104 = {'archtype': <class 'Policies.SlidingWindowUCB.SWUCB'>, 'params': {'alpha': 1, 'tau': 500}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][103]' = {'archtype': <class 'Policies.SlidingWindowUCB.SWUCB'>, 'params': {'alpha': 1, 'tau': 500}} ...
- Adding policy #105 = {'archtype': <class 'Policies.SlidingWindowUCB.SWUCBPlus'>, 'params': {'horizon': 1000, 'alpha': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][104]' = {'archtype': <class 'Policies.SlidingWindowUCB.SWUCBPlus'>, 'params': {'horizon': 1000, 'alpha': 1}} ...
- Adding policy #106 = {'archtype': <class 'Policies.DiscountedUCB.DiscountedUCB'>, 'params': {'alpha': 1, 'gamma': 0.9}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][105]' = {'archtype': <class 'Policies.DiscountedUCB.DiscountedUCB'>, 'params': {'alpha': 1, 'gamma': 0.9}} ...
- Adding policy #107 = {'archtype': <class 'Policies.DiscountedUCB.DiscountedUCBPlus'>, 'params': {'max_nb_random_events': 1, 'alpha': 1, 'horizon': 1000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][106]' = {'archtype': <class 'Policies.DiscountedUCB.DiscountedUCBPlus'>, 'params': {'max_nb_random_events': 1, 'alpha': 1, 'horizon': 1000}} ...
- Adding policy #108 = {'archtype': <class 'Policies.Monitored_UCB.Monitored_IndexPolicy'>, 'params': {'horizon': 1000, 'max_nb_random_events': 1, 'delta': 0.1, 'policy': <class 'Policies.UCB.UCB'>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][107]' = {'archtype': <class 'Policies.Monitored_UCB.Monitored_IndexPolicy'>, 'params': {'horizon': 1000, 'max_nb_random_events': 1, 'delta': 0.1, 'policy': <class 'Policies.UCB.UCB'>}} ...
Warning: the formula for w in the paper gave w = 18738, that's crazy large, we use instead 450
Warning: the formula for gamma in the paper gave gamma = 0.0, that's absurd, we use instead 0.0022172949002217295
- Adding policy #109 = {'archtype': <class 'Policies.Monitored_UCB.Monitored_IndexPolicy'>, 'params': {'policy': <class 'Policies.UCB.UCB'>, 'horizon': 1000, 'w': 80, 'b': 25.85024743855303}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][108]' = {'archtype': <class 'Policies.Monitored_UCB.Monitored_IndexPolicy'>, 'params': {'policy': <class 'Policies.UCB.UCB'>, 'horizon': 1000, 'w': 80, 'b': 25.85024743855303}} ...
Warning: the formula for gamma in the paper gave gamma = 0.0, that's absurd, we use instead 0.0022172949002217295
- Adding policy #110 = {'archtype': <class 'Policies.OracleSequentiallyRestartPolicy.OracleSequentiallyRestartPolicy'>, 'params': {'changePoints': [], 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][109]' = {'archtype': <class 'Policies.OracleSequentiallyRestartPolicy.OracleSequentiallyRestartPolicy'>, 'params': {'changePoints': [], 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True}} ...
WARNING: it is useless to use the wrapper OracleSequentiallyRestartPolicy when changePoints = [] is empty, just use the base policy without the wrapper!
Info: creating a new policy OracleRestart-UCB(Per-Arm), with change points = [[], [], [], [], [], [], [], [], []]...
- Adding policy #111 = {'archtype': <class 'Policies.CD_UCB.BernoulliGLR_IndexPolicy'>, 'params': {'horizon': 1000, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True, 'max_nb_random_events': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][110]' = {'archtype': <class 'Policies.CD_UCB.BernoulliGLR_IndexPolicy'>, 'params': {'horizon': 1000, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True, 'max_nb_random_events': 1}} ...
DEBUG: smart_alpha_from_T_UpsilonT: horizon = 1000, max_nb_random_events = 1, gives alpha = 0.04155645340672775...
- Adding policy #112 = {'archtype': <class 'Policies.CD_UCB.BernoulliGLR_IndexPolicy_WithTracking'>, 'params': {'horizon': 1000, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True, 'max_nb_random_events': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][111]' = {'archtype': <class 'Policies.CD_UCB.BernoulliGLR_IndexPolicy_WithTracking'>, 'params': {'horizon': 1000, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True, 'max_nb_random_events': 1}} ...
DEBUG: smart_alpha_from_T_UpsilonT: horizon = 1000, max_nb_random_events = 1, gives alpha = 0.04155645340672775...
- Adding policy #113 = {'archtype': <class 'Policies.CD_UCB.GaussianGLR_IndexPolicy'>, 'params': {'horizon': 1000, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True, 'max_nb_random_events': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][112]' = {'archtype': <class 'Policies.CD_UCB.GaussianGLR_IndexPolicy'>, 'params': {'horizon': 1000, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True, 'max_nb_random_events': 1}} ...
DEBUG: smart_alpha_from_T_UpsilonT: horizon = 1000, max_nb_random_events = 1, gives alpha = 0.04155645340672775...
- Adding policy #114 = {'archtype': <class 'Policies.CD_UCB.GaussianGLR_IndexPolicy_WithTracking'>, 'params': {'horizon': 1000, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True, 'max_nb_random_events': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][113]' = {'archtype': <class 'Policies.CD_UCB.GaussianGLR_IndexPolicy_WithTracking'>, 'params': {'horizon': 1000, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True, 'max_nb_random_events': 1}} ...
DEBUG: smart_alpha_from_T_UpsilonT: horizon = 1000, max_nb_random_events = 1, gives alpha = 0.04155645340672775...
- Adding policy #115 = {'archtype': <class 'Policies.CD_UCB.SubGaussianGLR_IndexPolicy'>, 'params': {'horizon': 1000, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True, 'max_nb_random_events': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][114]' = {'archtype': <class 'Policies.CD_UCB.SubGaussianGLR_IndexPolicy'>, 'params': {'horizon': 1000, 'policy': <class 'Policies.UCB.UCB'>, 'per_arm_restart': True, 'max_nb_random_events': 1}} ...
DEBUG: smart_alpha_from_T_UpsilonT: horizon = 1000, max_nb_random_events = 1, gives alpha = 0.04155645340672775...



- Evaluating policy #1/115: U(1..9) ...

Estimated order by the policy U(1..9) after 1000 steps: [8 5 4 7 2 1 0 3 6] ...
  ==> Optimal arm identification: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 11.11% (relative success)...



- Evaluating policy #2/115: UniformOnSome([0, 1]) ...

Estimated order by the policy UniformOnSome([0, 1]) after 1000 steps: [6 3 1 7 4 8 0 5 2] ...
  ==> Optimal arm identification: 33.33% (relative success)...
  ==> Mean distance from optimal ordering: 29.63% (relative success)...



- Evaluating policy #3/115: EmpiricalMeans ...

Estimated order by the policy EmpiricalMeans after 1000 steps: [2 3 4 5 0 1 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 69.14% (relative success)...



- Evaluating policy #4/115: TakeRandomFixedArm([2, 3, 6, 1]) ...

Estimated order by the policy TakeRandomFixedArm([2, 3, 6, 1]) after 1000 steps: [4 2 1 6 7 3 8 5 0] ...
  ==> Optimal arm identification: 11.11% (relative success)...
  ==> Mean distance from optimal ordering: 40.12% (relative success)...



- Evaluating policy #5/115: UniformOnSome([0, 1]) ...

Estimated order by the policy UniformOnSome([0, 1]) after 1000 steps: [5 2 6 3 8 4 1 0 7] ...
  ==> Optimal arm identification: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 32.10% (relative success)...



- Evaluating policy #6/115: TakeFixedArm(1) ...

Estimated order by the policy TakeFixedArm(1) after 1000 steps: [8 0 7 6 1 5 2 4 3] ...
  ==> Optimal arm identification: 44.44% (relative success)...
  ==> Mean distance from optimal ordering: 16.05% (relative success)...



- Evaluating policy #7/115: TakeFixedArm(0) ...

Estimated order by the policy TakeFixedArm(0) after 1000 steps: [7 3 5 2 1 0 8 4 6] ...
  ==> Optimal arm identification: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 26.54% (relative success)...



- Evaluating policy #8/115: TakeFixedArm(0) ...

Estimated order by the policy TakeFixedArm(0) after 1000 steps: [2 1 7 3 8 0 5 6 4] ...
  ==> Optimal arm identification: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 45.06% (relative success)...



- Evaluating policy #9/115: TakeFixedArm(1) ...

Estimated order by the policy TakeFixedArm(1) after 1000 steps: [7 0 5 8 4 6 2 3 1] ...
  ==> Optimal arm identification: 22.22% (relative success)...
  ==> Mean distance from optimal ordering: 27.16% (relative success)...



- Evaluating policy #10/115: EpsilonGreedy(0.1) ...

Estimated order by the policy EpsilonGreedy(0.1) after 1000 steps: [7 1 5 3 8 0 6 2 4] ...
  ==> Optimal arm identification: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 26.54% (relative success)...



- Evaluating policy #11/115: EpsilonDecreasing(e:0.1) ...

Estimated order by the policy EpsilonDecreasing(e:0.1) after 1000 steps: [7 2 5 8 6 3 0 1 4] ...
  ==> Optimal arm identification: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 22.22% (relative success)...



- Evaluating policy #12/115: EpsilonExpDecreasing(e:0.1, r:0.005) ...

Estimated order by the policy EpsilonExpDecreasing(e:0.1, r:0.005) after 1000 steps: [1 3 6 2 8 5 4 0 7] ...
  ==> Optimal arm identification: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 45.06% (relative success)...



- Evaluating policy #13/115: EpsilonFirst($T=1000$, $\varepsilon=0.1$) ...

Estimated order by the policy EpsilonFirst($T=1000$, $\varepsilon=0.1$) after 1000 steps: [5 7 0 8 4 2 1 3 6] ...
  ==> Optimal arm identification: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 27.16% (relative success)...



- Evaluating policy #14/115: ETC_KnownGap($T=1000$, $\Delta=0.05$, $T_0=1602$) ...

Estimated order by the policy ETC_KnownGap($T=1000$, $\Delta=0.05$, $T_0=1602$) after 1000 steps: [6 8 7 5 1 4 2 3 0] ...
  ==> Optimal arm identification: 11.11% (relative success)...
  ==> Mean distance from optimal ordering: 17.28% (relative success)...



- Evaluating policy #15/115: ETC_RandomStop($T=1000$) ...

Estimated order by the policy ETC_RandomStop($T=1000$) after 1000 steps: [1 2 7 3 5 0 6 4 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 52.47% (relative success)...



- Evaluating policy #16/115: Softmax(temp: 0.05) ...

Estimated order by the policy Softmax(temp: 0.05) after 1000 steps: [0 1 2 5 6 7 4 3 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 74.07% (relative success)...



- Evaluating policy #17/115: Softmax(decreasing) ...

Estimated order by the policy Softmax(decreasing) after 1000 steps: [0 1 2 5 3 7 4 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 73.46% (relative success)...



- Evaluating policy #18/115: SoftMix ...

Estimated order by the policy SoftMix after 1000 steps: [0 4 7 3 6 1 2 5 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 53.09% (relative success)...



- Evaluating policy #19/115: Softmax($T=1000$) ...

Estimated order by the policy Softmax($T=1000$) after 1000 steps: [0 5 8 1 2 3 4 6 7] ...
  ==> Optimal arm identification: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 64.20% (relative success)...



- Evaluating policy #20/115: BoltzmannGumbel($\alpha=0.5$) ...

Estimated order by the policy BoltzmannGumbel($\alpha=0.5$) after 1000 steps: [1 0 3 2 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 83.95% (relative success)...



- Evaluating policy #21/115: Exp3($\gamma: 0.001$) ...

Estimated order by the policy Exp3($\gamma: 0.001$) after 1000 steps: [0 1 2 3 5 4 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 91.98% (relative success)...



- Evaluating policy #22/115: Exp3(decreasing) ...

Estimated order by the policy Exp3(decreasing) after 1000 steps: [0 1 2 3 4 5 7 8 6] ...
  ==> Optimal arm identification: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 89.51% (relative success)...



- Evaluating policy #23/115: Exp3(SoftMix) ...

Estimated order by the policy Exp3(SoftMix) after 1000 steps: [0 1 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #24/115: Exp3($T=1000$) ...

Estimated order by the policy Exp3($T=1000$) after 1000 steps: [0 1 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #25/115: Exp3ELM($\delta=0.1$) ...

Estimated order by the policy Exp3ELM($\delta=0.1$) after 1000 steps: [0 1 2 4 3 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 91.98% (relative success)...



- Evaluating policy #26/115: Exp3++ ...

Estimated order by the policy Exp3++ after 1000 steps: [0 1 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #27/115: Pursuit(0.5) ...

Estimated order by the policy Pursuit(0.5) after 1000 steps: [2 8 6 7 5 1 0 3 4] ...
  ==> Optimal arm identification: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 22.22% (relative success)...



- Evaluating policy #28/115: Hedge($\varepsilon: 0.5$) ...

Estimated order by the policy Hedge($\varepsilon: 0.5$) after 1000 steps: [1 0 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 91.98% (relative success)...



- Evaluating policy #29/115: Hedge(decreasing) ...

Estimated order by the policy Hedge(decreasing) after 1000 steps: [0 1 2 4 3 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 91.98% (relative success)...



- Evaluating policy #30/115: Hedge($T=1000$) ...

Estimated order by the policy Hedge($T=1000$) after 1000 steps: [1 0 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 91.98% (relative success)...



- Evaluating policy #31/115: UCB ...

Estimated order by the policy UCB after 1000 steps: [1 5 2 0 4 6 3 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 62.96% (relative success)...



- Evaluating policy #32/115: UCBlog10 ...

Estimated order by the policy UCBlog10 after 1000 steps: [2 5 3 4 1 0 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 63.58% (relative success)...



- Evaluating policy #33/115: UCBwrong ...

Estimated order by the policy UCBwrong after 1000 steps: [7 6 5 4 3 1 2 0 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 27.16% (relative success)...



- Evaluating policy #34/115: UCB($\alpha=1$) ...

Estimated order by the policy UCB($\alpha=1$) after 1000 steps: [1 4 6 0 2 3 5 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 63.58% (relative success)...



- Evaluating policy #35/115: UCB($\alpha=1$, $\log_{10}$) ...

Estimated order by the policy UCB($\alpha=1$, $\log_{10}$) after 1000 steps: [1 3 4 0 2 6 5 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 68.52% (relative success)...



- Evaluating policy #36/115: UCBmin ...

Estimated order by the policy UCBmin after 1000 steps: [2 3 4 6 0 1 5 8 7] ...
  ==> Optimal arm identification: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 53.09% (relative success)...



- Evaluating policy #37/115: UCBplus ...

Estimated order by the policy UCBplus after 1000 steps: [1 2 0 4 6 3 5 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 70.99% (relative success)...



- Evaluating policy #38/115: UCBrandomInit ...

Estimated order by the policy UCBrandomInit after 1000 steps: [5 3 1 2 6 7 4 0 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 50.62% (relative success)...



- Evaluating policy #39/115: UCBcython($\alpha=4$) ...

Estimated order by the policy UCBcython($\alpha=4$) after 1000 steps: [4 0 1 5 3 2 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 60.49% (relative success)...



- Evaluating policy #40/115: UCBV ...

Estimated order by the policy UCBV after 1000 steps: [5 6 3 0 2 7 4 1 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 40.12% (relative success)...



- Evaluating policy #41/115: UCBVtuned ...

Estimated order by the policy UCBVtuned after 1000 steps: [2 0 3 5 1 4 7 8 6] ...
  ==> Optimal arm identification: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 60.49% (relative success)...



- Evaluating policy #42/115: SW-UCB($\tau=1000$, $\alpha=0.6$) ...

Estimated order by the policy SW-UCB($\tau=1000$, $\alpha=0.6$) after 1000 steps: [3 0 1 2 4 5 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 79.01% (relative success)...



- Evaluating policy #43/115: SW-UCB+($\tau=1000$, $\alpha=0.6$) ...

Estimated order by the policy SW-UCB+($\tau=1000$, $\alpha=0.6$) after 1000 steps: [0 2 1 3 5 4 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 83.95% (relative success)...



- Evaluating policy #44/115: D-UCB($\alpha=4$, $\gamma=1$) ...

Estimated order by the policy D-UCB($\alpha=4$, $\gamma=1$) after 1000 steps: [0 1 2 6 3 5 8 4 7] ...
  ==> Optimal arm identification: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 65.43% (relative success)...



- Evaluating policy #45/115: D-UCB+($\alpha=4$, $\gamma=1$) ...

Estimated order by the policy D-UCB+($\alpha=4$, $\gamma=1$) after 1000 steps: [0 2 5 1 6 7 8 3 4] ...
  ==> Optimal arm identification: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 58.64% (relative success)...



- Evaluating policy #46/115: SW-Restart(SWR_UCB, $\tau=100$, $\varepsilon=0.005$) ...

Estimated order by the policy SW-Restart(SWR_UCB, $\tau=100$, $\varepsilon=0.005$) after 1000 steps: [0 4 2 5 6 8 7 1 3] ...
  ==> Optimal arm identification: 44.44% (relative success)...
  ==> Mean distance from optimal ordering: 50.62% (relative success)...



- Evaluating policy #47/115: SW-Restart(UCB($\alpha=0.5$), $\tau=100$, $\varepsilon=0.005$) ...

Estimated order by the policy SW-Restart(UCB($\alpha=0.5$), $\tau=100$, $\varepsilon=0.005$) after 1000 steps: [0 2 3 1 5 4 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 73.46% (relative success)...



- Evaluating policy #48/115: SparseUCB($s=2$, $\alpha=4$) ...

Estimated order by the policy SparseUCB($s=2$, $\alpha=4$) after 1000 steps: [0 7 8 1 2 4 3 5 6] ...
  ==> Optimal arm identification: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 37.04% (relative success)...



- Evaluating policy #49/115: Sparse-kl-UCB($s=2$, Bern) ...

Estimated order by the policy Sparse-kl-UCB($s=2$, Bern) after 1000 steps: [4 6 5 0 1 2 3 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 53.70% (relative success)...



- Evaluating policy #50/115: MOSS ...

Estimated order by the policy MOSS after 1000 steps: [0 1 3 2 5 6 4 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 81.48% (relative success)...



- Evaluating policy #51/115: MOSS-H($T=1000$) ...

Estimated order by the policy MOSS-H($T=1000$) after 1000 steps: [0 1 2 6 5 7 3 4 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 68.52% (relative success)...



- Evaluating policy #52/115: MOSS-Anytime($\alpha=1.35$) ...

Estimated order by the policy MOSS-Anytime($\alpha=1.35$) after 1000 steps: [0 2 1 4 5 3 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 73.46% (relative success)...



- Evaluating policy #53/115: MOSS-Experimental ...

Estimated order by the policy MOSS-Experimental after 1000 steps: [3 0 2 5 4 1 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 65.43% (relative success)...



- Evaluating policy #54/115: OC-UCB($\eta=1.1$, $\rho=1$) ...

Estimated order by the policy OC-UCB($\eta=1.1$, $\rho=1$) after 1000 steps: [0 2 3 5 6 7 4 1 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 69.14% (relative success)...



- Evaluating policy #55/115: CPUCB ...

Estimated order by the policy CPUCB after 1000 steps: [0 2 1 3 6 5 8 4 7] ...
  ==> Optimal arm identification: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 65.43% (relative success)...



- Evaluating policy #56/115: DMED$^+$(Bern) ...

Estimated order by the policy DMED$^+$(Bern) after 1000 steps: [5 1 0 3 4 7 8 6 2] ...
  ==> Optimal arm identification: 33.33% (relative success)...
  ==> Mean distance from optimal ordering: 55.56% (relative success)...



- Evaluating policy #57/115: DMED(Bern) ...

Estimated order by the policy DMED(Bern) after 1000 steps: [8 6 2 7 0 4 3 5 1] ...
  ==> Optimal arm identification: 22.22% (relative success)...
  ==> Mean distance from optimal ordering: 13.58% (relative success)...



- Evaluating policy #58/115: Thompson ...

Estimated order by the policy Thompson after 1000 steps: [1 3 4 0 6 5 7 2 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 63.58% (relative success)...



- Evaluating policy #59/115: Thompson(Gauss) ...

Estimated order by the policy Thompson(Gauss) after 1000 steps: [0 2 5 6 1 4 7 3 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 63.58% (relative success)...



- Evaluating policy #60/115: ThompsonRobust(averageOn = 10) ...

Estimated order by the policy ThompsonRobust(averageOn = 10) after 1000 steps: [1 4 2 0 7 3 6 5 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 54.94% (relative success)...



- Evaluating policy #61/115: kl-UCB ...

Estimated order by the policy kl-UCB after 1000 steps: [0 3 4 5 2 7 6 1 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 63.58% (relative success)...



- Evaluating policy #62/115: SW-Restart(kl-UCB, $\tau=100$, $\varepsilon=0.005$) ...

Estimated order by the policy SW-Restart(kl-UCB, $\tau=100$, $\varepsilon=0.005$) after 1000 steps: [0 1 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #63/115: kl-UCB($\log\log$, Bern) ...

Estimated order by the policy kl-UCB($\log\log$, Bern) after 1000 steps: [0 2 5 3 4 1 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 70.99% (relative success)...



- Evaluating policy #64/115: kl-UCB($\log_{10}$, Bern) ...

Estimated order by the policy kl-UCB($\log_{10}$, Bern) after 1000 steps: [0 1 2 6 3 4 5 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 87.04% (relative success)...



- Evaluating policy #65/115: kl-UCB($\log_{10}\log_{10}$, Bern) ...

Estimated order by the policy kl-UCB($\log_{10}\log_{10}$, Bern) after 1000 steps: [0 1 3 2 4 6 7 5 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 81.48% (relative success)...



- Evaluating policy #66/115: kl-UCB$^+$ ...

Estimated order by the policy kl-UCB$^+$ after 1000 steps: [0 1 3 4 2 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 89.51% (relative success)...



- Evaluating policy #67/115: kl-UCB-H($T=1000$, Bern) ...

Estimated order by the policy kl-UCB-H($T=1000$, Bern) after 1000 steps: [0 2 1 3 5 4 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 83.95% (relative success)...



- Evaluating policy #68/115: kl-UCB-H+($T=1000$, Bern) ...

Estimated order by the policy kl-UCB-H+($T=1000$, Bern) after 1000 steps: [1 2 3 0 6 5 4 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 70.99% (relative success)...



- Evaluating policy #69/115: kl-UCB$^{++}$($T=1000$) ...

Estimated order by the policy kl-UCB$^{++}$($T=1000$) after 1000 steps: [2 0 1 4 3 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 81.48% (relative success)...



- Evaluating policy #70/115: BayesUCB ...

Estimated order by the policy BayesUCB after 1000 steps: [5 0 1 2 3 6 4 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 74.07% (relative success)...



- Evaluating policy #71/115: AdBandits($T=1000$, $\alpha=0.5$) ...

Estimated order by the policy AdBandits($T=1000$, $\alpha=0.5$) after 1000 steps: [1 4 5 0 8 2 3 7 6] ...
  ==> Optimal arm identification: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 45.06% (relative success)...



- Evaluating policy #72/115: ApprFHG($T=1100$) ...

Estimated order by the policy ApprFHG($T=1100$) after 1000 steps: [1 0 4 2 3 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 81.48% (relative success)...



- Evaluating policy #73/115: UnsupervisedLearning(SimpleGaussianKernel, :math:`T_0=100`, :math:`T_1=1000`, :math:`M=100`) ...

Estimated order by the policy UnsupervisedLearning(SimpleGaussianKernel, :math:`T_0=100`, :math:`T_1=1000`, :math:`M=100`) after 1000 steps: [0 2 3 1 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 89.51% (relative success)...



- Evaluating policy #74/115: OSSB($\varepsilon=0.01$, $\gamma=0$, Bern) ...

Estimated order by the policy OSSB($\varepsilon=0.01$, $\gamma=0$, Bern) after 1000 steps: [0 6 7 5 8 4 2 1 3] ...
  ==> Optimal arm identification: 44.44% (relative success)...
  ==> Mean distance from optimal ordering: 32.72% (relative success)...



- Evaluating policy #75/115: BESA ...

Estimated order by the policy BESA after 1000 steps: [0 2 1 3 6 5 4 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 75.93% (relative success)...



- Evaluating policy #76/115: UCB$\dagger$($T=1000$) ...

Estimated order by the policy UCB$\dagger$($T=1000$) after 1000 steps: [0 1 4 5 3 7 6 2 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 68.52% (relative success)...



- Evaluating policy #77/115: $\mathrm{UCB}_{d=d_{bq}}$($c=0$) ...

Estimated order by the policy $\mathrm{UCB}_{d=d_{bq}}$($c=0$) after 1000 steps: [3 5 4 6 7 0 1 2 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 35.19% (relative success)...



- Evaluating policy #78/115: $\mathrm{UCB}_{d=d_h}$($c=0$) ...

Estimated order by the policy $\mathrm{UCB}_{d=d_h}$($c=0$) after 1000 steps: [0 1 3 4 7 2 5 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 76.54% (relative success)...



- Evaluating policy #79/115: $\mathrm{UCB}_{d=d_{lb}}$($c=0$) ...

Estimated order by the policy $\mathrm{UCB}_{d=d_{lb}}$($c=0$) after 1000 steps: [3 4 0 2 6 5 1 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 58.02% (relative success)...



- Evaluating policy #80/115: UCBoost($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$) ...

Estimated order by the policy UCBoost($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$) after 1000 steps: [0 1 4 2 7 3 6 5 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 65.43% (relative success)...



- Evaluating policy #81/115: UCBoost($\varepsilon=0.1$, $c=0$) ...

Estimated order by the policy UCBoost($\varepsilon=0.1$, $c=0$) after 1000 steps: [0 3 4 7 1 5 2 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 58.02% (relative success)...



- Evaluating policy #82/115: UCBoost($\varepsilon=0.05$, $c=0$) ...

Estimated order by the policy UCBoost($\varepsilon=0.05$, $c=0$) after 1000 steps: [0 3 5 1 2 4 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 76.54% (relative success)...



- Evaluating policy #83/115: UCBoost($\varepsilon=0.01$, $c=0$) ...

Estimated order by the policy UCBoost($\varepsilon=0.01$, $c=0$) after 1000 steps: [0 1 5 3 7 4 2 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 62.96% (relative success)...



- Evaluating policy #84/115: $\mathrm{UCBcython}_{d=d_{bq}}$($c=0$) ...

Estimated order by the policy $\mathrm{UCBcython}_{d=d_{bq}}$($c=0$) after 1000 steps: [0 1 4 6 2 3 5 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 76.54% (relative success)...



- Evaluating policy #85/115: $\mathrm{UCBcython}_{d=d_h}$($c=0$) ...

Estimated order by the policy $\mathrm{UCBcython}_{d=d_h}$($c=0$) after 1000 steps: [0 1 3 5 6 7 2 4 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 71.60% (relative success)...



- Evaluating policy #86/115: $\mathrm{UCBcython}_{d=d_{lb}}$($c=0$) ...

Estimated order by the policy $\mathrm{UCBcython}_{d=d_{lb}}$($c=0$) after 1000 steps: [1 2 3 7 0 4 5 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 71.60% (relative success)...



- Evaluating policy #87/115: UCBoostcython($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$) ...

Estimated order by the policy UCBoostcython($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$) after 1000 steps: [0 1 2 4 5 7 8 3 6] ...
  ==> Optimal arm identification: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 74.07% (relative success)...



- Evaluating policy #88/115: UCBoostcython($\varepsilon=0.1$, $c=0$) ...

Estimated order by the policy UCBoostcython($\varepsilon=0.1$, $c=0$) after 1000 steps: [2 0 1 3 5 4 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 81.48% (relative success)...



- Evaluating policy #89/115: UCBoostcython($\varepsilon=0.05$, $c=0$) ...

Estimated order by the policy UCBoostcython($\varepsilon=0.05$, $c=0$) after 1000 steps: [6 5 4 1 2 3 0 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 48.15% (relative success)...



- Evaluating policy #90/115: UCBoostcython($\varepsilon=0.01$, $c=0$) ...

Estimated order by the policy UCBoostcython($\varepsilon=0.01$, $c=0$) after 1000 steps: [3 7 6 4 5 0 1 2 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 35.19% (relative success)...



- Evaluating policy #91/115: UCBcython($\alpha=4$) ...

Estimated order by the policy UCBcython($\alpha=4$) after 1000 steps: [0 1 4 2 3 6 7 5 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 79.01% (relative success)...



- Evaluating policy #92/115: UCBcython($\alpha=1$) ...

Estimated order by the policy UCBcython($\alpha=1$) after 1000 steps: [0 4 1 6 2 7 3 5 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 58.02% (relative success)...



- Evaluating policy #93/115: UCBcython($\alpha=0.5$) ...

Estimated order by the policy UCBcython($\alpha=0.5$) after 1000 steps: [0 4 2 3 6 5 1 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 70.99% (relative success)...



- Evaluating policy #94/115: Exp3++ ...

Estimated order by the policy Exp3++ after 1000 steps: [0 1 4 2 5 3 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 81.48% (relative success)...



- Evaluating policy #95/115: DiscountedThompson($\gamma=0.99$) ...

Estimated order by the policy DiscountedThompson($\gamma=0.99$) after 1000 steps: [0 1 4 3 5 2 8 6 7] ...
  ==> Optimal arm identification: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 70.99% (relative success)...



- Evaluating policy #96/115: Exp3R($T=1000$, $c=8.77$, $\alpha=0.01$) ...

Estimated order by the policy Exp3R($T=1000$, $c=8.77$, $\alpha=0.01$) after 1000 steps: [0 1 2 3 4 6 5 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 91.98% (relative success)...



- Evaluating policy #97/115: Exp3R++($T=1000$, $c=1.76$, $\alpha=0.247$) ...

Estimated order by the policy Exp3R++($T=1000$, $c=9.92$, $\alpha=0.00781$) after 1000 steps: [0 1 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...



- Evaluating policy #98/115: AdSwitch($T=1000$, $C_1=1$, $C_2=1$) ...

Estimated order by the policy AdSwitch($T=1000$, $C_1=1$, $C_2=1$) after 1000 steps: [1 8 7 5 2 6 4 0 3] ...
  ==> Optimal arm identification: 44.44% (relative success)...
  ==> Mean distance from optimal ordering: 21.60% (relative success)...



- Evaluating policy #99/115: LM-DSEE($\gamma=200$, $\rho=0.6$, $\ell=1.48e+04$, $a=1$, $b=0.25$) ...

Estimated order by the policy LM-DSEE($\gamma=200$, $\rho=0.6$, $\ell=1.48e+04$, $a=1$, $b=0.25$) after 1000 steps: [8 0 2 6 7 1 3 4 5] ...
  ==> Optimal arm identification: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 43.21% (relative success)...



- Evaluating policy #100/115: CUSUM-UCB($\alpha=0.0416$, $M=50$, Per-Arm) ...
For a player CUSUM-UCB($\alpha=0.0416$, $M=50$, Per-Arm) a change was detected at time 468 for arm 8 after seeing reward = 0.0!
For a player CUSUM-UCB($\alpha=0.0416$, $M=50$, Per-Arm) a change was detected at time 498 for arm 8 after seeing reward = 0.0!
For a player CUSUM-UCB($\alpha=0.0416$, $M=50$, Per-Arm) a change was detected at time 896 for arm 8 after seeing reward = 1.0!
For a player CUSUM-UCB($\alpha=0.0416$, $M=50$, Per-Arm) a change was detected at time 962 for arm 8 after seeing reward = 0.0!

Estimated order by the policy CUSUM-UCB($\alpha=0.0416$, $M=50$, Per-Arm) after 1000 steps: [1 0 2 3 6 5 7 4 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 73.46% (relative success)...



- Evaluating policy #101/115: PHT-UCB($\alpha=0.0416$, $M=50$, Per-Arm) ...

Estimated order by the policy PHT-UCB($\alpha=0.0416$, $M=50$, Per-Arm) after 1000 steps: [4 0 1 2 5 3 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 76.54% (relative success)...



- Evaluating policy #102/115: SW-UCB#($\lambda=1$, $\alpha=1$) ...

Estimated order by the policy SW-UCB#($\lambda=1$, $\alpha=1$) after 1000 steps: [3 2 4 1 5 7 0 8 6] ...
  ==> Optimal arm identification: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 53.09% (relative success)...



- Evaluating policy #103/115: SW-Restart(UCB, $\tau=100$, $\varepsilon=0.005$) ...

Estimated order by the policy SW-Restart(UCB, $\tau=100$, $\varepsilon=0.005$) after 1000 steps: [0 1 2 8 4 5 6 3 7] ...
  ==> Optimal arm identification: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 76.54% (relative success)...



- Evaluating policy #104/115: SW-UCB($\tau=500$, $\alpha=1$) ...

Estimated order by the policy SW-UCB($\tau=500$, $\alpha=1$) after 1000 steps: [1 4 3 0 2 7 5 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 60.49% (relative success)...



- Evaluating policy #105/115: SW-UCB+($\tau=332$, $\alpha=1$) ...

Estimated order by the policy SW-UCB+($\tau=332$, $\alpha=1$) after 1000 steps: [3 2 0 6 4 5 7 8 1] ...
  ==> Optimal arm identification: 22.22% (relative success)...
  ==> Mean distance from optimal ordering: 55.56% (relative success)...



- Evaluating policy #106/115: D-UCB($\alpha=1$, $\gamma=0.9$) ...

Estimated order by the policy D-UCB($\alpha=1$, $\gamma=0.9$) after 1000 steps: [1 3 4 5 6 0 2 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 66.67% (relative success)...



- Evaluating policy #107/115: D-UCB+($\alpha=1$, $\gamma=0.99209$) ...

Estimated order by the policy D-UCB+($\alpha=1$, $\gamma=0.99209$) after 1000 steps: [0 2 3 5 8 1 4 7 6] ...
  ==> Optimal arm identification: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 58.02% (relative success)...



- Evaluating policy #108/115: M-UCB($w=450$) ...

Estimated order by the policy M-UCB($w=450$) after 1000 steps: [0 1 4 3 5 7 2 8 6] ...
  ==> Optimal arm identification: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 68.52% (relative success)...



- Evaluating policy #109/115: M-UCB($w=80$) ...

Estimated order by the policy M-UCB($w=80$) after 1000 steps: [0 2 3 4 1 7 6 5 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 70.99% (relative success)...



- Evaluating policy #110/115: OracleRestart-UCB(Per-Arm) ...

Estimated order by the policy OracleRestart-UCB(Per-Arm) after 1000 steps: [1 2 5 3 6 4 8 0 7] ...
  ==> Optimal arm identification: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 55.56% (relative success)...



- Evaluating policy #111/115: Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) ...
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) a change was detected at time 61 for arm 8 after seeing reward = 1.0!
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) a change was detected at time 204 for arm 6 after seeing reward = 1.0!
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) a change was detected at time 196 for arm 5 after seeing reward = 0.0!
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) a change was detected at time 246 for arm 7 after seeing reward = 1.0!
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) a change was detected at time 338 for arm 7 after seeing reward = 1.0!
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) a change was detected at time 370 for arm 5 after seeing reward = 1.0!
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) a change was detected at time 286 for arm 4 after seeing reward = 1.0!
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) a change was detected at time 665 for arm 0 after seeing reward = 0.0!
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) a change was detected at time 802 for arm 4 after seeing reward = 0.0!
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) a change was detected at time 870 for arm 7 after seeing reward = 1.0!

Estimated order by the policy Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) after 1000 steps: [1 0 2 5 7 6 4 3 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 60.49% (relative success)...



- Evaluating policy #112/115: Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking) ...
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking) a change was detected at time 217 for arm 3 after seeing reward = 0.0!
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking) a change was detected at time 301 for arm 5 after seeing reward = 1.0!
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking) a change was detected at time 444 for arm 4 after seeing reward = 0.0!
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking) a change was detected at time 528 for arm 4 after seeing reward = 1.0!
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking) a change was detected at time 542 for arm 4 after seeing reward = 0.0!
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking) a change was detected at time 581 for arm 4 after seeing reward = 1.0!
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking) a change was detected at time 604 for arm 6 after seeing reward = 1.0!
For a player Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking) a change was detected at time 754 for arm 1 after seeing reward = 0.0!

Estimated order by the policy Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking) after 1000 steps: [0 2 1 3 6 5 8 4 7] ...
  ==> Optimal arm identification: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 65.43% (relative success)...



- Evaluating policy #113/115: Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) ...
For a player Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) a change was detected at time 109 for arm 8 after seeing reward = 1.0!
For a player Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) a change was detected at time 134 for arm 7 after seeing reward = 1.0!
For a player Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) a change was detected at time 797 for arm 5 after seeing reward = 1.0!
For a player Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) a change was detected at time 791 for arm 4 after seeing reward = 0.0!

Estimated order by the policy Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$) after 1000 steps: [0 2 1 4 3 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 83.95% (relative success)...



- Evaluating policy #114/115: Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking) ...
For a player Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking) a change was detected at time 102 for arm 8 after seeing reward = 1.0!
For a player Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking) a change was detected at time 116 for arm 5 after seeing reward = 1.0!
For a player Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking) a change was detected at time 937 for arm 4 after seeing reward = 1.0!

Estimated order by the policy Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking) after 1000 steps: [5 1 4 3 6 2 7 8 0] ...
  ==> Optimal arm identification: 11.11% (relative success)...
  ==> Mean distance from optimal ordering: 45.06% (relative success)...



- Evaluating policy #115/115: SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) ...
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 78 for arm 7 after seeing reward = 1.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 76 for arm 8 after seeing reward = 1.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 107 for arm 6 after seeing reward = 1.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 96 for arm 4 after seeing reward = 0.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 197 for arm 3 after seeing reward = 1.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 205 for arm 5 after seeing reward = 1.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 221 for arm 5 after seeing reward = 1.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 311 for arm 5 after seeing reward = 0.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 327 for arm 1 after seeing reward = 0.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 348 for arm 5 after seeing reward = 1.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 362 for arm 5 after seeing reward = 0.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 376 for arm 5 after seeing reward = 0.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 219 for arm 5 after seeing reward = 0.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 238 for arm 5 after seeing reward = 1.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 272 for arm 5 after seeing reward = 0.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 293 for arm 5 after seeing reward = 0.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 533 for arm 3 after seeing reward = 0.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 307 for arm 5 after seeing reward = 1.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 554 for arm 5 after seeing reward = 1.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 854 for arm 4 after seeing reward = 1.0!

Estimated order by the policy SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) after 1000 steps: [0 2 4 1 3 6 5 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 73.46% (relative success)...
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 736 for arm 4 after seeing reward = 0.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 944 for arm 4 after seeing reward = 1.0!
For a player SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$) a change was detected at time 964 for arm 4 after seeing reward = 0.0!

Giving the vector of final regrets ...

  For policy #0 called 'U(1..9)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 395
Mean of   last regrets R_T = 404
Median of last regrets R_T = 404
Max of    last regrets R_T = 414
STD of    last regrets R_T = 6.72

  For policy #1 called 'UniformOnSome([0, 1])' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 737
Mean of   last regrets R_T = 745
Median of last regrets R_T = 746
Max of    last regrets R_T = 752
STD of    last regrets R_T = 5.4

  For policy #2 called 'EmpiricalMeans' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = -1
Mean of   last regrets R_T = 56.5
Median of last regrets R_T = 55
Max of    last regrets R_T = 117
STD of    last regrets R_T = 53.4

  For policy #3 called 'TakeRandomFixedArm([2, 3, 6, 1])' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 505
Mean of   last regrets R_T = 507
Median of last regrets R_T = 505
Max of    last regrets R_T = 512
STD of    last regrets R_T = 3.03

  For policy #4 called 'UniformOnSome([0, 1])' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 731
Mean of   last regrets R_T = 744
Median of last regrets R_T = 747
Max of    last regrets R_T = 749
STD of    last regrets R_T = 7.4

  For policy #5 called 'TakeFixedArm(1)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 682
Mean of   last regrets R_T = 698
Median of last regrets R_T = 699
Max of    last regrets R_T = 712
STD of    last regrets R_T = 10.8

  For policy #6 called 'TakeFixedArm(0)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 780
Mean of   last regrets R_T = 791
Median of last regrets R_T = 793
Max of    last regrets R_T = 799
STD of    last regrets R_T = 7.26

  For policy #7 called 'TakeFixedArm(0)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 791
Mean of   last regrets R_T = 799
Median of last regrets R_T = 800
Max of    last regrets R_T = 806
STD of    last regrets R_T = 5.34

  For policy #8 called 'TakeFixedArm(1)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 694
Mean of   last regrets R_T = 700
Median of last regrets R_T = 697
Max of    last regrets R_T = 711
STD of    last regrets R_T = 6.8

  For policy #9 called 'EpsilonGreedy(0.1)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 46
Mean of   last regrets R_T = 236
Median of last regrets R_T = 144
Max of    last regrets R_T = 609
STD of    last regrets R_T = 228

  For policy #10 called 'EpsilonDecreasing(e:0.1)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 9
Mean of   last regrets R_T = 162
Median of last regrets R_T = 113
Max of    last regrets R_T = 414
STD of    last regrets R_T = 152

  For policy #11 called 'EpsilonExpDecreasing(e:0.1, r:0.005)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 114
Mean of   last regrets R_T = 289
Median of last regrets R_T = 312
Max of    last regrets R_T = 418
STD of    last regrets R_T = 110

  For policy #12 called 'EpsilonFirst($T=1000$, $\varepsilon=0.1$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 115
Mean of   last regrets R_T = 236
Median of last regrets R_T = 160
Max of    last regrets R_T = 508
STD of    last regrets R_T = 160

  For policy #13 called 'ETC_KnownGap($T=1000$, $\Delta=0.05$, $T_0=1602$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 443
Mean of   last regrets R_T = 451
Median of last regrets R_T = 449
Max of    last regrets R_T = 463
STD of    last regrets R_T = 8.15

  For policy #14 called 'ETC_RandomStop($T=1000$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 439
Mean of   last regrets R_T = 462
Median of last regrets R_T = 460
Max of    last regrets R_T = 488
STD of    last regrets R_T = 20

  For policy #15 called 'Softmax(temp: 0.05)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 1
Mean of   last regrets R_T = 51.3
Median of last regrets R_T = 18
Max of    last regrets R_T = 168
STD of    last regrets R_T = 68.7

  For policy #16 called 'Softmax(decreasing)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 6
Mean of   last regrets R_T = 19.5
Median of last regrets R_T = 18.5
Max of    last regrets R_T = 35
STD of    last regrets R_T = 10.4

  For policy #17 called 'SoftMix' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 11
Mean of   last regrets R_T = 17.3
Median of last regrets R_T = 15.5
Max of    last regrets R_T = 27
STD of    last regrets R_T = 6.18

  For policy #18 called 'Softmax($T=1000$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = -11
Mean of   last regrets R_T = 69
Median of last regrets R_T = 89.5
Max of    last regrets R_T = 108
STD of    last regrets R_T = 47.8

  For policy #19 called 'BoltzmannGumbel($\alpha=0.5$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 77
Mean of   last regrets R_T = 78.8
Median of last regrets R_T = 79
Max of    last regrets R_T = 80
STD of    last regrets R_T = 1.09

  For policy #20 called 'Exp3($\gamma: 0.001$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 387
Mean of   last regrets R_T = 405
Median of last regrets R_T = 408
Max of    last regrets R_T = 416
STD of    last regrets R_T = 10.7

  For policy #21 called 'Exp3(decreasing)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 263
Mean of   last regrets R_T = 280
Median of last regrets R_T = 275
Max of    last regrets R_T = 307
STD of    last regrets R_T = 17.8

  For policy #22 called 'Exp3(SoftMix)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 327
Mean of   last regrets R_T = 348
Median of last regrets R_T = 344
Max of    last regrets R_T = 376
STD of    last regrets R_T = 18.1

  For policy #23 called 'Exp3($T=1000$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 294
Mean of   last regrets R_T = 310
Median of last regrets R_T = 310
Max of    last regrets R_T = 325
STD of    last regrets R_T = 11.9

  For policy #24 called 'Exp3ELM($\delta=0.1$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 97
Mean of   last regrets R_T = 125
Median of last regrets R_T = 117
Max of    last regrets R_T = 169
STD of    last regrets R_T = 29.2

  For policy #25 called 'Exp3++' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 176
Mean of   last regrets R_T = 188
Median of last regrets R_T = 187
Max of    last regrets R_T = 203
STD of    last regrets R_T = 11.8

  For policy #26 called 'Pursuit(0.5)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 5
Mean of   last regrets R_T = 14.5
Median of last regrets R_T = 16.5
Max of    last regrets R_T = 20
STD of    last regrets R_T = 5.68

  For policy #27 called 'Hedge($\varepsilon: 0.5$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 222
Mean of   last regrets R_T = 235
Median of last regrets R_T = 234
Max of    last regrets R_T = 250
STD of    last regrets R_T = 12.2

  For policy #28 called 'Hedge(decreasing)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 298
Mean of   last regrets R_T = 318
Median of last regrets R_T = 319
Max of    last regrets R_T = 336
STD of    last regrets R_T = 16.3

  For policy #29 called 'Hedge($T=1000$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 336
Mean of   last regrets R_T = 343
Median of last regrets R_T = 341
Max of    last regrets R_T = 356
STD of    last regrets R_T = 7.66

  For policy #30 called 'UCB' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 122
Mean of   last regrets R_T = 141
Median of last regrets R_T = 142
Max of    last regrets R_T = 156
STD of    last regrets R_T = 13.4

  For policy #31 called 'UCBlog10' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 57
Mean of   last regrets R_T = 76.5
Median of last regrets R_T = 80.5
Max of    last regrets R_T = 88
STD of    last regrets R_T = 12.8

  For policy #32 called 'UCBwrong' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 204
Mean of   last regrets R_T = 223
Median of last regrets R_T = 227
Max of    last regrets R_T = 234
STD of    last regrets R_T = 12.4

  For policy #33 called 'UCB($\alpha=1$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 45
Mean of   last regrets R_T = 50.8
Median of last regrets R_T = 51
Max of    last regrets R_T = 56
STD of    last regrets R_T = 3.9

  For policy #34 called 'UCB($\alpha=1$, $\log_{10}$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 30
Mean of   last regrets R_T = 40
Median of last regrets R_T = 39
Max of    last regrets R_T = 52
STD of    last regrets R_T = 7.97

  For policy #35 called 'UCBmin' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 76
Mean of   last regrets R_T = 91.8
Median of last regrets R_T = 94.5
Max of    last regrets R_T = 102
STD of    last regrets R_T = 9.65

  For policy #36 called 'UCBplus' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 76
Mean of   last regrets R_T = 85.5
Median of last regrets R_T = 85
Max of    last regrets R_T = 96
STD of    last regrets R_T = 9.53

  For policy #37 called 'UCBrandomInit' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 122
Mean of   last regrets R_T = 134
Median of last regrets R_T = 134
Max of    last regrets R_T = 147
STD of    last regrets R_T = 11.3

  For policy #38 called 'UCBcython($\alpha=4$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 102
Mean of   last regrets R_T = 126
Median of last regrets R_T = 128
Max of    last regrets R_T = 147
STD of    last regrets R_T = 18.3

  For policy #39 called 'UCBV' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 181
Mean of   last regrets R_T = 205
Median of last regrets R_T = 209
Max of    last regrets R_T = 220
STD of    last regrets R_T = 14.9

  For policy #40 called 'UCBVtuned' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 88
Mean of   last regrets R_T = 97.3
Median of last regrets R_T = 100
Max of    last regrets R_T = 101
STD of    last regrets R_T = 5.4

  For policy #41 called 'SW-UCB($\tau=1000$, $\alpha=0.6$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 44
Mean of   last regrets R_T = 65.3
Median of last regrets R_T = 67.5
Max of    last regrets R_T = 82
STD of    last regrets R_T = 13.7

  For policy #42 called 'SW-UCB+($\tau=1000$, $\alpha=0.6$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 54
Mean of   last regrets R_T = 63.5
Median of last regrets R_T = 63.5
Max of    last regrets R_T = 73
STD of    last regrets R_T = 6.73

  For policy #43 called 'D-UCB($\alpha=4$, $\gamma=1$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 144
Mean of   last regrets R_T = 149
Median of last regrets R_T = 147
Max of    last regrets R_T = 157
STD of    last regrets R_T = 4.92

  For policy #44 called 'D-UCB+($\alpha=4$, $\gamma=1$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 123
Mean of   last regrets R_T = 130
Median of last regrets R_T = 130
Max of    last regrets R_T = 137
STD of    last regrets R_T = 6.08

  For policy #45 called 'SW-Restart(SWR_UCB, $\tau=100$, $\varepsilon=0.005$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 203
Mean of   last regrets R_T = 212
Median of last regrets R_T = 214
Max of    last regrets R_T = 216
STD of    last regrets R_T = 5.02

  For policy #46 called 'SW-Restart(UCB($\alpha=0.5$), $\tau=100$, $\varepsilon=0.005$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 100
Mean of   last regrets R_T = 111
Median of last regrets R_T = 104
Max of    last regrets R_T = 137
STD of    last regrets R_T = 15.1

  For policy #47 called 'SparseUCB($s=2$, $\alpha=4$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 69
Mean of   last regrets R_T = 99.8
Median of last regrets R_T = 98.5
Max of    last regrets R_T = 133
STD of    last regrets R_T = 22.7

  For policy #48 called 'Sparse-kl-UCB($s=2$, Bern)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 1
Mean of   last regrets R_T = 71.8
Median of last regrets R_T = 28
Max of    last regrets R_T = 230
STD of    last regrets R_T = 92

  For policy #49 called 'MOSS' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 36
Mean of   last regrets R_T = 47.5
Median of last regrets R_T = 48
Max of    last regrets R_T = 58
STD of    last regrets R_T = 7.83

  For policy #50 called 'MOSS-H($T=1000$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 30
Mean of   last regrets R_T = 50.5
Median of last regrets R_T = 50.5
Max of    last regrets R_T = 71
STD of    last regrets R_T = 14.5

  For policy #51 called 'MOSS-Anytime($\alpha=1.35$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 40
Mean of   last regrets R_T = 49.5
Median of last regrets R_T = 48.5
Max of    last regrets R_T = 61
STD of    last regrets R_T = 8.14

  For policy #52 called 'MOSS-Experimental' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 14
Mean of   last regrets R_T = 26.5
Median of last regrets R_T = 28
Max of    last regrets R_T = 36
STD of    last regrets R_T = 7.95

  For policy #53 called 'OC-UCB($\eta=1.1$, $\rho=1$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 112
Mean of   last regrets R_T = 124
Median of last regrets R_T = 122
Max of    last regrets R_T = 139
STD of    last regrets R_T = 11.6

  For policy #54 called 'CPUCB' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 122
Mean of   last regrets R_T = 134
Median of last regrets R_T = 136
Max of    last regrets R_T = 144
STD of    last regrets R_T = 7.89

  For policy #55 called 'DMED$^+$(Bern)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 16
Mean of   last regrets R_T = 30.5
Median of last regrets R_T = 33.5
Max of    last regrets R_T = 39
STD of    last regrets R_T = 9.01

  For policy #56 called 'DMED(Bern)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 59
Mean of   last regrets R_T = 73.3
Median of last regrets R_T = 72
Max of    last regrets R_T = 90
STD of    last regrets R_T = 13.1

  For policy #57 called 'Thompson' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 30
Mean of   last regrets R_T = 33.8
Median of last regrets R_T = 34
Max of    last regrets R_T = 37
STD of    last regrets R_T = 2.86

  For policy #58 called 'Thompson(Gauss)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 19
Mean of   last regrets R_T = 28.3
Median of last regrets R_T = 31
Max of    last regrets R_T = 32
STD of    last regrets R_T = 5.4

  For policy #59 called 'ThompsonRobust(averageOn = 10)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 2
Mean of   last regrets R_T = 18
Median of last regrets R_T = 12.5
Max of    last regrets R_T = 45
STD of    last regrets R_T = 16.4

  For policy #60 called 'kl-UCB' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 38
Mean of   last regrets R_T = 51.3
Median of last regrets R_T = 53.5
Max of    last regrets R_T = 60
STD of    last regrets R_T = 8.98

  For policy #61 called 'SW-Restart(kl-UCB, $\tau=100$, $\varepsilon=0.005$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 75
Mean of   last regrets R_T = 96.8
Median of last regrets R_T = 94.5
Max of    last regrets R_T = 123
STD of    last regrets R_T = 17.1

  For policy #62 called 'kl-UCB($\log\log$, Bern)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 34
Mean of   last regrets R_T = 42
Median of last regrets R_T = 42
Max of    last regrets R_T = 50
STD of    last regrets R_T = 5.83

  For policy #63 called 'kl-UCB($\log_{10}$, Bern)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 21
Mean of   last regrets R_T = 28
Median of last regrets R_T = 27
Max of    last regrets R_T = 37
STD of    last regrets R_T = 6.75

  For policy #64 called 'kl-UCB($\log_{10}\log_{10}$, Bern)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 12
Mean of   last regrets R_T = 23.5
Median of last regrets R_T = 23.5
Max of    last regrets R_T = 35
STD of    last regrets R_T = 9.34

  For policy #65 called 'kl-UCB$^+$' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 20
Mean of   last regrets R_T = 33
Median of last regrets R_T = 31
Max of    last regrets R_T = 50
STD of    last regrets R_T = 10.8

  For policy #66 called 'kl-UCB-H($T=1000$, Bern)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 23
Mean of   last regrets R_T = 35.8
Median of last regrets R_T = 37.5
Max of    last regrets R_T = 45
STD of    last regrets R_T = 8.17

  For policy #67 called 'kl-UCB-H+($T=1000$, Bern)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 2
Mean of   last regrets R_T = 28.5
Median of last regrets R_T = 32.5
Max of    last regrets R_T = 47
STD of    last regrets R_T = 16.4

  For policy #68 called 'kl-UCB$^{++}$($T=1000$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 20
Mean of   last regrets R_T = 29
Median of last regrets R_T = 27
Max of    last regrets R_T = 42
STD of    last regrets R_T = 8.31

  For policy #69 called 'BayesUCB' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 14
Mean of   last regrets R_T = 18.8
Median of last regrets R_T = 18.5
Max of    last regrets R_T = 24
STD of    last regrets R_T = 4.32

  For policy #70 called 'AdBandits($T=1000$, $\alpha=0.5$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 29
Mean of   last regrets R_T = 33.5
Median of last regrets R_T = 31.5
Max of    last regrets R_T = 42
STD of    last regrets R_T = 5.12

  For policy #71 called 'ApprFHG($T=1100$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 43
Mean of   last regrets R_T = 49.8
Median of last regrets R_T = 47
Max of    last regrets R_T = 62
STD of    last regrets R_T = 7.4

  For policy #72 called 'UnsupervisedLearning(SimpleGaussianKernel, :math:`T_0=100`, :math:`T_1=1000`, :math:`M=100`)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 367
Mean of   last regrets R_T = 373
Median of last regrets R_T = 373
Max of    last regrets R_T = 377
STD of    last regrets R_T = 4.56

  For policy #73 called 'OSSB($\varepsilon=0.01$, $\gamma=0$, Bern)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 17
Mean of   last regrets R_T = 30.3
Median of last regrets R_T = 33
Max of    last regrets R_T = 38
STD of    last regrets R_T = 7.95

  For policy #74 called 'BESA' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 20
Mean of   last regrets R_T = 29.5
Median of last regrets R_T = 30.5
Max of    last regrets R_T = 37
STD of    last regrets R_T = 6.1

  For policy #75 called 'UCB$\dagger$($T=1000$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 60
Mean of   last regrets R_T = 71.5
Median of last regrets R_T = 73
Max of    last regrets R_T = 80
STD of    last regrets R_T = 7.37

  For policy #76 called '$\mathrm{UCB}_{d=d_{bq}}$($c=0$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 38
Mean of   last regrets R_T = 48.8
Median of last regrets R_T = 49
Max of    last regrets R_T = 59
STD of    last regrets R_T = 8.93

  For policy #77 called '$\mathrm{UCB}_{d=d_h}$($c=0$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 2.27e-13
Mean of   last regrets R_T = 12.8
Median of last regrets R_T = 15
Max of    last regrets R_T = 21
STD of    last regrets R_T = 7.79

  For policy #78 called '$\mathrm{UCB}_{d=d_{lb}}$($c=0$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 22
Mean of   last regrets R_T = 29.5
Median of last regrets R_T = 29.5
Max of    last regrets R_T = 37
STD of    last regrets R_T = 5.85

  For policy #79 called 'UCBoost($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = -2
Mean of   last regrets R_T = 103
Median of last regrets R_T = 4
Max of    last regrets R_T = 406
STD of    last regrets R_T = 175

  For policy #80 called 'UCBoost($\varepsilon=0.1$, $c=0$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 24
Mean of   last regrets R_T = 28
Median of last regrets R_T = 28
Max of    last regrets R_T = 32
STD of    last regrets R_T = 2.92

  For policy #81 called 'UCBoost($\varepsilon=0.05$, $c=0$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 26
Mean of   last regrets R_T = 29.8
Median of last regrets R_T = 27.5
Max of    last regrets R_T = 38
STD of    last regrets R_T = 4.82

  For policy #82 called 'UCBoost($\varepsilon=0.01$, $c=0$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 15
Mean of   last regrets R_T = 35
Median of last regrets R_T = 36.5
Max of    last regrets R_T = 52
STD of    last regrets R_T = 13.5

  For policy #83 called '$\mathrm{UCBcython}_{d=d_{bq}}$($c=0$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 38
Mean of   last regrets R_T = 45.5
Median of last regrets R_T = 45
Max of    last regrets R_T = 54
STD of    last regrets R_T = 5.85

  For policy #84 called '$\mathrm{UCBcython}_{d=d_h}$($c=0$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 2.27e-13
Mean of   last regrets R_T = 30.5
Median of last regrets R_T = 7.5
Max of    last regrets R_T = 107
STD of    last regrets R_T = 44.4

  For policy #85 called '$\mathrm{UCBcython}_{d=d_{lb}}$($c=0$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 10
Mean of   last regrets R_T = 35.8
Median of last regrets R_T = 35.5
Max of    last regrets R_T = 62
STD of    last regrets R_T = 18.8

  For policy #86 called 'UCBoostcython($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 5
Mean of   last regrets R_T = 135
Median of last regrets R_T = 115
Max of    last regrets R_T = 307
STD of    last regrets R_T = 127

  For policy #87 called 'UCBoostcython($\varepsilon=0.1$, $c=0$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 44
Mean of   last regrets R_T = 60.3
Median of last regrets R_T = 60.5
Max of    last regrets R_T = 76
STD of    last regrets R_T = 13.9

  For policy #88 called 'UCBoostcython($\varepsilon=0.05$, $c=0$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 21
Mean of   last regrets R_T = 40
Median of last regrets R_T = 43
Max of    last regrets R_T = 53
STD of    last regrets R_T = 12.2

  For policy #89 called 'UCBoostcython($\varepsilon=0.01$, $c=0$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 33
Mean of   last regrets R_T = 37.3
Median of last regrets R_T = 38
Max of    last regrets R_T = 40
STD of    last regrets R_T = 2.95

  For policy #90 called 'UCBcython($\alpha=4$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 122
Mean of   last regrets R_T = 138
Median of last regrets R_T = 141
Max of    last regrets R_T = 147
STD of    last regrets R_T = 9.66

  For policy #91 called 'UCBcython($\alpha=1$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 121
Mean of   last regrets R_T = 131
Median of last regrets R_T = 130
Max of    last regrets R_T = 144
STD of    last regrets R_T = 8.7

  For policy #92 called 'UCBcython($\alpha=0.5$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 126
Mean of   last regrets R_T = 141
Median of last regrets R_T = 143
Max of    last regrets R_T = 154
STD of    last regrets R_T = 10.7

  For policy #93 called 'Exp3++' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 173
Mean of   last regrets R_T = 189
Median of last regrets R_T = 187
Max of    last regrets R_T = 208
STD of    last regrets R_T = 15.1

  For policy #94 called 'DiscountedThompson($\gamma=0.99$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 76
Mean of   last regrets R_T = 85.5
Median of last regrets R_T = 82
Max of    last regrets R_T = 102
STD of    last regrets R_T = 10.5

  For policy #95 called 'Exp3R($T=1000$, $c=8.77$, $\alpha=0.01$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 335
Mean of   last regrets R_T = 359
Median of last regrets R_T = 358
Max of    last regrets R_T = 384
STD of    last regrets R_T = 17.4

  For policy #96 called 'Exp3R++($T=1000$, $c=1.76$, $\alpha=0.247$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 175
Mean of   last regrets R_T = 192
Median of last regrets R_T = 189
Max of    last regrets R_T = 213
STD of    last regrets R_T = 13.8

  For policy #97 called 'AdSwitch($T=1000$, $C_1=1$, $C_2=1$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 404
Mean of   last regrets R_T = 424
Median of last regrets R_T = 425
Max of    last regrets R_T = 444
STD of    last regrets R_T = 17.5

  For policy #98 called 'LM-DSEE($\gamma=200$, $\rho=0.6$, $\ell=1.48e+04$, $a=1$, $b=0.25$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 787
Mean of   last regrets R_T = 797
Median of last regrets R_T = 797
Max of    last regrets R_T = 808
STD of    last regrets R_T = 9.78

  For policy #99 called 'CUSUM-UCB($\alpha=0.0416$, $M=50$, Per-Arm)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 116
Mean of   last regrets R_T = 131
Median of last regrets R_T = 127
Max of    last regrets R_T = 152
STD of    last regrets R_T = 13.3

  For policy #100 called 'PHT-UCB($\alpha=0.0416$, $M=50$, Per-Arm)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 129
Mean of   last regrets R_T = 133
Median of last regrets R_T = 133
Max of    last regrets R_T = 138
STD of    last regrets R_T = 4.26

  For policy #101 called 'SW-UCB#($\lambda=1$, $\alpha=1$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 122
Mean of   last regrets R_T = 134
Median of last regrets R_T = 133
Max of    last regrets R_T = 147
STD of    last regrets R_T = 10.7

  For policy #102 called 'SW-Restart(UCB, $\tau=100$, $\varepsilon=0.005$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 191
Mean of   last regrets R_T = 213
Median of last regrets R_T = 217
Max of    last regrets R_T = 229
STD of    last regrets R_T = 14.2

  For policy #103 called 'SW-UCB($\tau=500$, $\alpha=1$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 109
Mean of   last regrets R_T = 128
Median of last regrets R_T = 127
Max of    last regrets R_T = 149
STD of    last regrets R_T = 14.8

  For policy #104 called 'SW-UCB+($\tau=332$, $\alpha=1$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 133
Mean of   last regrets R_T = 151
Median of last regrets R_T = 154
Max of    last regrets R_T = 161
STD of    last regrets R_T = 10.9

  For policy #105 called 'D-UCB($\alpha=1$, $\gamma=0.9$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = -2
Mean of   last regrets R_T = 86.8
Median of last regrets R_T = 69.5
Max of    last regrets R_T = 210
STD of    last regrets R_T = 83.9

  For policy #106 called 'D-UCB+($\alpha=1$, $\gamma=0.99209$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = -3
Mean of   last regrets R_T = 133
Median of last regrets R_T = 153
Max of    last regrets R_T = 230
STD of    last regrets R_T = 94.2

  For policy #107 called 'M-UCB($w=450$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 118
Mean of   last regrets R_T = 132
Median of last regrets R_T = 130
Max of    last regrets R_T = 148
STD of    last regrets R_T = 11.8

  For policy #108 called 'M-UCB($w=80$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 146
Mean of   last regrets R_T = 149
Median of last regrets R_T = 147
Max of    last regrets R_T = 155
STD of    last regrets R_T = 3.77

  For policy #109 called 'OracleRestart-UCB(Per-Arm)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 130
Mean of   last regrets R_T = 143
Median of last regrets R_T = 143
Max of    last regrets R_T = 157
STD of    last regrets R_T = 12.5

  For policy #110 called 'Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 134
Mean of   last regrets R_T = 150
Median of last regrets R_T = 152
Max of    last regrets R_T = 161
STD of    last regrets R_T = 10.5

  For policy #111 called 'Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 136
Mean of   last regrets R_T = 151
Median of last regrets R_T = 150
Max of    last regrets R_T = 169
STD of    last regrets R_T = 11.8

  For policy #112 called 'Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 134
Mean of   last regrets R_T = 155
Median of last regrets R_T = 153
Max of    last regrets R_T = 181
STD of    last regrets R_T = 19.7

  For policy #113 called 'Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 120
Mean of   last regrets R_T = 137
Median of last regrets R_T = 127
Max of    last regrets R_T = 174
STD of    last regrets R_T = 22

  For policy #114 called 'SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$)' ...
  Last regrets (for all repetitions) have:
Min of    last regrets R_T = 121
Mean of   last regrets R_T = 163
Median of last regrets R_T = 166
Max of    last regrets R_T = 198
STD of    last regrets R_T = 28.3

Giving the final ranks ...

Final ranking for this environment #0 :
- Policy '$\mathrm{UCB}_{d=d_h}$($c=0$)'	was ranked	1 / 115 for this simulation (last regret = 5.575).
- Policy 'SoftMix'	was ranked	2 / 115 for this simulation (last regret = 6.425).
- Policy 'Softmax(decreasing)'	was ranked	3 / 115 for this simulation (last regret = 8.7).
- Policy 'Pursuit(0.5)'	was ranked	4 / 115 for this simulation (last regret = 11.125).
- Policy 'ThompsonRobust(averageOn = 10)'	was ranked	5 / 115 for this simulation (last regret = 16.775).
- Policy 'BayesUCB'	was ranked	6 / 115 for this simulation (last regret = 19.025).
- Policy 'kl-UCB($\log_{10}$, Bern)'	was ranked	7 / 115 for this simulation (last regret = 21.4).
- Policy 'kl-UCB($\log_{10}\log_{10}$, Bern)'	was ranked	8 / 115 for this simulation (last regret = 22.725).
- Policy 'AdBandits($T=1000$, $\alpha=0.5$)'	was ranked	9 / 115 for this simulation (last regret = 23.7).
- Policy 'BESA'	was ranked	10 / 115 for this simulation (last regret = 25.775).
- Policy '$\mathrm{UCB}_{d=d_{lb}}$($c=0$)'	was ranked	11 / 115 for this simulation (last regret = 25.85).
- Policy 'kl-UCB$^{++}$($T=1000$)'	was ranked	12 / 115 for this simulation (last regret = 25.95).
- Policy 'DMED$^+$(Bern)'	was ranked	13 / 115 for this simulation (last regret = 29.075).
- Policy 'UCBoostcython($\varepsilon=0.01$, $c=0$)'	was ranked	14 / 115 for this simulation (last regret = 29.075).
- Policy 'Thompson(Gauss)'	was ranked	15 / 115 for this simulation (last regret = 29.175).
- Policy 'Thompson'	was ranked	16 / 115 for this simulation (last regret = 29.225).
- Policy 'UCBoost($\varepsilon=0.05$, $c=0$)'	was ranked	17 / 115 for this simulation (last regret = 30.2).
- Policy 'UCBoost($\varepsilon=0.1$, $c=0$)'	was ranked	18 / 115 for this simulation (last regret = 30.325).
- Policy '$\mathrm{UCBcython}_{d=d_h}$($c=0$)'	was ranked	19 / 115 for this simulation (last regret = 30.925).
- Policy 'kl-UCB$^+$'	was ranked	20 / 115 for this simulation (last regret = 31.1).
- Policy 'UCB($\alpha=1$, $\log_{10}$)'	was ranked	21 / 115 for this simulation (last regret = 31.175).
- Policy 'MOSS-Experimental'	was ranked	22 / 115 for this simulation (last regret = 32.675).
- Policy '$\mathrm{UCBcython}_{d=d_{lb}}$($c=0$)'	was ranked	23 / 115 for this simulation (last regret = 34.775).
- Policy 'kl-UCB-H+($T=1000$, Bern)'	was ranked	24 / 115 for this simulation (last regret = 35.675).
- Policy 'OSSB($\varepsilon=0.01$, $\gamma=0$, Bern)'	was ranked	25 / 115 for this simulation (last regret = 36).
- Policy 'UCBoost($\varepsilon=0.01$, $c=0$)'	was ranked	26 / 115 for this simulation (last regret = 36.225).
- Policy 'kl-UCB-H($T=1000$, Bern)'	was ranked	27 / 115 for this simulation (last regret = 39.95).
- Policy 'kl-UCB($\log\log$, Bern)'	was ranked	28 / 115 for this simulation (last regret = 41.45).
- Policy 'UCB($\alpha=1$)'	was ranked	29 / 115 for this simulation (last regret = 41.6).
- Policy '$\mathrm{UCBcython}_{d=d_{bq}}$($c=0$)'	was ranked	30 / 115 for this simulation (last regret = 41.625).
- Policy 'MOSS'	was ranked	31 / 115 for this simulation (last regret = 42.9).
- Policy 'MOSS-H($T=1000$)'	was ranked	32 / 115 for this simulation (last regret = 42.975).
- Policy 'kl-UCB'	was ranked	33 / 115 for this simulation (last regret = 44.55).
- Policy 'UCBoostcython($\varepsilon=0.05$, $c=0$)'	was ranked	34 / 115 for this simulation (last regret = 45.925).
- Policy '$\mathrm{UCB}_{d=d_{bq}}$($c=0$)'	was ranked	35 / 115 for this simulation (last regret = 48.625).
- Policy 'MOSS-Anytime($\alpha=1.35$)'	was ranked	36 / 115 for this simulation (last regret = 49.35).
- Policy 'ApprFHG($T=1100$)'	was ranked	37 / 115 for this simulation (last regret = 49.95).
- Policy 'Softmax(temp: 0.05)'	was ranked	38 / 115 for this simulation (last regret = 50.95).
- Policy 'EmpiricalMeans'	was ranked	39 / 115 for this simulation (last regret = 54.15).
- Policy 'UCBoostcython($\varepsilon=0.1$, $c=0$)'	was ranked	40 / 115 for this simulation (last regret = 57.125).
- Policy 'SW-UCB+($\tau=1000$, $\alpha=0.6$)'	was ranked	41 / 115 for this simulation (last regret = 65.9).
- Policy 'Sparse-kl-UCB($s=2$, Bern)'	was ranked	42 / 115 for this simulation (last regret = 65.95).
- Policy 'DMED(Bern)'	was ranked	43 / 115 for this simulation (last regret = 70.95).
- Policy 'SW-UCB($\tau=1000$, $\alpha=0.6$)'	was ranked	44 / 115 for this simulation (last regret = 72.425).
- Policy 'UCB$\dagger$($T=1000$)'	was ranked	45 / 115 for this simulation (last regret = 74.075).
- Policy 'UCBlog10'	was ranked	46 / 115 for this simulation (last regret = 75.875).
- Policy 'D-UCB($\alpha=1$, $\gamma=0.9$)'	was ranked	47 / 115 for this simulation (last regret = 77.35).
- Policy 'Softmax($T=1000$)'	was ranked	48 / 115 for this simulation (last regret = 78.35).
- Policy 'UCBplus'	was ranked	49 / 115 for this simulation (last regret = 82.025).
- Policy 'DiscountedThompson($\gamma=0.99$)'	was ranked	50 / 115 for this simulation (last regret = 82.575).
- Policy 'UCBmin'	was ranked	51 / 115 for this simulation (last regret = 87.475).
- Policy 'BoltzmannGumbel($\alpha=0.5$)'	was ranked	52 / 115 for this simulation (last regret = 90.05).
- Policy 'SparseUCB($s=2$, $\alpha=4$)'	was ranked	53 / 115 for this simulation (last regret = 92.325).
- Policy 'SW-Restart(kl-UCB, $\tau=100$, $\varepsilon=0.005$)'	was ranked	54 / 115 for this simulation (last regret = 95.075).
- Policy 'UCBoost($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)'	was ranked	55 / 115 for this simulation (last regret = 104).
- Policy 'UCBVtuned'	was ranked	56 / 115 for this simulation (last regret = 105.88).
- Policy 'SW-Restart(UCB($\alpha=0.5$), $\tau=100$, $\varepsilon=0.005$)'	was ranked	57 / 115 for this simulation (last regret = 107.43).
- Policy 'SW-UCB($\tau=500$, $\alpha=1$)'	was ranked	58 / 115 for this simulation (last regret = 117.68).
- Policy 'OC-UCB($\eta=1.1$, $\rho=1$)'	was ranked	59 / 115 for this simulation (last regret = 124.33).
- Policy 'CUSUM-UCB($\alpha=0.0416$, $M=50$, Per-Arm)'	was ranked	60 / 115 for this simulation (last regret = 125.1).
- Policy 'D-UCB+($\alpha=1$, $\gamma=0.99209$)'	was ranked	61 / 115 for this simulation (last regret = 126.35).
- Policy 'D-UCB+($\alpha=4$, $\gamma=1$)'	was ranked	62 / 115 for this simulation (last regret = 126.73).
- Policy 'UCBoostcython($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)'	was ranked	63 / 115 for this simulation (last regret = 128.3).
- Policy 'UCBrandomInit'	was ranked	64 / 115 for this simulation (last regret = 128.5).
- Policy 'UCBcython($\alpha=4$)'	was ranked	65 / 115 for this simulation (last regret = 128.97).
- Policy 'M-UCB($w=450$)'	was ranked	66 / 115 for this simulation (last regret = 129.5).
- Policy 'SW-UCB#($\lambda=1$, $\alpha=1$)'	was ranked	67 / 115 for this simulation (last regret = 129.88).
- Policy 'UCBcython($\alpha=1$)'	was ranked	68 / 115 for this simulation (last regret = 132.3).
- Policy 'Exp3ELM($\delta=0.1$)'	was ranked	69 / 115 for this simulation (last regret = 134.18).
- Policy 'D-UCB($\alpha=4$, $\gamma=1$)'	was ranked	70 / 115 for this simulation (last regret = 134.58).
- Policy 'OracleRestart-UCB(Per-Arm)'	was ranked	71 / 115 for this simulation (last regret = 137).
- Policy 'UCBcython($\alpha=0.5$)'	was ranked	72 / 115 for this simulation (last regret = 137.3).
- Policy 'UCBcython($\alpha=4$)'	was ranked	73 / 115 for this simulation (last regret = 138.13).
- Policy 'CPUCB'	was ranked	74 / 115 for this simulation (last regret = 139.08).
- Policy 'PHT-UCB($\alpha=0.0416$, $M=50$, Per-Arm)'	was ranked	75 / 115 for this simulation (last regret = 139.23).
- Policy 'Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking)'	was ranked	76 / 115 for this simulation (last regret = 139.85).
- Policy 'UCB'	was ranked	77 / 115 for this simulation (last regret = 141.1).
- Policy 'M-UCB($w=80$)'	was ranked	78 / 115 for this simulation (last regret = 142.78).
- Policy 'EpsilonDecreasing(e:0.1)'	was ranked	79 / 115 for this simulation (last regret = 149.5).
- Policy 'Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking)'	was ranked	80 / 115 for this simulation (last regret = 152.2).
- Policy 'Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$)'	was ranked	81 / 115 for this simulation (last regret = 152.6).
- Policy 'Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$)'	was ranked	82 / 115 for this simulation (last regret = 153.98).
- Policy 'SW-UCB+($\tau=332$, $\alpha=1$)'	was ranked	83 / 115 for this simulation (last regret = 156.33).
- Policy 'SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$)'	was ranked	84 / 115 for this simulation (last regret = 159.75).
- Policy 'Exp3++'	was ranked	85 / 115 for this simulation (last regret = 184.8).
- Policy 'Exp3++'	was ranked	86 / 115 for this simulation (last regret = 186.53).
- Policy 'Exp3R++($T=1000$, $c=1.76$, $\alpha=0.247$)'	was ranked	87 / 115 for this simulation (last regret = 190.95).
- Policy 'UCBV'	was ranked	88 / 115 for this simulation (last regret = 206.73).
- Policy 'SW-Restart(SWR_UCB, $\tau=100$, $\varepsilon=0.005$)'	was ranked	89 / 115 for this simulation (last regret = 209.1).
- Policy 'SW-Restart(UCB, $\tau=100$, $\varepsilon=0.005$)'	was ranked	90 / 115 for this simulation (last regret = 210.03).
- Policy 'EpsilonGreedy(0.1)'	was ranked	91 / 115 for this simulation (last regret = 219.62).
- Policy 'Hedge($\varepsilon: 0.5$)'	was ranked	92 / 115 for this simulation (last regret = 231.22).
- Policy 'UCBwrong'	was ranked	93 / 115 for this simulation (last regret = 242.6).
- Policy 'EpsilonFirst($T=1000$, $\varepsilon=0.1$)'	was ranked	94 / 115 for this simulation (last regret = 245.72).
- Policy 'Exp3(decreasing)'	was ranked	95 / 115 for this simulation (last regret = 270.83).
- Policy 'EpsilonExpDecreasing(e:0.1, r:0.005)'	was ranked	96 / 115 for this simulation (last regret = 275.73).
- Policy 'Exp3($T=1000$)'	was ranked	97 / 115 for this simulation (last regret = 307.02).
- Policy 'Hedge(decreasing)'	was ranked	98 / 115 for this simulation (last regret = 321.45).
- Policy 'Hedge($T=1000$)'	was ranked	99 / 115 for this simulation (last regret = 340.42).
- Policy 'Exp3(SoftMix)'	was ranked	100 / 115 for this simulation (last regret = 347.3).
- Policy 'UnsupervisedLearning(SimpleGaussianKernel, :math:`T_0=100`, :math:`T_1=1000`, :math:`M=100`)'	was ranked	101 / 115 for this simulation (last regret = 360.23).
- Policy 'Exp3R($T=1000$, $c=8.77$, $\alpha=0.01$)'	was ranked	102 / 115 for this simulation (last regret = 363.48).
- Policy 'U(1..9)'	was ranked	103 / 115 for this simulation (last regret = 389.8).
- Policy 'Exp3($\gamma: 0.001$)'	was ranked	104 / 115 for this simulation (last regret = 393.25).
- Policy 'AdSwitch($T=1000$, $C_1=1$, $C_2=1$)'	was ranked	105 / 115 for this simulation (last regret = 420.4).
- Policy 'ETC_KnownGap($T=1000$, $\Delta=0.05$, $T_0=1602$)'	was ranked	106 / 115 for this simulation (last regret = 448.15).
- Policy 'ETC_RandomStop($T=1000$)'	was ranked	107 / 115 for this simulation (last regret = 449.53).
- Policy 'TakeRandomFixedArm([2, 3, 6, 1])'	was ranked	108 / 115 for this simulation (last regret = 498.3).
- Policy 'TakeFixedArm(1)'	was ranked	109 / 115 for this simulation (last regret = 693.7).
- Policy 'TakeFixedArm(1)'	was ranked	110 / 115 for this simulation (last regret = 693.7).
- Policy 'UniformOnSome([0, 1])'	was ranked	111 / 115 for this simulation (last regret = 741.33).
- Policy 'UniformOnSome([0, 1])'	was ranked	112 / 115 for this simulation (last regret = 742.7).
- Policy 'LM-DSEE($\gamma=200$, $\rho=0.6$, $\ell=1.48e+04$, $a=1$, $b=0.25$)'	was ranked	113 / 115 for this simulation (last regret = 792.8).
- Policy 'TakeFixedArm(0)'	was ranked	114 / 115 for this simulation (last regret = 792.8).
- Policy 'TakeFixedArm(0)'	was ranked	115 / 115 for this simulation (last regret = 792.8).

Giving the mean and std running times ...

For policy #8 called 'TakeFixedArm(1)' ...
    8.25 ms ± 166 µs per loop (mean ± std. dev. of 4 runs)

For policy #7 called 'TakeFixedArm(0)' ...
    8.28 ms ± 1.74 ms per loop (mean ± std. dev. of 4 runs)

For policy #5 called 'TakeFixedArm(1)' ...
    8.41 ms ± 1.12 ms per loop (mean ± std. dev. of 4 runs)

For policy #6 called 'TakeFixedArm(0)' ...
    9.74 ms ± 2.26 ms per loop (mean ± std. dev. of 4 runs)

For policy #1 called 'UniformOnSome([0, 1])' ...
    10.7 ms ± 2.08 ms per loop (mean ± std. dev. of 4 runs)

For policy #98 called 'LM-DSEE($\gamma=200$, $\rho=0.6$, $\ell=1.48e+04$, $a=1$, $b=0.25$)' ...
    14.7 ms ± 2.53 ms per loop (mean ± std. dev. of 4 runs)

For policy #4 called 'UniformOnSome([0, 1])' ...
    15.1 ms ± 3.11 ms per loop (mean ± std. dev. of 4 runs)

For policy #0 called 'U(1..9)' ...
    15.2 ms ± 9.38 ms per loop (mean ± std. dev. of 4 runs)

For policy #13 called 'ETC_KnownGap($T=1000$, $\Delta=0.05$, $T_0=1602$)' ...
    18.8 ms ± 1.99 ms per loop (mean ± std. dev. of 4 runs)

For policy #3 called 'TakeRandomFixedArm([2, 3, 6, 1])' ...
    27.4 ms ± 9.02 ms per loop (mean ± std. dev. of 4 runs)

For policy #14 called 'ETC_RandomStop($T=1000$)' ...
    37.2 ms ± 8.51 ms per loop (mean ± std. dev. of 4 runs)

For policy #12 called 'EpsilonFirst($T=1000$, $\varepsilon=0.1$)' ...
    46.3 ms ± 7.57 ms per loop (mean ± std. dev. of 4 runs)

For policy #97 called 'AdSwitch($T=1000$, $C_1=1$, $C_2=1$)' ...
    48 ms ± 10.6 ms per loop (mean ± std. dev. of 4 runs)

For policy #10 called 'EpsilonDecreasing(e:0.1)' ...
    53.4 ms ± 6.14 ms per loop (mean ± std. dev. of 4 runs)

For policy #90 called 'UCBcython($\alpha=4$)' ...
    55.7 ms ± 3.82 ms per loop (mean ± std. dev. of 4 runs)

For policy #9 called 'EpsilonGreedy(0.1)' ...
    55.9 ms ± 3.55 ms per loop (mean ± std. dev. of 4 runs)

For policy #91 called 'UCBcython($\alpha=1$)' ...
    56.3 ms ± 4.07 ms per loop (mean ± std. dev. of 4 runs)

For policy #92 called 'UCBcython($\alpha=0.5$)' ...
    56.5 ms ± 4.35 ms per loop (mean ± std. dev. of 4 runs)

For policy #11 called 'EpsilonExpDecreasing(e:0.1, r:0.005)' ...
    58 ms ± 6.76 ms per loop (mean ± std. dev. of 4 runs)

For policy #2 called 'EmpiricalMeans' ...
    68.4 ms ± 10.8 ms per loop (mean ± std. dev. of 4 runs)

For policy #72 called 'UnsupervisedLearning(SimpleGaussianKernel, :math:`T_0=100`, :math:`T_1=1000`, :math:`M=100`)' ...
    69.3 ms ± 5.23 ms per loop (mean ± std. dev. of 4 runs)

For policy #54 called 'CPUCB' ...
    70 ms ± 4.56 ms per loop (mean ± std. dev. of 4 runs)

For policy #31 called 'UCBlog10' ...
    71.5 ms ± 16.8 ms per loop (mean ± std. dev. of 4 runs)

For policy #34 called 'UCB($\alpha=1$, $\log_{10}$)' ...
    74.3 ms ± 5.01 ms per loop (mean ± std. dev. of 4 runs)

For policy #53 called 'OC-UCB($\eta=1.1$, $\rho=1$)' ...
    74.3 ms ± 5.84 ms per loop (mean ± std. dev. of 4 runs)

For policy #109 called 'OracleRestart-UCB(Per-Arm)' ...
    77.7 ms ± 3.09 ms per loop (mean ± std. dev. of 4 runs)

For policy #57 called 'Thompson' ...
    77.9 ms ± 7.12 ms per loop (mean ± std. dev. of 4 runs)

For policy #49 called 'MOSS' ...
    78.6 ms ± 10.2 ms per loop (mean ± std. dev. of 4 runs)

For policy #19 called 'BoltzmannGumbel($\alpha=0.5$)' ...
    79.2 ms ± 3.69 ms per loop (mean ± std. dev. of 4 runs)

For policy #51 called 'MOSS-Anytime($\alpha=1.35$)' ...
    79.2 ms ± 2.24 ms per loop (mean ± std. dev. of 4 runs)

For policy #102 called 'SW-Restart(UCB, $\tau=100$, $\varepsilon=0.005$)' ...
    79.7 ms ± 3.96 ms per loop (mean ± std. dev. of 4 runs)

For policy #50 called 'MOSS-H($T=1000$)' ...
    83.3 ms ± 9.52 ms per loop (mean ± std. dev. of 4 runs)

For policy #32 called 'UCBwrong' ...
    83.4 ms ± 15.1 ms per loop (mean ± std. dev. of 4 runs)

For policy #56 called 'DMED(Bern)' ...
    84.2 ms ± 6.92 ms per loop (mean ± std. dev. of 4 runs)

For policy #107 called 'M-UCB($w=450$)' ...
    87.1 ms ± 2.37 ms per loop (mean ± std. dev. of 4 runs)

For policy #46 called 'SW-Restart(UCB($\alpha=0.5$), $\tau=100$, $\varepsilon=0.005$)' ...
    87.6 ms ± 4.55 ms per loop (mean ± std. dev. of 4 runs)

For policy #33 called 'UCB($\alpha=1$)' ...
    88.6 ms ± 20.9 ms per loop (mean ± std. dev. of 4 runs)

For policy #94 called 'DiscountedThompson($\gamma=0.99$)' ...
    91.8 ms ± 4.16 ms per loop (mean ± std. dev. of 4 runs)

For policy #38 called 'UCBcython($\alpha=4$)' ...
    92.2 ms ± 38.7 ms per loop (mean ± std. dev. of 4 runs)

For policy #37 called 'UCBrandomInit' ...
    92.7 ms ± 11 ms per loop (mean ± std. dev. of 4 runs)

For policy #36 called 'UCBplus' ...
    94.8 ms ± 18.4 ms per loop (mean ± std. dev. of 4 runs)

For policy #52 called 'MOSS-Experimental' ...
    95.6 ms ± 5.18 ms per loop (mean ± std. dev. of 4 runs)

For policy #71 called 'ApprFHG($T=1100$)' ...
    96.8 ms ± 16.1 ms per loop (mean ± std. dev. of 4 runs)

For policy #30 called 'UCB' ...
    99.2 ms ± 9.58 ms per loop (mean ± std. dev. of 4 runs)

For policy #106 called 'D-UCB+($\alpha=1$, $\gamma=0.99209$)' ...
    99.9 ms ± 6.04 ms per loop (mean ± std. dev. of 4 runs)

For policy #35 called 'UCBmin' ...
    104 ms ± 13.6 ms per loop (mean ± std. dev. of 4 runs)

For policy #45 called 'SW-Restart(SWR_UCB, $\tau=100$, $\varepsilon=0.005$)' ...
    105 ms ± 17.8 ms per loop (mean ± std. dev. of 4 runs)

For policy #108 called 'M-UCB($w=80$)' ...
    105 ms ± 5.49 ms per loop (mean ± std. dev. of 4 runs)

For policy #58 called 'Thompson(Gauss)' ...
    106 ms ± 11.8 ms per loop (mean ± std. dev. of 4 runs)

For policy #83 called '$\mathrm{UCBcython}_{d=d_{bq}}$($c=0$)' ...
    110 ms ± 4.09 ms per loop (mean ± std. dev. of 4 runs)

For policy #84 called '$\mathrm{UCBcython}_{d=d_h}$($c=0$)' ...
    111 ms ± 4.88 ms per loop (mean ± std. dev. of 4 runs)

For policy #105 called 'D-UCB($\alpha=1$, $\gamma=0.9$)' ...
    111 ms ± 4.78 ms per loop (mean ± std. dev. of 4 runs)

For policy #55 called 'DMED$^+$(Bern)' ...
    111 ms ± 9.53 ms per loop (mean ± std. dev. of 4 runs)

For policy #85 called '$\mathrm{UCBcython}_{d=d_{lb}}$($c=0$)' ...
    112 ms ± 5.53 ms per loop (mean ± std. dev. of 4 runs)

For policy #26 called 'Pursuit(0.5)' ...
    119 ms ± 10.7 ms per loop (mean ± std. dev. of 4 runs)

For policy #114 called 'SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$)' ...
    122 ms ± 14.9 ms per loop (mean ± std. dev. of 4 runs)

For policy #100 called 'PHT-UCB($\alpha=0.0416$, $M=50$, Per-Arm)' ...
    130 ms ± 3.57 ms per loop (mean ± std. dev. of 4 runs)

For policy #86 called 'UCBoostcython($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)' ...
    135 ms ± 2.14 ms per loop (mean ± std. dev. of 4 runs)

For policy #76 called '$\mathrm{UCB}_{d=d_{bq}}$($c=0$)' ...
    138 ms ± 5.15 ms per loop (mean ± std. dev. of 4 runs)

For policy #112 called 'Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$)' ...
    141 ms ± 14.6 ms per loop (mean ± std. dev. of 4 runs)

For policy #47 called 'SparseUCB($s=2$, $\alpha=4$)' ...
    144 ms ± 17.8 ms per loop (mean ± std. dev. of 4 runs)

For policy #40 called 'UCBVtuned' ...
    144 ms ± 17.4 ms per loop (mean ± std. dev. of 4 runs)

For policy #39 called 'UCBV' ...
    145 ms ± 10.7 ms per loop (mean ± std. dev. of 4 runs)

For policy #87 called 'UCBoostcython($\varepsilon=0.1$, $c=0$)' ...
    152 ms ± 2.87 ms per loop (mean ± std. dev. of 4 runs)

For policy #65 called 'kl-UCB$^+$' ...
    154 ms ± 9.64 ms per loop (mean ± std. dev. of 4 runs)

For policy #63 called 'kl-UCB($\log_{10}$, Bern)' ...
    165 ms ± 12.3 ms per loop (mean ± std. dev. of 4 runs)

For policy #43 called 'D-UCB($\alpha=4$, $\gamma=1$)' ...
    166 ms ± 19.2 ms per loop (mean ± std. dev. of 4 runs)

For policy #110 called 'Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$)' ...
    167 ms ± 13.4 ms per loop (mean ± std. dev. of 4 runs)

For policy #66 called 'kl-UCB-H($T=1000$, Bern)' ...
    172 ms ± 15.6 ms per loop (mean ± std. dev. of 4 runs)

For policy #78 called '$\mathrm{UCB}_{d=d_{lb}}$($c=0$)' ...
    180 ms ± 15.9 ms per loop (mean ± std. dev. of 4 runs)

For policy #60 called 'kl-UCB' ...
    181 ms ± 28.8 ms per loop (mean ± std. dev. of 4 runs)

For policy #68 called 'kl-UCB$^{++}$($T=1000$)' ...
    183 ms ± 4.59 ms per loop (mean ± std. dev. of 4 runs)

For policy #61 called 'SW-Restart(kl-UCB, $\tau=100$, $\varepsilon=0.005$)' ...
    183 ms ± 33 ms per loop (mean ± std. dev. of 4 runs)

For policy #62 called 'kl-UCB($\log\log$, Bern)' ...
    184 ms ± 32 ms per loop (mean ± std. dev. of 4 runs)

For policy #16 called 'Softmax(decreasing)' ...
    186 ms ± 9.26 ms per loop (mean ± std. dev. of 4 runs)

For policy #99 called 'CUSUM-UCB($\alpha=0.0416$, $M=50$, Per-Arm)' ...
    187 ms ± 18 ms per loop (mean ± std. dev. of 4 runs)

For policy #15 called 'Softmax(temp: 0.05)' ...
    187 ms ± 5.7 ms per loop (mean ± std. dev. of 4 runs)

For policy #77 called '$\mathrm{UCB}_{d=d_h}$($c=0$)' ...
    189 ms ± 22.1 ms per loop (mean ± std. dev. of 4 runs)

For policy #69 called 'BayesUCB' ...
    189 ms ± 26.3 ms per loop (mean ± std. dev. of 4 runs)

For policy #44 called 'D-UCB+($\alpha=4$, $\gamma=1$)' ...
    189 ms ± 16.1 ms per loop (mean ± std. dev. of 4 runs)

For policy #64 called 'kl-UCB($\log_{10}\log_{10}$, Bern)' ...
    193 ms ± 20.9 ms per loop (mean ± std. dev. of 4 runs)

For policy #67 called 'kl-UCB-H+($T=1000$, Bern)' ...
    201 ms ± 23.7 ms per loop (mean ± std. dev. of 4 runs)

For policy #88 called 'UCBoostcython($\varepsilon=0.05$, $c=0$)' ...
    206 ms ± 9.33 ms per loop (mean ± std. dev. of 4 runs)

For policy #17 called 'SoftMix' ...
    208 ms ± 11 ms per loop (mean ± std. dev. of 4 runs)

For policy #18 called 'Softmax($T=1000$)' ...
    210 ms ± 5.8 ms per loop (mean ± std. dev. of 4 runs)

For policy #70 called 'AdBandits($T=1000$, $\alpha=0.5$)' ...
    213 ms ± 13.7 ms per loop (mean ± std. dev. of 4 runs)

For policy #28 called 'Hedge(decreasing)' ...
    217 ms ± 9.83 ms per loop (mean ± std. dev. of 4 runs)

For policy #79 called 'UCBoost($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)' ...
    229 ms ± 13.6 ms per loop (mean ± std. dev. of 4 runs)

For policy #27 called 'Hedge($\varepsilon: 0.5$)' ...
    234 ms ± 25.3 ms per loop (mean ± std. dev. of 4 runs)

For policy #73 called 'OSSB($\varepsilon=0.01$, $\gamma=0$, Bern)' ...
    239 ms ± 21 ms per loop (mean ± std. dev. of 4 runs)

For policy #29 called 'Hedge($T=1000$)' ...
    241 ms ± 23.4 ms per loop (mean ± std. dev. of 4 runs)

For policy #113 called 'Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking)' ...
    282 ms ± 3.57 ms per loop (mean ± std. dev. of 4 runs)

For policy #104 called 'SW-UCB+($\tau=332$, $\alpha=1$)' ...
    290 ms ± 7.99 ms per loop (mean ± std. dev. of 4 runs)

For policy #103 called 'SW-UCB($\tau=500$, $\alpha=1$)' ...
    302 ms ± 4.06 ms per loop (mean ± std. dev. of 4 runs)

For policy #20 called 'Exp3($\gamma: 0.001$)' ...
    308 ms ± 14.6 ms per loop (mean ± std. dev. of 4 runs)

For policy #111 called 'Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking)' ...
    314 ms ± 19.8 ms per loop (mean ± std. dev. of 4 runs)

For policy #21 called 'Exp3(decreasing)' ...
    354 ms ± 17.3 ms per loop (mean ± std. dev. of 4 runs)

For policy #95 called 'Exp3R($T=1000$, $c=8.77$, $\alpha=0.01$)' ...
    377 ms ± 9.24 ms per loop (mean ± std. dev. of 4 runs)

For policy #93 called 'Exp3++' ...
    381 ms ± 8.76 ms per loop (mean ± std. dev. of 4 runs)

For policy #41 called 'SW-UCB($\tau=1000$, $\alpha=0.6$)' ...
    396 ms ± 48.2 ms per loop (mean ± std. dev. of 4 runs)

For policy #25 called 'Exp3++' ...
    399 ms ± 13.8 ms per loop (mean ± std. dev. of 4 runs)

For policy #23 called 'Exp3($T=1000$)' ...
    410 ms ± 22.9 ms per loop (mean ± std. dev. of 4 runs)

For policy #22 called 'Exp3(SoftMix)' ...
    411 ms ± 14.3 ms per loop (mean ± std. dev. of 4 runs)

For policy #42 called 'SW-UCB+($\tau=1000$, $\alpha=0.6$)' ...
    436 ms ± 35.4 ms per loop (mean ± std. dev. of 4 runs)

For policy #75 called 'UCB$\dagger$($T=1000$)' ...
    474 ms ± 26.1 ms per loop (mean ± std. dev. of 4 runs)

For policy #48 called 'Sparse-kl-UCB($s=2$, Bern)' ...
    502 ms ± 42 ms per loop (mean ± std. dev. of 4 runs)

For policy #96 called 'Exp3R++($T=1000$, $c=1.76$, $\alpha=0.247$)' ...
    509 ms ± 12.4 ms per loop (mean ± std. dev. of 4 runs)

For policy #89 called 'UCBoostcython($\varepsilon=0.01$, $c=0$)' ...
    668 ms ± 108 ms per loop (mean ± std. dev. of 4 runs)

For policy #59 called 'ThompsonRobust(averageOn = 10)' ...
    689 ms ± 27 ms per loop (mean ± std. dev. of 4 runs)

For policy #80 called 'UCBoost($\varepsilon=0.1$, $c=0$)' ...
    711 ms ± 53.6 ms per loop (mean ± std. dev. of 4 runs)

For policy #24 called 'Exp3ELM($\delta=0.1$)' ...
    754 ms ± 38 ms per loop (mean ± std. dev. of 4 runs)

For policy #81 called 'UCBoost($\varepsilon=0.05$, $c=0$)' ...
    1.13 s ± 82.1 ms per loop (mean ± std. dev. of 4 runs)

For policy #74 called 'BESA' ...
    1.18 s ± 36.2 ms per loop (mean ± std. dev. of 4 runs)

For policy #101 called 'SW-UCB#($\lambda=1$, $\alpha=1$)' ...
    2.12 s ± 35.8 ms per loop (mean ± std. dev. of 4 runs)

For policy #82 called 'UCBoost($\varepsilon=0.01$, $c=0$)' ...
    12.2 s ± 826 ms per loop (mean ± std. dev. of 4 runs)

Giving the mean and std memory consumption ...

For policy #5 called 'TakeFixedArm(1)' ...
    119.5 B ± 66.7 B (mean ± std. dev. of 4 runs)

For policy #7 called 'TakeFixedArm(0)' ...
    119.5 B ± 66.7 B (mean ± std. dev. of 4 runs)

For policy #8 called 'TakeFixedArm(1)' ...
    119.5 B ± 66.7 B (mean ± std. dev. of 4 runs)

For policy #6 called 'TakeFixedArm(0)' ...
    158 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #4 called 'UniformOnSome([0, 1])' ...
    175 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #1 called 'UniformOnSome([0, 1])' ...
    178.2 B ± 5.6 B (mean ± std. dev. of 4 runs)

For policy #0 called 'U(1..9)' ...
    264 B ± 143.2 B (mean ± std. dev. of 4 runs)

For policy #3 called 'TakeRandomFixedArm([2, 3, 6, 1])' ...
    351 B ± 36.4 B (mean ± std. dev. of 4 runs)

For policy #15 called 'Softmax(temp: 0.05)' ...
    480.8 B ± 197.8 B (mean ± std. dev. of 4 runs)

For policy #2 called 'EmpiricalMeans' ...
    513.5 B ± 256.3 B (mean ± std. dev. of 4 runs)

For policy #14 called 'ETC_RandomStop($T=1000$)' ...
    517.8 B ± 296.6 B (mean ± std. dev. of 4 runs)

For policy #36 called 'UCBplus' ...
    559 B ± 320.4 B (mean ± std. dev. of 4 runs)

For policy #54 called 'CPUCB' ...
    567.2 B ± 325.2 B (mean ± std. dev. of 4 runs)

For policy #60 called 'kl-UCB' ...
    568.5 B ± 558.5 B (mean ± std. dev. of 4 runs)

For policy #69 called 'BayesUCB' ...
    570.5 B ± 560.5 B (mean ± std. dev. of 4 runs)

For policy #77 called '$\mathrm{UCB}_{d=d_h}$($c=0$)' ...
    586.8 B ± 336.5 B (mean ± std. dev. of 4 runs)

For policy #53 called 'OC-UCB($\eta=1.1$, $\rho=1$)' ...
    595 B ± 341.2 B (mean ± std. dev. of 4 runs)

For policy #34 called 'UCB($\alpha=1$, $\log_{10}$)' ...
    604.8 B ± 346.8 B (mean ± std. dev. of 4 runs)

For policy #91 called 'UCBcython($\alpha=1$)' ...
    607.8 B ± 348.6 B (mean ± std. dev. of 4 runs)

For policy #81 called 'UCBoost($\varepsilon=0.05$, $c=0$)' ...
    614.5 B ± 352.5 B (mean ± std. dev. of 4 runs)

For policy #83 called '$\mathrm{UCBcython}_{d=d_{bq}}$($c=0$)' ...
    619 B ± 355.1 B (mean ± std. dev. of 4 runs)

For policy #9 called 'EpsilonGreedy(0.1)' ...
    628 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #10 called 'EpsilonDecreasing(e:0.1)' ...
    638 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #87 called 'UCBoostcython($\varepsilon=0.1$, $c=0$)' ...
    643 B ± 368.9 B (mean ± std. dev. of 4 runs)

For policy #89 called 'UCBoostcython($\varepsilon=0.01$, $c=0$)' ...
    643.8 B ± 369.4 B (mean ± std. dev. of 4 runs)

For policy #12 called 'EpsilonFirst($T=1000$, $\varepsilon=0.1$)' ...
    667 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #37 called 'UCBrandomInit' ...
    706 B ± 405.3 B (mean ± std. dev. of 4 runs)

For policy #11 called 'EpsilonExpDecreasing(e:0.1, r:0.005)' ...
    709 B ± 43.3 B (mean ± std. dev. of 4 runs)

For policy #13 called 'ETC_KnownGap($T=1000$, $\Delta=0.05$, $T_0=1602$)' ...
    713 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #85 called '$\mathrm{UCBcython}_{d=d_{lb}}$($c=0$)' ...
    715 B ± 188.8 B (mean ± std. dev. of 4 runs)

For policy #28 called 'Hedge(decreasing)' ...
    724 B ± 415.7 B (mean ± std. dev. of 4 runs)

For policy #30 called 'UCB' ...
    732 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #21 called 'Exp3(decreasing)' ...
    732.2 B ± 420.5 B (mean ± std. dev. of 4 runs)

For policy #56 called 'DMED(Bern)' ...
    732.2 B ± 420.5 B (mean ± std. dev. of 4 runs)

For policy #49 called 'MOSS' ...
    735 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #35 called 'UCBmin' ...
    741 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #23 called 'Exp3($T=1000$)' ...
    744.2 B ± 427.4 B (mean ± std. dev. of 4 runs)

For policy #31 called 'UCBlog10' ...
    761 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #32 called 'UCBwrong' ...
    761 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #50 called 'MOSS-H($T=1000$)' ...
    766 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #33 called 'UCB($\alpha=1$)' ...
    768 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #52 called 'MOSS-Experimental' ...
    772 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #78 called '$\mathrm{UCB}_{d=d_{lb}}$($c=0$)' ...
    785 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #76 called '$\mathrm{UCB}_{d=d_{bq}}$($c=0$)' ...
    785 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #51 called 'MOSS-Anytime($\alpha=1.35$)' ...
    793 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #75 called 'UCB$\dagger$($T=1000$)' ...
    794 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #19 called 'BoltzmannGumbel($\alpha=0.5$)' ...
    799 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #38 called 'UCBcython($\alpha=4$)' ...
    809 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #90 called 'UCBcython($\alpha=4$)' ...
    809 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #92 called 'UCBcython($\alpha=0.5$)' ...
    811 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #58 called 'Thompson(Gauss)' ...
    813.5 B ± 727.8 B (mean ± std. dev. of 4 runs)

For policy #80 called 'UCBoost($\varepsilon=0.1$, $c=0$)' ...
    817 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #82 called 'UCBoost($\varepsilon=0.01$, $c=0$)' ...
    818 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #84 called '$\mathrm{UCBcython}_{d=d_h}$($c=0$)' ...
    820 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #26 called 'Pursuit(0.5)' ...
    827 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #71 called 'ApprFHG($T=1100$)' ...
    854 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #88 called 'UCBoostcython($\varepsilon=0.05$, $c=0$)' ...
    857 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #62 called 'kl-UCB($\log\log$, Bern)' ...
    868.8 B ± 499.3 B (mean ± std. dev. of 4 runs)

For policy #66 called 'kl-UCB-H($T=1000$, Bern)' ...
    874 B ± 502.3 B (mean ± std. dev. of 4 runs)

For policy #73 called 'OSSB($\varepsilon=0.01$, $\gamma=0$, Bern)' ...
    877 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #17 called 'SoftMix' ...
    889 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #64 called 'kl-UCB($\log_{10}\log_{10}$, Bern)' ...
    889.8 B ± 511.4 B (mean ± std. dev. of 4 runs)

For policy #79 called 'UCBoost($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)' ...
    892 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #59 called 'ThompsonRobust(averageOn = 10)' ...
    899 B ± 516.7 B (mean ± std. dev. of 4 runs)

For policy #39 called 'UCBV' ...
    907 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #16 called 'Softmax(decreasing)' ...
    911 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #40 called 'UCBVtuned' ...
    922 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #18 called 'Softmax($T=1000$)' ...
    927 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #86 called 'UCBoostcython($D=\{d_{bq},d_h,d_{lb}\}$, $c=0$)' ...
    931 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #27 called 'Hedge($\varepsilon: 0.5$)' ...
    962 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #22 called 'Exp3(SoftMix)' ...
    969 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #20 called 'Exp3($\gamma: 0.001$)' ...
    970 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #29 called 'Hedge($T=1000$)' ...
    980 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #55 called 'DMED$^+$(Bern)' ...
    983 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #70 called 'AdBandits($T=1000$, $\alpha=0.5$)' ...
    1000 B ± 0 B (mean ± std. dev. of 4 runs)

For policy #47 called 'SparseUCB($s=2$, $\alpha=4$)' ...
    1.1 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #24 called 'Exp3ELM($\delta=0.1$)' ...
    1.1 KiB ± 620.9 B (mean ± std. dev. of 4 runs)

For policy #57 called 'Thompson' ...
    1.1 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #65 called 'kl-UCB$^+$' ...
    1.1 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #63 called 'kl-UCB($\log_{10}$, Bern)' ...
    1.1 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #67 called 'kl-UCB-H+($T=1000$, Bern)' ...
    1.1 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #68 called 'kl-UCB$^{++}$($T=1000$)' ...
    1.2 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #93 called 'Exp3++' ...
    1.3 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #25 called 'Exp3++' ...
    1.3 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #43 called 'D-UCB($\alpha=4$, $\gamma=1$)' ...
    1.3 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #105 called 'D-UCB($\alpha=1$, $\gamma=0.9$)' ...
    1.3 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #44 called 'D-UCB+($\alpha=4$, $\gamma=1$)' ...
    1.3 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #94 called 'DiscountedThompson($\gamma=0.99$)' ...
    1.4 KiB ± 4.9 B (mean ± std. dev. of 4 runs)

For policy #106 called 'D-UCB+($\alpha=1$, $\gamma=0.99209$)' ...
    1.4 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #48 called 'Sparse-kl-UCB($s=2$, Bern)' ...
    1.4 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #72 called 'UnsupervisedLearning(SimpleGaussianKernel, :math:`T_0=100`, :math:`T_1=1000`, :math:`M=100`)' ...
    3.2 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #108 called 'M-UCB($w=80$)' ...
    5.6 KiB ± 61.5 B (mean ± std. dev. of 4 runs)

For policy #45 called 'SW-Restart(SWR_UCB, $\tau=100$, $\varepsilon=0.005$)' ...
    6.1 KiB ± 3.5 KiB (mean ± std. dev. of 4 runs)

For policy #46 called 'SW-Restart(UCB($\alpha=0.5$), $\tau=100$, $\varepsilon=0.005$)' ...
    6.1 KiB ± 3.5 KiB (mean ± std. dev. of 4 runs)

For policy #104 called 'SW-UCB+($\tau=332$, $\alpha=1$)' ...
    6.2 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #102 called 'SW-Restart(UCB, $\tau=100$, $\varepsilon=0.005$)' ...
    6.4 KiB ± 3.7 KiB (mean ± std. dev. of 4 runs)

For policy #111 called 'Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking)' ...
    7.5 KiB ± 4.4 KiB (mean ± std. dev. of 4 runs)

For policy #112 called 'Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$)' ...
    7.8 KiB ± 4.5 KiB (mean ± std. dev. of 4 runs)

For policy #99 called 'CUSUM-UCB($\alpha=0.0416$, $M=50$, Per-Arm)' ...
    7.9 KiB ± 1.9 KiB (mean ± std. dev. of 4 runs)

For policy #95 called 'Exp3R($T=1000$, $c=8.77$, $\alpha=0.01$)' ...
    8 KiB ± 4.6 KiB (mean ± std. dev. of 4 runs)

For policy #61 called 'SW-Restart(kl-UCB, $\tau=100$, $\varepsilon=0.005$)' ...
    8.5 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #103 called 'SW-UCB($\tau=500$, $\alpha=1$)' ...
    8.8 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #114 called 'SubGaussian-GLR-UCB($\delta=0.01$, $\sigma=0.25$, joint, $\alpha=0.0416$)' ...
    9.7 KiB ± 653.1 B (mean ± std. dev. of 4 runs)

For policy #107 called 'M-UCB($w=450$)' ...
    9.7 KiB ± 347.3 B (mean ± std. dev. of 4 runs)

For policy #98 called 'LM-DSEE($\gamma=200$, $\rho=0.6$, $\ell=1.48e+04$, $a=1$, $b=0.25$)' ...
    9.7 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #110 called 'Bernoulli-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$)' ...
    9.9 KiB ± 445.5 B (mean ± std. dev. of 4 runs)

For policy #109 called 'OracleRestart-UCB(Per-Arm)' ...
    10.3 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #113 called 'Gaussian-GLR-UCB($\delta=\frac{1}{T}$, $\alpha=0.0416$, tracking)' ...
    10.6 KiB ± 178.4 B (mean ± std. dev. of 4 runs)

For policy #100 called 'PHT-UCB($\alpha=0.0416$, $M=50$, Per-Arm)' ...
    10.6 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #96 called 'Exp3R++($T=1000$, $c=1.76$, $\alpha=0.247$)' ...
    11 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #97 called 'AdSwitch($T=1000$, $C_1=1$, $C_2=1$)' ...
    12.7 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #41 called 'SW-UCB($\tau=1000$, $\alpha=0.6$)' ...
    16.6 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #42 called 'SW-UCB+($\tau=1000$, $\alpha=0.6$)' ...
    16.6 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #101 called 'SW-UCB#($\lambda=1$, $\alpha=1$)' ...
    39.8 KiB ± 0 B (mean ± std. dev. of 4 runs)

For policy #74 called 'BESA' ...
    53.8 KiB ± 30.9 KiB (mean ± std. dev. of 4 runs)
Warning: environment MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9) did not have a method plotHistoryOfMeans...
